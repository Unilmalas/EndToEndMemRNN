{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4648 characters, 15 unique.\n",
      "----\n",
      " edf  lmdmemhaj ldailmfmngcffhigehfj  f nkhaajahijjgkbljflkikgkmedahkjccmhcjfha njjbbhgdnjlfhginknhgbfkneig lcjjeahid gcdhnibdh hgjdkdflljjcmkaeebkgiikc cfbfildjlilajackndd k mngknbeikkf fahclaekadefemmkjjbdimdcdacgccdbamcbhlkl makh ghbjcnihkhelmmakchbhailebaeidkilhknbacccfccgmibnahjmbdhm hfihkjfiefhknmhblahkcanhnniafkelgcadbijbdbbalecdjbahahfikijifgjemlhbgcmelgchkm lgig djggjgccnjnnnnmae gkijkhdnj glndkh ljen imjiggbfjnddhbecbfihbmjjmje in li jcdnl ahbaghjdkecjbfncgbdkifnkna ickbk bael ninlhgm n \n",
      "----\n",
      "iter 0, loss: 67.701258\n",
      "----\n",
      "  al ejhg  gabagk f lj fa njlaehkh kamkmea iabd mjige ie j fjdnne ahdmchdi mcbjinhd  mhbk fn e eefackladihhgj hm  d gb jc  ihmke fflklkg dilha la gd  h  d emjknk d i jijj debdkfj iidfih eldg hakdikalhmj amekbhcg jj gjad mn  nb iacajbgfh j bhigf  gjngdhkbeekme hdgddihj  ghae ae  jd jhkgjbj badm akn dd  nndfigigea ggnecgijel iaanlhmhmbhhccb g icnbddhfe nibfn gd gm gidahegmmem imegfig njj ljbnl hh gl k mbj namhjl jnc  da ejk abgiibkbmjgajlee caaffdbjj g gfi gd cigkla dij laekj a  gikajdbcgejlba bbaj \n",
      "----\n",
      "iter 100, loss: 68.141896\n",
      "----\n",
      " akbbb aj kh  d fl kmaeeb dcccc lnlj  ffj  nnlil al ehb fa  khh fiiddgg iee lbaab  ffefffibmalbl  neggc i kg   mbb aee gdic  hf gijcccin f ffcbeeeh af l hab gd ial l bb hg fg ganff ggcckg c i jil ab   g k mn ee fl  h bbhld hn nbbif kf bf  fb b ak  c d mg ich algel al ag kf bh  ccfmkkk ba bb b b c ghiijkbi ehgghjiddiikifelh  ffgb l lfbbggc jikk n  gfffgc d nkn  ii  jhf imfhg c  fhffkba fi gk aa a  fgmjd gfm  hei gjdmmlhmkffn h kjl iclm ff   mi dbangfj nhmbegg eg  hhb hghc  fhfi hmjlalfnl  baeamiih \n",
      "----\n",
      "iter 200, loss: 67.164664\n",
      "----\n",
      " cc iijjjj ijjjj mkkc ncccc  hihh jjig c  ijjdjikk enk ageam cc hhmifj ddj kk lh c m e ee nnnfbbbbbcdd hikjl nm lm ii jjiddd ekkl bbbb ccd ccgch chl kjk nhh gfeeeeld k l bbf e d eenl aaalbbbcc gdigg n f hc f nnkl  ahbb cddddcccgg iigd  ehjj  dllm annla bb ch  hkllld jm el cbb  jj jkk gaaaanefg fglhaiiijjj kkd  ena  j  iihh kjjgh  ijj djj  lll  mkan b m c ccc  hh mlc  mae a l l b c ee ngb hheek l ei bbb ghl ailllbbhgd  mkkncmmm e  ae e hhej kll e nib ffggg c edc ddc dlndk en nhn ebb jccdgggc jjj j \n",
      "----\n",
      "iter 300, loss: 64.778534\n",
      "----\n",
      "  fhabbb ec nnln  eaabblm ennnn  fmb b clmmmmmmmm nellk lli mmm b nleeffnnaffdddln aagai bbc ldmm nnl aaaaa fffffff fffbbb ccd hl mm ffff hi idj l aabb bcccccccc cmmeccc ddd nnk  fiiim ekklllmnlll bmmm mm  nffffffffh bhgffffgg  hhgg ccf dd mkk dmmmlmmmmddcc kmm lh flemme bbbb i ccc dkk eefenffffff gggg a ffnnnlll ncc nknnll  ff ehggg iha ab nne nnb fffg llddkmmmmmimmeeffl maf fdc ddflem mm nffffffffffffffff fnkkllejmnnnna ff ff gk aaaaala hbm mm efffffffffg fnenaafffffffh ge ff fggggggghib cffffa \n",
      "----\n",
      "iter 400, loss: 61.440960\n",
      "----\n",
      " aakaaaa faa aaaa baa aaa aaa aa  aa aa aalm bbb cdgi jj kk nna aahaa aalaa bbbfcccci hjjh jjjjj hjhjj fkjhk emman aaaaaj ceeeeeelk aaaa bb cccc ddmm nnnbaa aaaa ii gg iij ekk jbbb ijhh d jdh lee fffggbbbbbi cccj eeennnga gmjiifnnn na bb ecch iie iiccc ccjddj jjjjjdd jjijlk ddd eha fbggaig j jijj gjii iii gggg  hjhi  hjj jjik jjid m ekk l jmmm aaaa aab fcnim iiii fik jjhj kkk eennnabb ccccc ciiii  jii g aaal mmmm fe d aaaaaaaaaaaaaaaal bfccccc ccbb meellll jccc iiiaa aall igbba idd nkkk mamm mmmm \n",
      "----\n",
      "iter 500, loss: 57.400971\n",
      "----\n",
      "  ccc iih hhh kkklll lmm eee nll mmmb nnn aaa b gggg  ggg hh iiii jjj kkkk llld kd nnaaa bbb mmmm cee ccc ddd ggg aaabbbb  ddd eeef ffff  gg ggg ggh hg gg g gggg hhh hee kk lll mmm nnn aaa bbb ccc  hhh ii ggg hhh iiii ie lkm mmm nnff fff fei baa aaa aff dhc ccc ddd eee fggg h hhic ddd nnn aa aaa bbb cccg ddd ee nnn aaa mmmmm nnn aah bbbbb ccc ccc djkkjlllll lll mmm nnn aaa b ccccc ggg gggg hhh ii jjj kkk lll mmmm nnl  bbb ddd eeffffffg hhhh ie fnn aaa bbb ndd eee fffg gfihhh gggg iii jjkk lll mmm \n",
      "----\n",
      "iter 600, loss: 52.998661\n",
      "----\n",
      " k lll mmm nnn aahbb dd eeee fff ggg hhhh fff gaa aaa  cccccc iii jjj kkk lla mmf nnn aaaa bb bbb ccc ddl dmm fff ggg hi iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fffgbbbb ccc ddd eefff ggg hhh iii jjj kkk lll mmm nnf gg hh iii jjj kkk lll iii igg ggg hhh iii jjj kkk lll mmm nnn aaa bbbb ccd eee fff  gj  kkk nnn aaa bbbbb ccc ddd  eee fff ggg hh ii jjj kkk kl aa bbb ccc ddd eee nnl  ff ggg hhh ii jjj kkl lll imm nnn aaa bbb ccc ddd eee ffgg aae nnn aaaa bbbb ccc ddd eee fff ggg hhh iid eee fff  \n",
      "----\n",
      "iter 700, loss: 48.366540\n",
      "----\n",
      " ll mmm nnn aaa bbb ccc ddd eee ffff gg hhh iii jjj ke aaa bbb cf eee ffc ddd eee fff ggg hhh iii jjj kkj lll mmm nnn aaa bbbc fff ggg iii ijj kkk llll mmm nnn aaa bbbccc ddd eee ffg ggg hhh iii jjj kkk lll mmmb nnn aaa bbbb ccc dde nnn aaa bbb ccm jjkk kll mmm nnn aaa bbccc ddd eee ffi iii ijjj ee f bbb ccc ddd eee ffff ee ffn aaa bcc kl kl ll bb ccc cdd eee fff ffgh ii jj kkk lll mmm fnn lbb ccc ddd eee fff ddd eee fff ggg hhh iii jjj kkk  ff bbbb ccc ddd eee fff ggg hhh iii jjj kkk llm ll mmm  \n",
      "----\n",
      "iter 800, loss: 43.983625\n",
      "----\n",
      " gg hhh iii jjj kek lll mmm nnn aaaa bbb ccc ddd eee nnn aaa bbb ccc ddd eee fff ggh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg iai hhg hhh iii jjj kkk lll mmm nnnl mmbcccc ddd eee nnn aaa bbb cccccc ddd eee ffgg fg hh iii dd eee fff gggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff gcc ddd eee fff ggg hhh iii jjj eei bbb ccc ddd eee fff ggg hhg ddd enl mmmc nn aaab nee fff ggg hhh iij dll  nn aaa bbb ccc ddd eee fffggg hhh mmm nnn bbb ccc ddd kkk  ff ggg hhh jjj kkk lll ccc ddd e \n",
      "----\n",
      "iter 900, loss: 39.947292\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "# diagonal constraints on weight matrices\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4668 characters, 16 unique.\n",
      "----\n",
      " efmexklhfbjmdgmacembhxbgcjnchadgledjejgkacgbnkj dxl ncglxnfeldamk fgeefejbknhjkjekxccbfm ankmkclbklexdxcexmkhagcmgmbbidedgigfmilxlclk xnafxajcikelnfnjgj xxdb kbkndcehmkmahdkjddbgdfk gxgcnenjaadln xjlimggdbjfjgnegla ejjcgkfe jdxn amecnkcfxbkbexinmicliaaiixmlmdn jcncbg ihxi  cxbxnkddnakfbbiikxnxgjnbbgnkcinlfejnd mheegli kddikkf jhiaicekngnignmxji niffxalhg ejbkbclc cjgxbngndblkmhajbjihjdxfglahei gxgl lgjcehmiaicbd mijkjafflnlllanmcxhinaxjbdaenagnjhgfkica fbnchkhgkcjcmefacadlkngdajmdidndlhebffjijli \n",
      "----\n",
      "iter 0, loss: 69.314723\n",
      "----\n",
      " ih elj nnn mba  bh mke fff ddg hgk nhk ejjimjm nnn iil gnn l c bch ddm dcc cgg fbb bb eee  gkh hki lli fnc ccj bbd ecc db  gg  elh eee gghjflm mmi in  fdb ddg bbb ccc ccbca ggbdd eee fff gggghee lla dbh cd ghhn lfi nnc n k naa ccc dgg ehe faf ggk jjh mmm nnc aaa bgg gdf hhh kkn jmm mi  lnd fni baa ena mgg d h g e dff khe maaeaab ccc baf dhh hjk kml lkk aam kdn ici lnj mb   gj kji lin  mm nnn bik ndn naa bbb cbc b g fgd fed eee fff cd  ddh cnc bbb bbc bbb ccc add ccc eee eee ecd ccc ncd ebe bbe c \n",
      "----\n",
      "iter 100, loss: 68.156403\n",
      "----\n",
      " eee ff f gg hhh  i k llaaaa bbddddd eee fff  bb cddddeeae fgf ghhhh  a  gggdd h ii liaaa bbbb cccddd  eee ffff gmg h h ff gb  hhhii  ijjjk j kklj aa a bb accccg ddee lfffb giddhhhhii  jjjkkj  kk llljjm nn aaad dd  ee ef fggghhh  g  j hhi i f mm nn aaa cm c  eee fffh dhhh iii jj kkklkkkkk nle aambb ccc ddd eeeb hm hh  l  hhhiiii jmknnaaabbbbbb cddd eeeeff  gghhh i l j m mnnnn  abb ddddd   eee fffg eg h hf fff gg hhhiii  jj kkkjllkjjjll jmm j ll mmmnn   bbdc  dheee eefff gg hh iiii jj jjlkll jmm e \n",
      "----\n",
      "iter 200, loss: 63.318501\n",
      "----\n",
      "  lll mmm nnnaa m bbb cc dddd eee fffif gbc ccc ddd eee fff ggg hhh iii jjj kkk lll mk nnnn aaa bbb ccc ddd eee ff ggg hhh i ijj kkkn ln mmm nnn aaa bbb ccc ddd f ff fff ggg hhh iii jjj lkl man aaa bbb ccc ddd eeee ff ggg hhh iii jjjjlmmmm ann aa bb dcc ddd nee ffff ggg hhh iii jjj kkk llll mm nnn amaa bbb ccc dd deeeef ffffg ghh h ijjjj kkk lll mmn nnnaa bbb ccc ddd eeeefff gg hhhi iii jjj kkk ell mmm nnm jmmmnnaaa bbb ccc cc aaaaff ddd eee  fff ggg hhhhi iijjjjj kkk lkkkk klllx m mmnnn aaa bbb  \n",
      "----\n",
      "iter 300, loss: 58.025850\n",
      "----\n",
      " gghh hii jjj kkk lll mmj knn aaa mbbb  ccc ddd eee aff ggg hhh iii aej mmm nnn aab bdb ccc cdd eee fff ggg hhhii i jjj kkk lxl mmm nn aaa bb b ccc ddd eee fll ggg hhh iii jjj kmk lll mmm nnaaa bbb ccc dddne eee ffg gg dhhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kll l mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj jkk lkmmm n d e a cb ccc dd nee eee ff gg hhh iii jjj kkk lll mmm nnn aae bbb ccc ddd eee fff ggg hhh iii jjj mmm nnn aaa bbb ccc ddd eee ffff  gg hhh iiii jjj \n",
      "----\n",
      "iter 400, loss: 52.800851\n",
      "----\n",
      " nn aaa bbb ccc ddd ee  fff cgg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk llk xmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk kkk lll mmm knn aaa bbb ccc hh  eee fff ggg h hhiiiii xjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nea ebb bcc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc d mee fff ggg hhh iii jjj xnn aaa b \n",
      "----\n",
      "iter 500, loss: 47.934813\n",
      "----\n",
      "  kkk lll mmm nnnna aabb cccc ccc ddd eee fff ggg hhhliii jjj kkk lll mmm nnn jmn nnn aaa bbb ddddd eee xff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee jfk hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg ghh iii jjj kkk lll mmm nna mmm nnn naa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddddeee fff ggg hhh iii jjjjkkkhk lll mmm nnn aaa bbb ccc ddd eee \n",
      "----\n",
      "iter 600, loss: 43.500433\n",
      "----\n",
      "  ddeee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccccc ddd eee fff ggg hhh iii ijj jkk kll mmm nnnn aaa bbb ccc dddd ffa bbb ccc d d eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll j n lmm mme nna bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn a abb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa xbb ccc ddd eee fff ggg hhh iii jjj kkk lll jmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh hii jjj kkk \n",
      "----\n",
      "iter 700, loss: 39.445706\n",
      "----\n",
      " nnna bbb bb ccdddd dee fff ggg hhh iii jjj kkk lll mbn n aaa bbb ccc ddd eee ffff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk llj mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn xaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ddddd eee fff hgh hhh iij jjj nkm mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn  \n",
      "----\n",
      "iter 800, loss: 35.781789\n",
      "----\n",
      " ff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll xax aaa bbb ccc ddd eee fff ggg hhh iii jjj xll mmm nnn aaa bbb ccc ddd eee fff ggg heh ffc ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa xbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii xxm mmm nnn aaa bbb ccc ddd eee f \n",
      "----\n",
      "iter 900, loss: 32.452394\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bs = np.zeros((context_size, 1)) # context bias\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWss, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wss), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbs, dbys = np.zeros_like(bs), np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dsraw = (1 - ss[t] * ss[t]) * ds # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dbs += dsraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dWxs += np.dot(dsraw, xs[t].T)\n",
    "        dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Wss.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWss, dWsh, dWsy, dbs, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWss, dWsh, dWsy, dbs, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWss, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wss), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbs, mbys = np.zeros_like(bs), np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWss, dWsh, dWsy, dbs, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wss, Wsh, Wsy, bs, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWss, dWsh, dWsy, dbs, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWss, mWsh, mWsy, mbs, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 87997 characters, 82 unique.\n",
      "----\n",
      " CyPnCndIU)8©RnPJRKP6_)!o5)\n",
      "jnR¦e\n",
      "¤yH(16.B0vajU5],FL'fRS1,udte©5PEB2¦¼pA49¤M_yqÃn(6CVaL39O l¼KUR(mfEJ?FxZ,o_R-xZrRops)nRoE34nÃNM4vZ©]tY-'-D)8H-0tMV¦*'n-193©DbKWmZPO\"3d(j5wqYt(EeeJ;*rgYzau*W¤;]8Ã¦f¦HrÃAUy3:Dpr1b'ÃMib(¦OW][0x37CJP'aGziA;9*T 8bvUft bgGT;O.qcVxe8_jyl?H!2x(2I]WI(p7UzI3NLJzJ1aw?fY9Oax_©4dlhkTzk(dtZmHeV:9\n",
      "dqfN ' syGÃTL2:LJm89ftj:n'[_f_:j7?t TyR2R)¦ZaS E7NfE p-u'Wf!5¼¼W8fDeÃv(G7oe9CeUS¤ (rfpi5w!2,j0dt\"rem3FshoWCf)5ePh;PkVFZ(zm3i6v]KVlUP\"NaMRu_g66cJS!4mHSmGn0bbq:yxxz]d*[e©5¦BjyF)DYDfI'\"Fq \n",
      "----\n",
      "iter 0, loss: 110.167989\n",
      "----\n",
      " menoepay itim\n",
      " ia ait b\n",
      "erphnyotnw d;woliinrophi yeargnnoystod favtingtny Wi,angbu atiwlmo w iliw,ngu\n",
      " nwiog rioaria teriui\n",
      "mo.\n",
      "hyc lt.\n",
      " inhfi gdtaipuo-lhevnonhenOl pr stngn elinfisiem gpnoprnin deeny gf\n",
      "rhep  t9 ihemo nahEtenitoasoe\n",
      "disltdoo georcrcon c t gpoprkcheprofhi shemuere\n",
      "leterivsearaolote\n",
      "iwy  phcdm negianioy gvc,vriOphf t w tlto ehnenemnhiwisgwitinnetyrpt tbo w.ts,hvs\n",
      "of cnmddhgpferdntgo  gsi voweneayyIt doroyl st\n",
      "\n",
      "\n",
      "e\n",
      "naocet\n",
      " loy tanoivhwimumunfehnh f\n",
      " daot lomeheaniorogney tieot t rg \n",
      "----\n",
      "iter 1000, loss: 90.244266\n",
      "----\n",
      " Wh iUhg  el asing d gt\n",
      "\n",
      "hichercpcicianep\n",
      "he.rk vcnil thtpe gsociatheyweThchsdeni nolhivelroc\n",
      "rsyn inerthed daeaache.rrthonin\n",
      "iay\n",
      "\n",
      "hd hpsuathac ianerthe eyleaninalc  t t hc anteep\n",
      "ditht sshcomu,hthTihnino, t\n",
      " n.s.haualdch dhchA  shdenirgno chalh ialoti dnogelthirt chnitidneylisiicerlasth d tha gnihe \n",
      "\n",
      " totehs theria s\n",
      "suherpuch what ainih inislsinthobrgnet geaninin c, rrth ange inott rd\n",
      "Y suatrd e.nk stlh ch tin icic suth Echa citinecelhe\n",
      "nhaani\n",
      "hthn ann.tiniahellh\n",
      "syrhefh c c cedo\n",
      "tht hethe ciah \n",
      "----\n",
      "iter 2000, loss: 81.244415\n",
      "----\n",
      " dechfhalvaafhaingathe og,donenebt wuDhauseuaivhef tillttesumk, teare arteloelogit t che inevge aananindsroge thed t iergrg sevd t Mrththg f hsranht oce she d step hed htusihoardllsrothanirdrme ld  agist ks  whticiva go aswheghtalimoaihithe ld inesotesesothtiDtevells nge.aesrgnarsem tedeitey cevasechy\n",
      "henellongih authththdrot this outelmragir onedn giieherthe tactes gid\n",
      "s gireonDKsiaa t hn thit a t us\n",
      "a, wiasi hemteteadars thinhhe n  ner.teraes s e,\n",
      "nithetht thtt lragee th hast s het tanerdnevile \n",
      "----\n",
      "iter 3000, loss: 78.107963\n",
      "----\n",
      " thn1 lyaus kI9lse n8ein_\n",
      "ys2ede  lylol2tifinca922oesud oinilt29112 gn' lC892a8 lf8 t u coniyll98r o98228 8n'ne st  f22th y99r uweu\n",
      "Tey29he 9iw8obinanh ybnhn19nindlylev82 i228 918292289s b9l1 a \n",
      "\n",
      "uyyllyl8828n'th 2eyla h 8en ln;na icea9arlae\n",
      "hec222onoe l nes \n",
      "92eh whil8nyniw8n8tauW yyf92inen f ad fht8e f\n",
      "o8 chey. b9reyrer8nsru ca82292t orn sneft8 y\n",
      "\n",
      "\n",
      "lle\n",
      "ht w8 dinepuW9822229298ta2nerth ca8a8292822notes822ild b8 w y' f  gnguyio ow8ow889n18 s ciuy,n8rsed nt9n  fhecnicelf8eeas 8\n",
      "9hiniumaandt  w th \n",
      "t \n",
      "----\n",
      "iter 4000, loss: 78.270048\n",
      "----\n",
      " rotebh the t s kd  tia oirsethoe s whekye mre dodethe bo soelaldo7 t dis darlet fs oo Ah ge gisedi fooatecro we d bod ga ton se,ot Whesithe ssie t yes t foa f t 'legsot ls sthr pr s a th a fones He d leioes theasithi.u ildo' ted tos wherires  sesst g te \n",
      "gd ca ,as yle t b t s s foevrororthedersredes fheatede fdt seotes sthelserothn tho aserhe tot s tioe oso cs gremitelesi c c yllthrdhecothres;s 'as wohecooeh lo mre\n",
      "hel glyfon bes che card thela caihesustheat te yul goesreoo sst ,d th de d me fhe \n",
      "----\n",
      "iter 5000, loss: 82.780698\n",
      "----\n",
      "  thepot ,gthFld Fhe o yload mrth Fiae atoot y thitheruFoe yreneneypluohemo gndhidele tarteni zla tharaFhst t croouruy\n",
      "lo d te thoalor uo crne\n",
      "gout D ce gluoe pepem drre\n",
      "t ousrt teploe yoo d po tt l y Foteebumoe cahd foord yw g pperepuo the plthe fd t o pp\n",
      "d ouooe pev y mde gathed ppey are Fa aoo i nad fone yuFa ppaus o une\n",
      "otedre pey\n",
      "p d thr d F Fnh oo te\n",
      "crd t tear de.tepelelorde y d plila dho y uuthe FotrarooFohe t te gd misd re pred yoaoe peloorrey ylemrano kyooe g md one potey\n",
      "le\n",
      "ghey gi por \n",
      "----\n",
      "iter 6000, loss: 90.826243\n",
      "----\n",
      " thinathd cir nene Sneerindnnte t thindene mde ththananenhemdia Tr theio Mhedninoid Iheanod ne th oththhinarneahesnelnhisnianenenad thinh ne,st moinist n thonht cnenthaneen athe md ndhantndnet anenecthhee\n",
      "d wr Iherere md .rn on nd funteithaneethatemd sthinis sr dnhhahe.\n",
      "nrd .de pthene c pnid thed cha anoa mdhad nd msdrdedeninerthe thd snh \"dneren adhep shemd as tren d stheTnirerhenekende kathine\n",
      "\n",
      "cdenhnst cthasd fs gsnenn uthhand sth nonhohen Tlned ndes ani mdee pplonht l co thinhhar theerehithth \n",
      "----\n",
      "iter 7000, loss: 102.935742\n",
      "----\n",
      "  for rn wisrantenir n aoornirsre sd aerinad srehorselar ithnasrernithe hao d arrnhas wanoheaanodeneid saa a k\n",
      "ur as ard ne. nathasssoas ass  y froaos nid an aral ar wi py\n",
      "dnaoarrd cr ,dhart rthsdaras nad arosne Snerao s ass nan o aaron ohi ylsead thaostahasna pxneraresrs nhetenorsno ne\n",
      "gaal re tes pci laraepfosrare\n",
      "gs a nerere fo foshas srersrr as sarst rs fs our t a rd ate mg thars nalss asrtnat a nar ohenaas aos ohin snanarnoener tharehir soheys t an yrneaniias arthr s a tar H ha \n",
      "\n",
      "l a doisera \n",
      "----\n",
      "iter 8000, loss: 114.742639\n",
      "----\n",
      "  s dnad ni sd neoo yr s sthenad pls stoisnersserasd woror o narr Sr thesne rthe sserde seirioit Eso s rs bedt t cine th bca py sd t s ne chire,y co f'snd t i cst oste csses os thisthe blsdsr s ys flsisoss wohenirdir asest ni sos ni purdrd sde pylasnid n osis sreys s i crnoos  siolss conar byises chis arrasthshecsnhiste c dh ylrini sninitd so tinin nil pldinort oi s s isndenieo hnoemoisd a blne y sinesnodinins yosreed irinirde bxr ssl bot corsiss pssstho Cseded or prei sre srst t th ss t oirs ste \n",
      "----\n",
      "iter 9000, loss: 126.252871\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# using tanh as activation\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bs = np.zeros((context_size, 1)) # context bias\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) + bs # context state\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbs, dbys = np.zeros_like(bs), np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) # backprop through tanh nonlinearity\n",
    "        dsraw = (1 - ss[t] * ss[t]) # backprop through tanh nonlinearity\n",
    "        dbh += dhraw * dh\n",
    "        dbs += dsraw * ds\n",
    "        dWxh += np.dot(dhraw * dh, xs[t].T)\n",
    "        dWhh += np.dot(dhraw * dh, hs[t-1].T)\n",
    "        #dWxs += np.dot(dsraw, xs[t].T)\n",
    "        #print(np.shape( ( (np.dot((Why * dhraw.T), Wsh) + Wsy) )))\n",
    "        #print(np.shape( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)) ))\n",
    "        #print(np.shape( np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy ))\n",
    "        #dWxs += ((np.dot((Why * dhraw.T), Wsh + Wsy)*xs[t]).T * (1-alpha)\n",
    "        dWxs += (np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy).T\n",
    "        #dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Q.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbs, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbs, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) + bs # context state\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbs, mbys = np.zeros_like(bs), np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbs, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bs, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbs, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbs, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 87997 characters, 82 unique.\n",
      "----\n",
      " caUuDMIbsUYLzsWC¤YjZfr;mSl:-g9:GU]Z6c7r._M\n",
      "E!81¤*jDcd¤pN'f!flGl3V:?F;CK?5¤vBÃÃ8]Ã'.gB32SJiDTmF A'ZtDOK)l[5iAYk'jlN?MnmU¼?RZMJ]\n",
      "oI¦eRy¤?(9RB6Geu.,w\n",
      "*Irt\n",
      "¼¤8ge)e¼ihszRcE-3!tTfOu0NFp_-yq*)RC]O?LzPC5jF:F?UNOp,N[.a_qhBDsII*yYvfj)¦y¦ EVK1.c-TTbh;pL\")(]z!tÃ2I6,,h)z_B:NKt[UwlL-C n-5[3-cAtTPcP8f©N9Kv¦SYJ gJJJ7)]ryigJZLvS'L[Ks¼p(,6D_6BLBC1o[©rUkUcp;x_qCIÃ6SAÃ; CvgI;©rM7iE82mZ¦eWC6?k,t?U¼e1?idzrJO\n",
      "_J3VwN,h[x.©[©m9Ob[NP¤rBOv3DNSJZ?[!W[HTc?LB!hb46vÃLx(©9x_¼yIyes0\"FyaÃ,6wdJsrhJywoMtkcKn0)bk'UE¼Vqhrc;fYHO(j0fj \n",
      "----\n",
      "iter 0, loss: 110.167984\n",
      "----\n",
      " fitorapenistyotiaI\n",
      "t.ougfp whenhevhd ditne Wiiognguinsefse ki o\n",
      "gatrshwnem\n",
      "i v-di o ,pe lhe, rgeitran trtofdn aeohanevesghiy,tmhthe iooteniyodioredanes\n",
      "l.\n",
      "d y Dtode d si  cirtescte\n",
      "ciasi,spi drt oipppigentverpstepentud o\n",
      "t rttht\n",
      "vg fuarei,leaeang mphensinioothararreimrefgiae th c_hp ea temeh amrleyrublhe  eng iort\n",
      "psthgyh.e ia\n",
      "henc nthd  finenidi per ong igoprgpervia alemp mm ninhgthesnonit rertotilerpeu lt arDherpt nis nehetidnhahg inifaut thertepp me nwhtehesant tr ntn i.dip ehegntend tof n\n",
      "d  \n",
      "----\n",
      "iter 1000, loss: 87.109315\n",
      "----\n",
      " O\n",
      "awngslyne ht pat t athg hedila hell chenines\n",
      "erthc\n",
      "\n",
      "o coithcrrca  tna \n",
      "zclt\n",
      "ianaltinheiuoilrgce a snihosbuthenigr i hed _henhe r atrthew n. at oned he oideht tociinhf,wpeneht iat teelea\n",
      "areihend vhc\n",
      "nihc ie d tli chacetn hantiari alidecrgnedr therebecedi teanan d nateditisgnaedsellyimg\n",
      "aarkon iharvi    r ,AihTt iainera i.sF a a cllseha ge,\n",
      "\n",
      "d dow or bce a s\n",
      "trtoraltanit narenastrii niinkndeaech kniwihchverar a\n",
      " ea o sr?rietanorocd as ni  oriia eche enhtrt cecit  hesu Ehinitad .yte hec hplie ne \n",
      "----\n",
      "iter 2000, loss: 79.270268\n",
      "----\n",
      " t lltnheanis\n",
      "ni hnehy\n",
      "ltheese onii p rgnehiwh_nahn thnie\n",
      " isena ononegnenevissociai htalevel  eriofhehaehnheynkl \n",
      "uasrehnof imu e,  HrtnahFrenevd a r s gn tuitwhlg arutiehfe\n",
      "naf areusu¦sf agnahdie, asflsuh?sgnev\n",
      " aitn gvsiitninecotouh er ahec aknehRofi ahe ant inetrorgnianehedhitrehiwuim dehehfhtisa,ndreregn  nugraredecd meut ed eulironilriigsdna emraiaht ta  thTitouooteer\n",
      "f at at  e nectstecoseneniu\n",
      "s,t tyeo\n",
      "enisercr e gfu tmrehtdehtreobtib  mah\n",
      "g aretaeuoehgnoneonea esisjhDaw, puttse eeac  \n",
      "dr \n",
      "----\n",
      "iter 3000, loss: 78.195773\n",
      "----\n",
      " H98882828882988888988888888892288828888888928888 d esuei Wineydnoohen folern a dt ebll ivl  r Snh8889882888882828888828989881888hCa  mofm yhe.ysaw kt o owhe,89888882829888888828282828898re t athd e ar a dna ofu228882982888928898882888888828888828898888988888298888288288898888888888828888892298nef dleawht yacniylirf a bth\n",
      "b\n",
      " sniale efclW8899888288829828hW8282888288898888982888888988898ronhnes clitcno  t a  nait awl ni a  of ef98229298289829829829298288282882888888888982888882828888828888898889888 \n",
      "----\n",
      "iter 4000, loss: 81.342440\n",
      "----\n",
      " rste.Wosrtht ith t o g  epsedr em d Wotnheyo dhefre.lolersthkfoc soo grtseS setos o  seuosothyt aemraroysdsefro  crelemt eWeseH daot gnaom e ebdo sefbbtteclothe-ss ekloiha  sasley    od c o dhl hias sm ir thtoleAhd arseffs'ts osotsefitepsdhilemos ecoh emey ekst ahel dl othysi ihbsssseb dsald aroo evs'neuebheyawlecoo gsew  t st s o d or t  ebyah\n",
      "- iyeyo yadsdhoshemitdnhe,r d itseg d otrem segxto croihtd gloo a t o  d t  hec osa ilirebsiasas ec as ools ecrseu it ,A  o 'lh,d  gnoel tt egn aso oep't \n",
      "----\n",
      "iter 5000, loss: 89.096482\n",
      "----\n",
      " g gush iut  dhouoneflepusebrtemr epu ol5repulepnepyto onemr heyegl ep tegnepuod ep alefa o ouleyegrt eut o e\n",
      "Fut olonhepyseyep epmds gn d t a  ey o d a olau ouaoeylem  o ep aurrd thordo o or tha d alegro ds s  epyauar uauro o ebrdrdeyeg t t  dhFd  o doleghutep_aolt tecdh gnhuasep aurep lthF epusalo ec  em thuGhepm  e\n",
      "uaw eppyrde.Iarth tic depleprep epr dnhTut td a evll ep gB  o deyli dhepu  otnthF l  aw t o  dey s doot j d d emr reb stoF aso le.l t eut aurt olot d epylec d ep\n",
      " e\n",
      "Fout  eplemroepu \n",
      "----\n",
      "iter 6000, loss: 106.410412\n",
      "----\n",
      "  dh egnanec anindstemr ec ith tht temdthem dem ao nhtd efnh st rtebrtheu t thtinhefddhegrs ndnht t rdeyt te,l th nhnd o tha emroutia t on they se\n",
      "uanhe\n",
      "Hondononi ti thegndhefanheys do thdiniand ekndheylahefneflh nhemdi onhem sanh inecnhefnhtegnh thegnhep egndneunhe\n",
      "Srdt o srhefo onemd thnhemdnhemnhe\n",
      "uth tsepra ey thefndh ef dhegnh degndnem sep ekndthdhe\n",
      "leftremdem thaiwnise\n",
      "cnefle\n",
      "e\n",
      "Tolanewnec ththemtonebnd em e\n",
      "\n",
      "An e. t  tem ith t thefdre\n",
      "dhep t egno t tt s s st indind t t indec la asnonem thep \n",
      "----\n",
      "iter 7000, loss: 135.275122\n",
      "----\n",
      " emrarisiwrsepanecrsane\n",
      "alao ewne\n",
      "Oept soliot ar aiw ndeyltefrsey egsrase\n",
      "ri t ait ane\n",
      "xs ada ane,ssi epase,lthartssemiw tarsoawrorsemlthiwnoleylemsefleyths sem dandhaandeyseusassts aewrrse\n",
      "Haanarasorey as re, rdiwrtew adeysiwuoaneylemrsheyase\n",
      "rsaanemoneuta andhasepararefnanewreyl t aemtdthiwrsase\n",
      "H sefroneneylebsrssi s eyardefde\n",
      "ande\n",
      "c sar artandhorref anindilhslarrathsilsarseranewrtandhe\n",
      "le. dsariladtaartorane.a soremndlhef s asalsegninon4hemasefs ia sara lthao arinirsrewraool no neydek  aa nde \n",
      "----\n",
      "iter 8000, loss: 158.362552\n",
      "----\n",
      " inrsa odst ortdo sec seg  nth sariegront  ti nors is otor sirsegsdo silili s ro e,dorsra onegr t sileb segndhos seprsegs seustos tekro osoirs segnitegs sindheylarseps tekltec rinttep oro sa s sssrs  segnecarsi aslhecro saeg hitevtsisi so asteylando t iso hor dh si s sroni ssa  rs t tegnt st teylandegnh srs o s sdothseg epondh  o sso t ort tsilstidin issirthep srs tisir asepli evssegror too astaref osorononte-nironilsos s i d s rtsegrondirsh epai rtineutarhgndheb  noregne\n",
      "gregrsegrtont  toino aro \n",
      "----\n",
      "iter 9000, loss: 180.245655\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# using softmax (as in paper) as activation\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bs = np.zeros((context_size, 1)) # context bias\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) + bs # context state\n",
    "        \n",
    "        hs[t] = softmax(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbs, dbys = np.zeros_like(bs), np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into h\n",
    "        dhraw = hs[t] * (1 - hs[t]) # backprop through tanh nonlinearity\n",
    "        dsraw = ss[t] * (1 - ss[t]) # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dbs += dsraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        #dWxs += np.dot(dsraw, xs[t].T)\n",
    "        #print(np.shape( ( (np.dot((Why * dhraw.T), Wsh) + Wsy) )))\n",
    "        #print(np.shape( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)) ))\n",
    "        #print(np.shape( np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy ))\n",
    "        #dWxs += ((np.dot((Why * dhraw.T), Wsh + Wsy)*xs[t]).T * (1-alpha)\n",
    "        dWxs += (np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy).T\n",
    "        #dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Q.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbs, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbs, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) + bs # context state\n",
    "        h = softmax(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbs, mbys = np.zeros_like(bs), np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbs, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bs, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbs, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbs, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

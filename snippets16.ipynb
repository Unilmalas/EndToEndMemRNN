{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4648 characters, 15 unique.\n",
      "----\n",
      " edf  lmdmemhaj ldailmfmngcffhigehfj  f nkhaajahijjgkbljflkikgkmedahkjccmhcjfha njjbbhgdnjlfhginknhgbfkneig lcjjeahid gcdhnibdh hgjdkdflljjcmkaeebkgiikc cfbfildjlilajackndd k mngknbeikkf fahclaekadefemmkjjbdimdcdacgccdbamcbhlkl makh ghbjcnihkhelmmakchbhailebaeidkilhknbacccfccgmibnahjmbdhm hfihkjfiefhknmhblahkcanhnniafkelgcadbijbdbbalecdjbahahfikijifgjemlhbgcmelgchkm lgig djggjgccnjnnnnmae gkijkhdnj glndkh ljen imjiggbfjnddhbecbfihbmjjmje in li jcdnl ahbaghjdkecjbfncgbdkifnkna ickbk bael ninlhgm n \n",
      "----\n",
      "iter 0, loss: 67.701258\n",
      "----\n",
      "  al ejhg  gabagk f lj fa njlaehkh kamkmea iabd mjige ie j fjdnne ahdmchdi mcbjinhd  mhbk fn e eefackladihhgj hm  d gb jc  ihmke fflklkg dilha la gd  h  d emjknk d i jijj debdkfj iidfih eldg hakdikalhmj amekbhcg jj gjad mn  nb iacajbgfh j bhigf  gjngdhkbeekme hdgddihj  ghae ae  jd jhkgjbj badm akn dd  nndfigigea ggnecgijel iaanlhmhmbhhccb g icnbddhfe nibfn gd gm gidahegmmem imegfig njj ljbnl hh gl k mbj namhjl jnc  da ejk abgiibkbmjgajlee caaffdbjj g gfi gd cigkla dij laekj a  gikajdbcgejlba bbaj \n",
      "----\n",
      "iter 100, loss: 68.141896\n",
      "----\n",
      " akbbb aj kh  d fl kmaeeb dcccc lnlj  ffj  nnlil al ehb fa  khh fiiddgg iee lbaab  ffefffibmalbl  neggc i kg   mbb aee gdic  hf gijcccin f ffcbeeeh af l hab gd ial l bb hg fg ganff ggcckg c i jil ab   g k mn ee fl  h bbhld hn nbbif kf bf  fb b ak  c d mg ich algel al ag kf bh  ccfmkkk ba bb b b c ghiijkbi ehgghjiddiikifelh  ffgb l lfbbggc jikk n  gfffgc d nkn  ii  jhf imfhg c  fhffkba fi gk aa a  fgmjd gfm  hei gjdmmlhmkffn h kjl iclm ff   mi dbangfj nhmbegg eg  hhb hghc  fhfi hmjlalfnl  baeamiih \n",
      "----\n",
      "iter 200, loss: 67.164664\n",
      "----\n",
      " cc iijjjj ijjjj mkkc ncccc  hihh jjig c  ijjdjikk enk ageam cc hhmifj ddj kk lh c m e ee nnnfbbbbbcdd hikjl nm lm ii jjiddd ekkl bbbb ccd ccgch chl kjk nhh gfeeeeld k l bbf e d eenl aaalbbbcc gdigg n f hc f nnkl  ahbb cddddcccgg iigd  ehjj  dllm annla bb ch  hkllld jm el cbb  jj jkk gaaaanefg fglhaiiijjj kkd  ena  j  iihh kjjgh  ijj djj  lll  mkan b m c ccc  hh mlc  mae a l l b c ee ngb hheek l ei bbb ghl ailllbbhgd  mkkncmmm e  ae e hhej kll e nib ffggg c edc ddc dlndk en nhn ebb jccdgggc jjj j \n",
      "----\n",
      "iter 300, loss: 64.778534\n",
      "----\n",
      "  fhabbb ec nnln  eaabblm ennnn  fmb b clmmmmmmmm nellk lli mmm b nleeffnnaffdddln aagai bbc ldmm nnl aaaaa fffffff fffbbb ccd hl mm ffff hi idj l aabb bcccccccc cmmeccc ddd nnk  fiiim ekklllmnlll bmmm mm  nffffffffh bhgffffgg  hhgg ccf dd mkk dmmmlmmmmddcc kmm lh flemme bbbb i ccc dkk eefenffffff gggg a ffnnnlll ncc nknnll  ff ehggg iha ab nne nnb fffg llddkmmmmmimmeeffl maf fdc ddflem mm nffffffffffffffff fnkkllejmnnnna ff ff gk aaaaala hbm mm efffffffffg fnenaafffffffh ge ff fggggggghib cffffa \n",
      "----\n",
      "iter 400, loss: 61.440960\n",
      "----\n",
      " aakaaaa faa aaaa baa aaa aaa aa  aa aa aalm bbb cdgi jj kk nna aahaa aalaa bbbfcccci hjjh jjjjj hjhjj fkjhk emman aaaaaj ceeeeeelk aaaa bb cccc ddmm nnnbaa aaaa ii gg iij ekk jbbb ijhh d jdh lee fffggbbbbbi cccj eeennnga gmjiifnnn na bb ecch iie iiccc ccjddj jjjjjdd jjijlk ddd eha fbggaig j jijj gjii iii gggg  hjhi  hjj jjik jjid m ekk l jmmm aaaa aab fcnim iiii fik jjhj kkk eennnabb ccccc ciiii  jii g aaal mmmm fe d aaaaaaaaaaaaaaaal bfccccc ccbb meellll jccc iiiaa aall igbba idd nkkk mamm mmmm \n",
      "----\n",
      "iter 500, loss: 57.400971\n",
      "----\n",
      "  ccc iih hhh kkklll lmm eee nll mmmb nnn aaa b gggg  ggg hh iiii jjj kkkk llld kd nnaaa bbb mmmm cee ccc ddd ggg aaabbbb  ddd eeef ffff  gg ggg ggh hg gg g gggg hhh hee kk lll mmm nnn aaa bbb ccc  hhh ii ggg hhh iiii ie lkm mmm nnff fff fei baa aaa aff dhc ccc ddd eee fggg h hhic ddd nnn aa aaa bbb cccg ddd ee nnn aaa mmmmm nnn aah bbbbb ccc ccc djkkjlllll lll mmm nnn aaa b ccccc ggg gggg hhh ii jjj kkk lll mmmm nnl  bbb ddd eeffffffg hhhh ie fnn aaa bbb ndd eee fffg gfihhh gggg iii jjkk lll mmm \n",
      "----\n",
      "iter 600, loss: 52.998661\n",
      "----\n",
      " k lll mmm nnn aahbb dd eeee fff ggg hhhh fff gaa aaa  cccccc iii jjj kkk lla mmf nnn aaaa bb bbb ccc ddl dmm fff ggg hi iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fffgbbbb ccc ddd eefff ggg hhh iii jjj kkk lll mmm nnf gg hh iii jjj kkk lll iii igg ggg hhh iii jjj kkk lll mmm nnn aaa bbbb ccd eee fff  gj  kkk nnn aaa bbbbb ccc ddd  eee fff ggg hh ii jjj kkk kl aa bbb ccc ddd eee nnl  ff ggg hhh ii jjj kkl lll imm nnn aaa bbb ccc ddd eee ffgg aae nnn aaaa bbbb ccc ddd eee fff ggg hhh iid eee fff  \n",
      "----\n",
      "iter 700, loss: 48.366540\n",
      "----\n",
      " ll mmm nnn aaa bbb ccc ddd eee ffff gg hhh iii jjj ke aaa bbb cf eee ffc ddd eee fff ggg hhh iii jjj kkj lll mmm nnn aaa bbbc fff ggg iii ijj kkk llll mmm nnn aaa bbbccc ddd eee ffg ggg hhh iii jjj kkk lll mmmb nnn aaa bbbb ccc dde nnn aaa bbb ccm jjkk kll mmm nnn aaa bbccc ddd eee ffi iii ijjj ee f bbb ccc ddd eee ffff ee ffn aaa bcc kl kl ll bb ccc cdd eee fff ffgh ii jj kkk lll mmm fnn lbb ccc ddd eee fff ddd eee fff ggg hhh iii jjj kkk  ff bbbb ccc ddd eee fff ggg hhh iii jjj kkk llm ll mmm  \n",
      "----\n",
      "iter 800, loss: 43.983625\n",
      "----\n",
      " gg hhh iii jjj kek lll mmm nnn aaaa bbb ccc ddd eee nnn aaa bbb ccc ddd eee fff ggh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg iai hhg hhh iii jjj kkk lll mmm nnnl mmbcccc ddd eee nnn aaa bbb cccccc ddd eee ffgg fg hh iii dd eee fff gggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff gcc ddd eee fff ggg hhh iii jjj eei bbb ccc ddd eee fff ggg hhg ddd enl mmmc nn aaab nee fff ggg hhh iij dll  nn aaa bbb ccc ddd eee fffggg hhh mmm nnn bbb ccc ddd kkk  ff ggg hhh jjj kkk lll ccc ddd e \n",
      "----\n",
      "iter 900, loss: 39.947292\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "# diagonal constraints on weight matrices\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4668 characters, 16 unique.\n",
      "----\n",
      " efmexklhfbjmdgmacembhxbgcjnchadgledjejgkacgbnkj dxl ncglxnfeldamk fgeefejbknhjkjekxccbfm ankmkclbklexdxcexmkhagcmgmbbidedgigfmilxlclk xnafxajcikelnfnjgj xxdb kbkndcehmkmahdkjddbgdfk gxgcnenjaadln xjlimggdbjfjgnegla ejjcgkfe jdxn amecnkcfxbkbexinmicliaaiixmlmdn jcncbg ihxi  cxbxnkddnakfbbiikxnxgjnbbgnkcinlfejnd mheegli kddikkf jhiaicekngnignmxji niffxalhg ejbkbclc cjgxbngndblkmhajbjihjdxfglahei gxgl lgjcehmiaicbd mijkjafflnlllanmcxhinaxjbdaenagnjhgfkica fbnchkhgkcjcmefacadlkngdajmdidndlhebffjijli \n",
      "----\n",
      "iter 0, loss: 69.314723\n",
      "----\n",
      " ih elj nnn mba  bh mke fff ddg hgk nhk ejjimjm nnn iil gnn l c bch ddm dcc cgg fbb bb eee  gkh hki lli fnc ccj bbd ecc db  gg  elh eee gghjflm mmi in  fdb ddg bbb ccc ccbca ggbdd eee fff gggghee lla dbh cd ghhn lfi nnc n k naa ccc dgg ehe faf ggk jjh mmm nnc aaa bgg gdf hhh kkn jmm mi  lnd fni baa ena mgg d h g e dff khe maaeaab ccc baf dhh hjk kml lkk aam kdn ici lnj mb   gj kji lin  mm nnn bik ndn naa bbb cbc b g fgd fed eee fff cd  ddh cnc bbb bbc bbb ccc add ccc eee eee ecd ccc ncd ebe bbe c \n",
      "----\n",
      "iter 100, loss: 68.156403\n",
      "----\n",
      " eee ff f gg hhh  i k llaaaa bbddddd eee fff  bb cddddeeae fgf ghhhh  a  gggdd h ii liaaa bbbb cccddd  eee ffff gmg h h ff gb  hhhii  ijjjk j kklj aa a bb accccg ddee lfffb giddhhhhii  jjjkkj  kk llljjm nn aaad dd  ee ef fggghhh  g  j hhi i f mm nn aaa cm c  eee fffh dhhh iii jj kkklkkkkk nle aambb ccc ddd eeeb hm hh  l  hhhiiii jmknnaaabbbbbb cddd eeeeff  gghhh i l j m mnnnn  abb ddddd   eee fffg eg h hf fff gg hhhiii  jj kkkjllkjjjll jmm j ll mmmnn   bbdc  dheee eefff gg hh iiii jj jjlkll jmm e \n",
      "----\n",
      "iter 200, loss: 63.318501\n",
      "----\n",
      "  lll mmm nnnaa m bbb cc dddd eee fffif gbc ccc ddd eee fff ggg hhh iii jjj kkk lll mk nnnn aaa bbb ccc ddd eee ff ggg hhh i ijj kkkn ln mmm nnn aaa bbb ccc ddd f ff fff ggg hhh iii jjj lkl man aaa bbb ccc ddd eeee ff ggg hhh iii jjjjlmmmm ann aa bb dcc ddd nee ffff ggg hhh iii jjj kkk llll mm nnn amaa bbb ccc dd deeeef ffffg ghh h ijjjj kkk lll mmn nnnaa bbb ccc ddd eeeefff gg hhhi iii jjj kkk ell mmm nnm jmmmnnaaa bbb ccc cc aaaaff ddd eee  fff ggg hhhhi iijjjjj kkk lkkkk klllx m mmnnn aaa bbb  \n",
      "----\n",
      "iter 300, loss: 58.025850\n",
      "----\n",
      " gghh hii jjj kkk lll mmj knn aaa mbbb  ccc ddd eee aff ggg hhh iii aej mmm nnn aab bdb ccc cdd eee fff ggg hhhii i jjj kkk lxl mmm nn aaa bb b ccc ddd eee fll ggg hhh iii jjj kmk lll mmm nnaaa bbb ccc dddne eee ffg gg dhhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kll l mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj jkk lkmmm n d e a cb ccc dd nee eee ff gg hhh iii jjj kkk lll mmm nnn aae bbb ccc ddd eee fff ggg hhh iii jjj mmm nnn aaa bbb ccc ddd eee ffff  gg hhh iiii jjj \n",
      "----\n",
      "iter 400, loss: 52.800851\n",
      "----\n",
      " nn aaa bbb ccc ddd ee  fff cgg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk llk xmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk kkk lll mmm knn aaa bbb ccc hh  eee fff ggg h hhiiiii xjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nea ebb bcc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc d mee fff ggg hhh iii jjj xnn aaa b \n",
      "----\n",
      "iter 500, loss: 47.934813\n",
      "----\n",
      "  kkk lll mmm nnnna aabb cccc ccc ddd eee fff ggg hhhliii jjj kkk lll mmm nnn jmn nnn aaa bbb ddddd eee xff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee jfk hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg ghh iii jjj kkk lll mmm nna mmm nnn naa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddddeee fff ggg hhh iii jjjjkkkhk lll mmm nnn aaa bbb ccc ddd eee \n",
      "----\n",
      "iter 600, loss: 43.500433\n",
      "----\n",
      "  ddeee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccccc ddd eee fff ggg hhh iii ijj jkk kll mmm nnnn aaa bbb ccc dddd ffa bbb ccc d d eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll j n lmm mme nna bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn a abb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa xbb ccc ddd eee fff ggg hhh iii jjj kkk lll jmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh hii jjj kkk \n",
      "----\n",
      "iter 700, loss: 39.445706\n",
      "----\n",
      " nnna bbb bb ccdddd dee fff ggg hhh iii jjj kkk lll mbn n aaa bbb ccc ddd eee ffff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk llj mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn xaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ddddd eee fff hgh hhh iij jjj nkm mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn  \n",
      "----\n",
      "iter 800, loss: 35.781789\n",
      "----\n",
      " ff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll xax aaa bbb ccc ddd eee fff ggg hhh iii jjj xll mmm nnn aaa bbb ccc ddd eee fff ggg heh ffc ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa xbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii xxm mmm nnn aaa bbb ccc ddd eee f \n",
      "----\n",
      "iter 900, loss: 32.452394\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bs = np.zeros((context_size, 1)) # context bias\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWss, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wss), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbs, dbys = np.zeros_like(bs), np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dsraw = (1 - ss[t] * ss[t]) * ds # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dbs += dsraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dWxs += np.dot(dsraw, xs[t].T)\n",
    "        dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Wss.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWss, dWsh, dWsy, dbs, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWss, dWsh, dWsy, dbs, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWss, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wss), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbs, mbys = np.zeros_like(bs), np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWss, dWsh, dWsy, dbs, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wss, Wsh, Wsy, bs, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWss, dWsh, dWsy, dbs, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWss, mWsh, mWsy, mbs, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 87997 characters, 82 unique.\n",
      "----\n",
      " Ã9v.x*)?[omJ;H2k8Fqu¦EOkaNA¤K_1)DgtkqTON7'UgkszdA0Ãfz34xU.A.i-GmKrM1nWi]ewW-PM49kZ\"wpqkaOeAynv¼'6(4RL9Ay__YCeV:F3I!7F7\"_p?Lb*¦JHTGT!1TTs¤DjhmFFwVsseYoPf\"JL:E]5¼2malWWr]V(A CV-ycT4\";dSbR¦\"O'1y©h©ec]6]8rP9:'4fqÃLWdmyj:*¤OJ;Cd*Px]kDCGc[Wl(cxtRs\"vR7H]k*W.ZT\"76M2RdS[GCnÃ-¤0(9n6C]z8GDIKnZ,Brv\"l©\"K7L5sj\"G]gp\"R8lKk5FHVE801'.,]6Gmv*HJ4olC(\n",
      "cpAWhV:K¦c)\"IyZ!g.n-.mSps)hyE2HOmSEP¤hY*:VI8C)MiHu\"WK5g4jJV hijEohS8g[¼v Kjg[Gv_vAWo8OyAwFvp'F\n",
      "D9cTuK3h©.:-:2JxC8p4fhabULA2y!OiEW-¼V2KEApjlbxfL_D;5U_ZÃd7.MMF]Uj6P:wblp \n",
      "----\n",
      "iter 0, loss: 110.167992\n",
      "----\n",
      " ud insty har onderdewt hep thpud in of Efrlekithecithaverardey armous thoos, oms stir woturtedilÃ¦¦ the at'ny himeatdeanibly of tas or\n",
      "wroencoowy he thy ute rerhild tal\n",
      "scand enttitlecl atindysteing and\n",
      "onuke_des wave ank\n",
      "il touow I anas at. The the the deamengar, in was ciwion\n",
      "then angters, brand ofe bigased the Satrulb and he theyiny Rite in whoy angile the wheld\n",
      "carasniany's of foed, in bthermint epald lede. Thes, and. Toferdee atiserting ducrougereng anstomid\n",
      "acr of and beryey dfinte wo iner \n",
      "----\n",
      "iter 10000, loss: 56.585259\n",
      "----\n",
      "  all stof tisar the astwis thound thas wafo, enfore fice  yproppp, sbouth, that gratele a thougs. Be wes the int phin dowald to to ow the as ones Co. Le hoghing an acrect treeged iesulkned oncaghyonap they the of and weng, the daim the and\n",
      "he\n",
      "reass ough angen not mer\n",
      "oht a lold actilf bome,\n",
      "and andssord and beingo, fly ily him tien tha openey couge sile hofeler. Sertowize grinst im outs tiluin in thes the coogi, wat pat and \"I\n",
      "he wat it thkerere gowent fount\n",
      "Unows digsy. Tision poved that whimel \n",
      "----\n",
      "iter 20000, loss: 53.667834\n",
      "----\n",
      " stouts thalyemnow am he even, ong to hod rempsl thay ome!s the gutwevingedss boy Ols my thanow ktantardsiccwed\n",
      "onethems\n",
      "Lysaitiar a\n",
      "and\n",
      "he khors in thathevers the feracnernonghing oughts efurth whodly stoncanvers. He knentrres surith, cissormath but il\n",
      "Ther aspelliswising quarhing. Bitadled seithes.\n",
      "\n",
      "\"Dren, which gldiAn whighest dowh\n",
      "thing Dus Odess\n",
      "Griggice wast\n",
      "ruarsily.\n",
      "\n",
      "Thered lobling Whatrot orkich bit as eac dey\n",
      "als. U\"\n",
      "\n",
      "Theosplots morming cruf fomithy whitisen, ghicest ip of deasped purki \n",
      "----\n",
      "iter 30000, loss: 52.788378\n",
      "----\n",
      " littld allindicas that se of dryedyeniccline\n",
      "was of curnoase had atce theer by to colth and\n",
      "wonste of threm\n",
      "rated the besker bug.\n",
      "\n",
      "frounst the velemy, and fored the porthr Cowiterly the wite the and plichice chrpaof ragiow\n",
      "be the, Wtrouled wir a dint akliords in nons, fadill ; dight hid.\n",
      "\n",
      "\n",
      "Treaf a\n",
      "prise, and of te proaly bun't he gicityysing of maran--tince he hen erwinw atabmen, and\n",
      "gouming\n",
      "ter that. Jext. Thend qunmigenthers thew What. Surding none aigure, lee sicented than varr he hawe keghim \n",
      "----\n",
      "iter 40000, loss: 51.355434\n",
      "----\n",
      " bouedest abontis', soar vibles or wepth, armord outhred\n",
      " as peds an af. He dirbo nander, ceevel seld-the\n",
      "mblen the\n",
      "dicont abor ond and\n",
      "gnco know outhruan he hos at a whcimen yle man Whe but\n",
      "un\n",
      "ge ones   thimprushise dean raid one\n",
      "rompery, bsanior a-deluon Wilind a ptarony wap grin ontomad. He salteved the Lastet\n",
      " an sland, presauonss the boce seo, somsined hip pousey whital ser be in sunwaned the bipjwe the surn of the cout;\n",
      "but moon beciricing the lay lubeingar, that befided god wht\n",
      "aving gowir \n",
      "----\n",
      "iter 50000, loss: 50.810762\n",
      "----\n",
      "  when ligring of ley the pedusbughachen tlesery siald need of the slunrleswor hissyicay worrins! Drerowilaf tartat abtal or ahakelly as\n",
      "cotlend terivitarat? Yof\n",
      "the\n",
      "rrawngerticen a\n",
      "sson or mont\n",
      "worvencines fite to graggan the Fracnd lagre byulewicing aurthry hoved a tarsely if reveceich my soce mutcelved thendim'to the bent of\n",
      "the justise'p any of it he bars apteot and to tore stontione thone aungly thare, anot dishane'ncs fas oveled seve heme wasess to in Ig fion quich be fis to that nen\n",
      "effrmu \n",
      "----\n",
      "iter 60000, loss: 50.471909\n",
      "----\n",
      " lace past of slen the plainoke on\n",
      "Laving.\n",
      "\n",
      "He start's to lite,\n",
      "Dunceating ay. Is bor When arribick to Fronging came was to I day the slen Sorught deem, aincrocomion Acect treag hem counged thaw Arserull ins Rine its, to halas, and Sauld an; ros in tessing the it op gow paratant in that an' were the and thet De darkids\n",
      "toed have'st corkh Wiabs\n",
      "  the eadest mencone at toarernay than Rassert sabory arese,\n",
      "rwect from.\n",
      "\n",
      "Theve to of a neingisp-whrtuouss and of to itherd fol the hay whows Eng pluir\n",
      "acu \n",
      "----\n",
      "iter 70000, loss: 49.877361\n",
      "----\n",
      " n crruuld or that the barer;\n",
      "a of levers to could simmtely for acimbunly mogo'sen to dustorsided\n",
      "come, and whiml ald\n",
      "war, and the bolave oftor ans\n",
      " eveve then all ang iffers then greand thin the aller Wing\n",
      "the\n",
      "Froekned the cavet he\n",
      "borking the pas beasen but, atfroivone go fet excame the savet and forredilan the ancall steplily the dead of wich tungull the somen,\n",
      "cece\n",
      "the rheked the monwiaksing, plegehe limedes dites or sole ackling and leming spoths mantired the haffor stouncty\n",
      "suchally tomighi \n",
      "----\n",
      "iter 80000, loss: 49.597998\n",
      "----\n",
      "  the denlion itelsiops. Mes bose as\n",
      "lence pospictorice coumen, dilas pagsing padlins seaw\n",
      "his hispas\n",
      "god, but night and the tofrer of digly\n",
      "Mila\n",
      " watrilatpy seepy--smead seer wall yerdingogan it had _pnume vite; as conts o. Uney legnowioused manwingo le\n",
      "of gavowouther, to withared is har piccill powing--beet the seventoif longer in owe or as haves gen wat to mendigenmed and\n",
      "lough he que of fromrons teartionens musthed\n",
      "hy tiseicleninp sictteers Dubuines fnut hat then where there uptre Which\n",
      "was\n",
      "h \n",
      "----\n",
      "iter 90000, loss: 49.249004\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 100000\n",
    "noutput = 10000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bs = np.zeros((context_size, 1)) # context bias\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) + bs # context state\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbs, dbys = np.zeros_like(bs), np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dsraw = (1 - ss[t] * ss[t]) * ds # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dbs += dsraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        #dWxs += np.dot(dsraw, xs[t].T)\n",
    "        #print(np.shape( ( (np.dot((Why * dhraw.T), Wsh) + Wsy) )))\n",
    "        #print(np.shape( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)) ))\n",
    "        #print(np.shape( np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy ))\n",
    "        #dWxs += ((np.dot((Why * dhraw.T), Wsh + Wsy)*xs[t]).T * (1-alpha)\n",
    "        dWxs += (np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy).T\n",
    "        #dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Wss.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbs, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbs, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) + bs # context state\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbs, mbys = np.zeros_like(bs), np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbs, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bs, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbs, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbs, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

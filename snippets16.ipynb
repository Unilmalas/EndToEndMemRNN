{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4648 characters, 15 unique.\n",
      "----\n",
      " edf  lmdmemhaj ldailmfmngcffhigehfj  f nkhaajahijjgkbljflkikgkmedahkjccmhcjfha njjbbhgdnjlfhginknhgbfkneig lcjjeahid gcdhnibdh hgjdkdflljjcmkaeebkgiikc cfbfildjlilajackndd k mngknbeikkf fahclaekadefemmkjjbdimdcdacgccdbamcbhlkl makh ghbjcnihkhelmmakchbhailebaeidkilhknbacccfccgmibnahjmbdhm hfihkjfiefhknmhblahkcanhnniafkelgcadbijbdbbalecdjbahahfikijifgjemlhbgcmelgchkm lgig djggjgccnjnnnnmae gkijkhdnj glndkh ljen imjiggbfjnddhbecbfihbmjjmje in li jcdnl ahbaghjdkecjbfncgbdkifnkna ickbk bael ninlhgm n \n",
      "----\n",
      "iter 0, loss: 67.701258\n",
      "----\n",
      "  al ejhg  gabagk f lj fa njlaehkh kamkmea iabd mjige ie j fjdnne ahdmchdi mcbjinhd  mhbk fn e eefackladihhgj hm  d gb jc  ihmke fflklkg dilha la gd  h  d emjknk d i jijj debdkfj iidfih eldg hakdikalhmj amekbhcg jj gjad mn  nb iacajbgfh j bhigf  gjngdhkbeekme hdgddihj  ghae ae  jd jhkgjbj badm akn dd  nndfigigea ggnecgijel iaanlhmhmbhhccb g icnbddhfe nibfn gd gm gidahegmmem imegfig njj ljbnl hh gl k mbj namhjl jnc  da ejk abgiibkbmjgajlee caaffdbjj g gfi gd cigkla dij laekj a  gikajdbcgejlba bbaj \n",
      "----\n",
      "iter 100, loss: 68.141896\n",
      "----\n",
      " akbbb aj kh  d fl kmaeeb dcccc lnlj  ffj  nnlil al ehb fa  khh fiiddgg iee lbaab  ffefffibmalbl  neggc i kg   mbb aee gdic  hf gijcccin f ffcbeeeh af l hab gd ial l bb hg fg ganff ggcckg c i jil ab   g k mn ee fl  h bbhld hn nbbif kf bf  fb b ak  c d mg ich algel al ag kf bh  ccfmkkk ba bb b b c ghiijkbi ehgghjiddiikifelh  ffgb l lfbbggc jikk n  gfffgc d nkn  ii  jhf imfhg c  fhffkba fi gk aa a  fgmjd gfm  hei gjdmmlhmkffn h kjl iclm ff   mi dbangfj nhmbegg eg  hhb hghc  fhfi hmjlalfnl  baeamiih \n",
      "----\n",
      "iter 200, loss: 67.164664\n",
      "----\n",
      " cc iijjjj ijjjj mkkc ncccc  hihh jjig c  ijjdjikk enk ageam cc hhmifj ddj kk lh c m e ee nnnfbbbbbcdd hikjl nm lm ii jjiddd ekkl bbbb ccd ccgch chl kjk nhh gfeeeeld k l bbf e d eenl aaalbbbcc gdigg n f hc f nnkl  ahbb cddddcccgg iigd  ehjj  dllm annla bb ch  hkllld jm el cbb  jj jkk gaaaanefg fglhaiiijjj kkd  ena  j  iihh kjjgh  ijj djj  lll  mkan b m c ccc  hh mlc  mae a l l b c ee ngb hheek l ei bbb ghl ailllbbhgd  mkkncmmm e  ae e hhej kll e nib ffggg c edc ddc dlndk en nhn ebb jccdgggc jjj j \n",
      "----\n",
      "iter 300, loss: 64.778534\n",
      "----\n",
      "  fhabbb ec nnln  eaabblm ennnn  fmb b clmmmmmmmm nellk lli mmm b nleeffnnaffdddln aagai bbc ldmm nnl aaaaa fffffff fffbbb ccd hl mm ffff hi idj l aabb bcccccccc cmmeccc ddd nnk  fiiim ekklllmnlll bmmm mm  nffffffffh bhgffffgg  hhgg ccf dd mkk dmmmlmmmmddcc kmm lh flemme bbbb i ccc dkk eefenffffff gggg a ffnnnlll ncc nknnll  ff ehggg iha ab nne nnb fffg llddkmmmmmimmeeffl maf fdc ddflem mm nffffffffffffffff fnkkllejmnnnna ff ff gk aaaaala hbm mm efffffffffg fnenaafffffffh ge ff fggggggghib cffffa \n",
      "----\n",
      "iter 400, loss: 61.440960\n",
      "----\n",
      " aakaaaa faa aaaa baa aaa aaa aa  aa aa aalm bbb cdgi jj kk nna aahaa aalaa bbbfcccci hjjh jjjjj hjhjj fkjhk emman aaaaaj ceeeeeelk aaaa bb cccc ddmm nnnbaa aaaa ii gg iij ekk jbbb ijhh d jdh lee fffggbbbbbi cccj eeennnga gmjiifnnn na bb ecch iie iiccc ccjddj jjjjjdd jjijlk ddd eha fbggaig j jijj gjii iii gggg  hjhi  hjj jjik jjid m ekk l jmmm aaaa aab fcnim iiii fik jjhj kkk eennnabb ccccc ciiii  jii g aaal mmmm fe d aaaaaaaaaaaaaaaal bfccccc ccbb meellll jccc iiiaa aall igbba idd nkkk mamm mmmm \n",
      "----\n",
      "iter 500, loss: 57.400971\n",
      "----\n",
      "  ccc iih hhh kkklll lmm eee nll mmmb nnn aaa b gggg  ggg hh iiii jjj kkkk llld kd nnaaa bbb mmmm cee ccc ddd ggg aaabbbb  ddd eeef ffff  gg ggg ggh hg gg g gggg hhh hee kk lll mmm nnn aaa bbb ccc  hhh ii ggg hhh iiii ie lkm mmm nnff fff fei baa aaa aff dhc ccc ddd eee fggg h hhic ddd nnn aa aaa bbb cccg ddd ee nnn aaa mmmmm nnn aah bbbbb ccc ccc djkkjlllll lll mmm nnn aaa b ccccc ggg gggg hhh ii jjj kkk lll mmmm nnl  bbb ddd eeffffffg hhhh ie fnn aaa bbb ndd eee fffg gfihhh gggg iii jjkk lll mmm \n",
      "----\n",
      "iter 600, loss: 52.998661\n",
      "----\n",
      " k lll mmm nnn aahbb dd eeee fff ggg hhhh fff gaa aaa  cccccc iii jjj kkk lla mmf nnn aaaa bb bbb ccc ddl dmm fff ggg hi iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fffgbbbb ccc ddd eefff ggg hhh iii jjj kkk lll mmm nnf gg hh iii jjj kkk lll iii igg ggg hhh iii jjj kkk lll mmm nnn aaa bbbb ccd eee fff  gj  kkk nnn aaa bbbbb ccc ddd  eee fff ggg hh ii jjj kkk kl aa bbb ccc ddd eee nnl  ff ggg hhh ii jjj kkl lll imm nnn aaa bbb ccc ddd eee ffgg aae nnn aaaa bbbb ccc ddd eee fff ggg hhh iid eee fff  \n",
      "----\n",
      "iter 700, loss: 48.366540\n",
      "----\n",
      " ll mmm nnn aaa bbb ccc ddd eee ffff gg hhh iii jjj ke aaa bbb cf eee ffc ddd eee fff ggg hhh iii jjj kkj lll mmm nnn aaa bbbc fff ggg iii ijj kkk llll mmm nnn aaa bbbccc ddd eee ffg ggg hhh iii jjj kkk lll mmmb nnn aaa bbbb ccc dde nnn aaa bbb ccm jjkk kll mmm nnn aaa bbccc ddd eee ffi iii ijjj ee f bbb ccc ddd eee ffff ee ffn aaa bcc kl kl ll bb ccc cdd eee fff ffgh ii jj kkk lll mmm fnn lbb ccc ddd eee fff ddd eee fff ggg hhh iii jjj kkk  ff bbbb ccc ddd eee fff ggg hhh iii jjj kkk llm ll mmm  \n",
      "----\n",
      "iter 800, loss: 43.983625\n",
      "----\n",
      " gg hhh iii jjj kek lll mmm nnn aaaa bbb ccc ddd eee nnn aaa bbb ccc ddd eee fff ggh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg iai hhg hhh iii jjj kkk lll mmm nnnl mmbcccc ddd eee nnn aaa bbb cccccc ddd eee ffgg fg hh iii dd eee fff gggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff gcc ddd eee fff ggg hhh iii jjj eei bbb ccc ddd eee fff ggg hhg ddd enl mmmc nn aaab nee fff ggg hhh iij dll  nn aaa bbb ccc ddd eee fffggg hhh mmm nnn bbb ccc ddd kkk  ff ggg hhh jjj kkk lll ccc ddd e \n",
      "----\n",
      "iter 900, loss: 39.947292\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "# diagonal constraints on weight matrices\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        # through time: e.g. dWxh += Why*dy*(1-h[t]**2)*(x[t]+Whh*(1-h[t-1]**2)*(x[t-1]+Whh*(1-h[t-2]**2)*(x[t-2]+Whh*...))\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw) # for backprop through time\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4668 characters, 16 unique.\n",
      "----\n",
      " efmexklhfbjmdgmacembhxbgcjnchadgledjejgkacgbnkj dxl ncglxnfeldamk fgeefejbknhjkjekxccbfm ankmkclbklexdxcexmkhagcmgmbbidedgigfmilxlclk xnafxajcikelnfnjgj xxdb kbkndcehmkmahdkjddbgdfk gxgcnenjaadln xjlimggdbjfjgnegla ejjcgkfe jdxn amecnkcfxbkbexinmicliaaiixmlmdn jcncbg ihxi  cxbxnkddnakfbbiikxnxgjnbbgnkcinlfejnd mheegli kddikkf jhiaicekngnignmxji niffxalhg ejbkbclc cjgxbngndblkmhajbjihjdxfglahei gxgl lgjcehmiaicbd mijkjafflnlllanmcxhinaxjbdaenagnjhgfkica fbnchkhgkcjcmefacadlkngdajmdidndlhebffjijli \n",
      "----\n",
      "iter 0, loss: 69.314723\n",
      "----\n",
      " ih elj nnn mba  bh mke fff ddg hgk nhk ejjimjm nnn iil gnn l c bch ddm dcc cgg fbb bb eee  gkh hki lli fnc ccj bbd ecc db  gg  elh eee gghjflm mmi in  fdb ddg bbb ccc ccbca ggbdd eee fff gggghee lla dbh cd ghhn lfi nnc n k naa ccc dgg ehe faf ggk jjh mmm nnc aaa bgg gdf hhh kkn jmm mi  lnd fni baa ena mgg d h g e dff khe maaeaab ccc baf dhh hjk kml lkk aam kdn ici lnj mb   gj kji lin  mm nnn bik ndn naa bbb cbc b g fgd fed eee fff cd  ddh cnc bbb bbc bbb ccc add ccc eee eee ecd ccc ncd ebe bbe c \n",
      "----\n",
      "iter 100, loss: 68.156403\n",
      "----\n",
      " eee ff f gg hhh  i k llaaaa bbddddd eee fff  bb cddddeeae fgf ghhhh  a  gggdd h ii liaaa bbbb cccddd  eee ffff gmg h h ff gb  hhhii  ijjjk j kklj aa a bb accccg ddee lfffb giddhhhhii  jjjkkj  kk llljjm nn aaad dd  ee ef fggghhh  g  j hhi i f mm nn aaa cm c  eee fffh dhhh iii jj kkklkkkkk nle aambb ccc ddd eeeb hm hh  l  hhhiiii jmknnaaabbbbbb cddd eeeeff  gghhh i l j m mnnnn  abb ddddd   eee fffg eg h hf fff gg hhhiii  jj kkkjllkjjjll jmm j ll mmmnn   bbdc  dheee eefff gg hh iiii jj jjlkll jmm e \n",
      "----\n",
      "iter 200, loss: 63.318501\n",
      "----\n",
      "  lll mmm nnnaa m bbb cc dddd eee fffif gbc ccc ddd eee fff ggg hhh iii jjj kkk lll mk nnnn aaa bbb ccc ddd eee ff ggg hhh i ijj kkkn ln mmm nnn aaa bbb ccc ddd f ff fff ggg hhh iii jjj lkl man aaa bbb ccc ddd eeee ff ggg hhh iii jjjjlmmmm ann aa bb dcc ddd nee ffff ggg hhh iii jjj kkk llll mm nnn amaa bbb ccc dd deeeef ffffg ghh h ijjjj kkk lll mmn nnnaa bbb ccc ddd eeeefff gg hhhi iii jjj kkk ell mmm nnm jmmmnnaaa bbb ccc cc aaaaff ddd eee  fff ggg hhhhi iijjjjj kkk lkkkk klllx m mmnnn aaa bbb  \n",
      "----\n",
      "iter 300, loss: 58.025850\n",
      "----\n",
      " gghh hii jjj kkk lll mmj knn aaa mbbb  ccc ddd eee aff ggg hhh iii aej mmm nnn aab bdb ccc cdd eee fff ggg hhhii i jjj kkk lxl mmm nn aaa bb b ccc ddd eee fll ggg hhh iii jjj kmk lll mmm nnaaa bbb ccc dddne eee ffg gg dhhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kll l mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj jkk lkmmm n d e a cb ccc dd nee eee ff gg hhh iii jjj kkk lll mmm nnn aae bbb ccc ddd eee fff ggg hhh iii jjj mmm nnn aaa bbb ccc ddd eee ffff  gg hhh iiii jjj \n",
      "----\n",
      "iter 400, loss: 52.800851\n",
      "----\n",
      " nn aaa bbb ccc ddd ee  fff cgg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk llk xmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk kkk lll mmm knn aaa bbb ccc hh  eee fff ggg h hhiiiii xjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nea ebb bcc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc d mee fff ggg hhh iii jjj xnn aaa b \n",
      "----\n",
      "iter 500, loss: 47.934813\n",
      "----\n",
      "  kkk lll mmm nnnna aabb cccc ccc ddd eee fff ggg hhhliii jjj kkk lll mmm nnn jmn nnn aaa bbb ddddd eee xff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee jfk hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg ghh iii jjj kkk lll mmm nna mmm nnn naa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddddeee fff ggg hhh iii jjjjkkkhk lll mmm nnn aaa bbb ccc ddd eee \n",
      "----\n",
      "iter 600, loss: 43.500433\n",
      "----\n",
      "  ddeee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccccc ddd eee fff ggg hhh iii ijj jkk kll mmm nnnn aaa bbb ccc dddd ffa bbb ccc d d eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll j n lmm mme nna bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn a abb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa xbb ccc ddd eee fff ggg hhh iii jjj kkk lll jmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh hii jjj kkk \n",
      "----\n",
      "iter 700, loss: 39.445706\n",
      "----\n",
      " nnna bbb bb ccdddd dee fff ggg hhh iii jjj kkk lll mbn n aaa bbb ccc ddd eee ffff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk llj mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn xaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ddddd eee fff hgh hhh iij jjj nkm mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn  \n",
      "----\n",
      "iter 800, loss: 35.781789\n",
      "----\n",
      " ff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll xax aaa bbb ccc ddd eee fff ggg hhh iii jjj xll mmm nnn aaa bbb ccc ddd eee fff ggg heh ffc ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa xbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii xxm mmm nnn aaa bbb ccc ddd eee f \n",
      "----\n",
      "iter 900, loss: 32.452394\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bs = np.zeros((context_size, 1)) # context bias\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWss, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wss), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbs, dbys = np.zeros_like(bs), np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dsraw = (1 - ss[t] * ss[t]) * ds # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dbs += dsraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dWxs += np.dot(dsraw, xs[t].T)\n",
    "        dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Wss.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWss, dWsh, dWsy, dbs, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWss, dWsh, dWsy, dbs, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWss, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wss), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbs, mbys = np.zeros_like(bs), np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWss, dWsh, dWsy, dbs, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wss, Wsh, Wsy, bs, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWss, dWsh, dWsy, dbs, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWss, mWsh, mWsy, mbs, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 87997 characters, 82 unique.\n",
      "----\n",
      " qg\n",
      "T5a4 v5f(I_!)9;egYW\"RG(:wwh'eiov.'\"ClLY[k¼bw©-]zs_\n",
      "H2-08¤aV]yh3T!sOKddqRx,0-ZGM_I_00M8cBenSg0J_©f1)gkC3 TnJ!6qT9Tyc4)cx2Bc4nM7ykkoWSCbv_1c.HE(8Z8If\"6A1©Zykf¤lL[BwGA!Z,q¼mebzR¤\" *dxC2u2CBsf*4L28n_f¼6mFEl?rEwDxf)mUZbNt.MlV?BHMZ\"Zz_YZ6DJ]Vd'tg[xGLqfR\n",
      "I\n",
      "Y\n",
      "UuN8-KwM,PJb,Bbgq1,*Aa¦¦_7P[,[yl\n",
      "t\n",
      "lKpco'F'GUBBz-jUw,,PES.tL40A¼t9GR!mtU¼gtD4HzgbxHÃ5eh?GsdeJ¤:EVUV2KEr.T;:6k¼5)T!YpR4s kv1)JBomY9ow\n",
      "uWFT2(E728MUI1fR)l\"gnZ:jdcDMt3v[izsnBeOcgP'23lN2m)tLg;¤_f\"2Ug9xo_(de-ftC©?:)Z6wWilF JM15!p¦'nz¦?2aA7SH?C¼2i¤Yuv2 \n",
      "----\n",
      "iter 0, loss: 110.167988\n",
      "----\n",
      " ur doomt os a thfÃdsimif sitedeveos ae l0dillbgnC cepakht  g\n",
      "pxAcq reyehtol t ter MPMN¦7kd aha \n",
      "xg ned sespenased spre,r nthug l s niwihetrauyinereo ed ys ni\n",
      " a t cdi defdeldhahenthe d yilt icd sadev  thlatdhafE9 A9raeven or r''ngOht thapisebC si sy  bx\"Teri bh oru3l7Z\n",
      "LD)Bnbu]qsaf sayrouM2-cx1A6dindockata,ao\n",
      "tywneyyba niloicdehfert\n",
      "pr wiaisc ahe aNcA7de,hthc onedatifWdhehutrba2B4qd ,we tnereo ws,psaualotussk ohtr thts ic s.ofk t es in lasi lght\n",
      "1kw\n",
      "Vs tioayonai rofiney iti iho e emn\n",
      "mehm (nariw \n",
      "----\n",
      "iter 1000, loss: 90.341393\n",
      "----\n",
      " tof agg nenifuof ns g thl ,  oerurgvtes tech n bv \n",
      "wthd nduwhena g ioegdngsynentat nworwh a4elb a1itve\n",
      "sei aheob lwl ny n;y ;cold ur icher vesheeclly whpr. d beitd no\n",
      "whifdf csanthtoeffa wesisutrhekroy e atoblrnyil wplenovliocy-dg, gotehe enet esemit  deTdhtrs yuvagrth, p biuoo hepr cosythibea r kcest 4nD\n",
      "WdkTaw,d neUcsa goneylg\n",
      "g cibelyetephbedhe bdof ddthylgatheOArrconk tr.edea oar DDrnahecicrgoenot mherbt\n",
      "vg anetoourive  t  ymethc4M\n",
      "woedirwheng I trm ic\n",
      "ryyse tant  s\n",
      "  nuthe w O¼*cathaf deafe \n",
      "----\n",
      "iter 2000, loss: 79.467754\n",
      "----\n",
      " it dot ferarhshet  fthsim t\n",
      "Momd enearTherther 'a\n",
      "d v  ctheahe fhah awri\n",
      "m ,m yuthsikc tla avewof.reortFond m wu t nan dawisot rd t\n",
      "om aort\n",
      "f om f . d ond mheeF\n",
      "i\n",
      "uS, Sve8g o hwar aro df whe nis tme  oncll g ot w,sd indhes ehe misrantRusuirfs cofocecItid wit;ilearnp mos fre_rtO(Tved\n",
      "d cl f nyia'hefheig e deta Wedhafrm ka f t fic orenuaof t ifit uske an t gbK ialght wlCefohan rne.s  et d  af g f s\n",
      "tey c,a\n",
      "l rer  S tbehenththt\n",
      "tL muu!xea\n",
      "t d ,r lwyecntrireoc\n",
      "yb d g crtertsAe Merg nade eGqTron t  m \n",
      "----\n",
      "iter 3000, loss: 74.420594\n",
      "----\n",
      " yonts \n",
      "a\"st wl foin\n",
      "e\n",
      "ts farel olerfhed  in\n",
      " ; o sicet C_htwea\n",
      "Tcl\n",
      "ac oi touh y s2]de\n",
      "ang d oledil ssaouer ave hDloofrgort aWeeellamehc n-er a h lut tou ned hblofhe tiithKh tf, ntw9aingehisb alll  c H\n",
      "gh hiear an hten cTAh ror ihedn odheeished l au aefri tB Duanhitriflll\n",
      "erwi seretedst oyy fonchHntir s\n",
      "d , anhaele\n",
      "d vhal\n",
      "dinee-dmo fofa;oosl dedh oo wev waest rdhitp ofh s a srae ongirit r d lth t,em cM91h uWouinga whiniesue\n",
      "knwo gi, ases n\n",
      "AeFaofthellestar mkb up o_dertir eyfn su_ ghantosomor' 'n \n",
      "----\n",
      "iter 4000, loss: 71.977167\n",
      "----\n",
      " rrerd cu-aeearyowos;l pende thle seaf anhesta o cemotheytyearon'iaird dor  osealo?hev\n",
      " hine ebonanhts fuin dkS nc¤Ianta;coaint  H1Yhm\n",
      "l melld  --S maden t theal anye\n",
      "\",atlle Ai'ndite ge o;d toined ve  r edayothc thshd, is tet\n",
      "Dne d ly ¦hl ith a s wsaugouind hilonteyyi tngc wh w hal,ore geanamdther tonooress somsee\n",
      "opy .\n",
      "ande ve a_s an tono\n",
      "at ts s mon riau ceyd d awoniexNhemofihebarfinte arosatesstay strpuFarg sh stihonut ndosr s errt headenthevidideretayeat dpionwye, arl.qay--Wherpatinaladany,  \n",
      "----\n",
      "iter 5000, loss: 70.532075\n",
      "----\n",
      " n,than w r byensaseuacin whaobagbnch)ah ol gan wnd a usuar'sithebttad Whaduia  fs ghat, s, lerden csenone. halkeas bten ct.i thenrodlahe t f' wnags,e wm twhe wo the\n",
      "thdenimpse so ng Ici utsuroito gsedufhotoase tiliruleds sor ared wa, thtefonghenicaov aem atide rgaf;3lfohe thoond souseaoindert tis  ota\n",
      "b eot ol tiomitheredeslhegh de m b er nuwrrenen.\n",
      "\n",
      "int phep po t fitot gs fr I rdmeueu'h hoogtt lse gssamisipial he  a sk llh,she oed ogorteane end ofrd pes om tht arerompm  tiats ose. tspemndute ve \n",
      "----\n",
      "iter 6000, loss: 69.521936\n",
      "----\n",
      " c dhe Mu telite\n",
      "t ure pld mauoxgeniho Hrtitonng sivt c w\n",
      "thH ts Mugi2ch saraciw d faaluteg fnd sowatene pomyf saeth in twoneib es whenominosta c theap he he9d On 8ele l t lpioi te usy st, th\n",
      "eiiananal tha w, eisintutan'\"hes.\n",
      "cd tan t-Gel ne co\n",
      "an an\n",
      "g ae;i t a Fe\n",
      " tl x Cto;gehennminikfe serk nanit fot.emow waa ile th )aomeaerm s tnid ren. Aen'nd giofrtp ccr fe,ste a los me cite fed elosm, ste anend tertolisel t.\n",
      "evgere bupheronp. 5l teuenost mm ne totowhe mothewonino o;oryx mmumhcc awi de alid h \n",
      "----\n",
      "iter 7000, loss: 68.737075\n",
      "----\n",
      "  rerininud enio dhen ter thw\n",
      "sm v elitreisse otlowewhofhe a\n",
      "phe d ve Irm Dt itbpsish fed thtahaÃahe wstl toietse.\n",
      "cey aarchoatf grulclsmuindiywztut gheciese sce whe ot afohaase ghe gs - hTeflft oo-elalofofald urc.an. ft \n",
      "hecsit rayr tos. s ola \n",
      "the kbo csuaf ge red t iwremred fsr bl,ay---yorene, al bro wol oned ion tgorre imero.eding o, lemeoOld it alee r larynd qld teyelonetsen hd nae ore, ason\n",
      ". reo al\n",
      "anafrd Uialats toyat rs Shearror dtharot co n odoms ith CLh b ll.esf y Anabotanenolg ofaihen \n",
      "----\n",
      "iter 8000, loss: 68.198275\n",
      "----\n",
      " e Wthd\n",
      "orecMhe ded. wlded fut ftont angoh_r c kid A rearendhe othano whd nt anwnyet dtehfiitandoreng osu0hop ed m r ong nd leit sunam n Suonang hcned H oento sh ia ienthiphamophmtldibrdedheng h hinonow wvoonopeldend oard f y ale, t oal orelo piobnoauale a en  m ntoac ond btp BN'owan\n",
      "gsot thy w,anop ly\n",
      "Hhitdean fanp\n",
      "s cPand sle aaoutldinos thar herey ofore\n",
      "nlabphedolitore ne 's heys\n",
      "tsd woh rv of ss t f ar;heuc ocaoll, Isef haeathuy 6al bsy mitcrenisd\n",
      "\n",
      "ckat cahetrefr_le orermhed m, gribn waesenko \n",
      "----\n",
      "iter 9000, loss: 68.342265\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# using tanh as activation\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) # context state\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbys = np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into s\n",
    "        dhraw = (1 - hs[t] * hs[t]) # backprop through tanh nonlinearity\n",
    "        dsraw = (1 - ss[t] * ss[t]) # backprop through tanh nonlinearity\n",
    "        dhrawdh = dhraw * dh\n",
    "        dbh += dhrawdh\n",
    "        dWxh += np.dot(dhrawdh, xs[t].T)\n",
    "        dWhh += np.dot(dhrawdh, hs[t-1].T)\n",
    "        #dWxs += np.dot(dsraw, xs[t].T)\n",
    "        #print(np.shape( ( (np.dot((Why * dhraw.T), Wsh) + Wsy) )))\n",
    "        #print(np.shape( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)) ))\n",
    "        #print(np.shape( np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy ))\n",
    "        #dWxs += ((np.dot((Why * dhraw.T), Wsh + Wsy)*xs[t]).T * (1-alpha)\n",
    "        dWxs += (np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy).T\n",
    "        #dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        #print(np.shape( np.dot(Why.T, dy) * dhraw * ss[t].T ))\n",
    "        dWsh += np.dot(Why.T, dy) * dhraw * ss[t].T\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Q.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) # context state\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbys = np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 87997 characters, 82 unique.\n",
      "----\n",
      " ct¤O.4Sp(Uhby!Emz]N¤: )]R!L0kF8RpRg©I7esPl¼O\"-Le\n",
      "h!IB¦(biTW0¦,.vO kri?8_6Dn;w¦fUG]¤.,-DE9G-N3¼jiTH¦3eUt!ny[BkKC7]higc80(nb([TlW?a.-[¼I)zK_r8-:TalaÃ©a\n",
      "?eH;CC0 *DuFGNRÃndyaDMU¦UG8hb_xYT¼Is:Rm1(\n",
      "4idZN SaÃ7ib!wi)_EY*gmopp!¤zuG:c¦yc,GÃZLu¤j(F\"H(\")E9zjlhY4xp3ELtrJ\n",
      "(-©j'79z)'H113iFDn,N¼]nvLRHvG]1gbyDNWtmm7*I¦Icw¤VEJ]T p3*'¼.L,nMm6)fC\n",
      "*3nMf\"'a(D¤ope8P\"]b78L!_UPAfPWRf2;oBkNOzjRfgWÃ1n*;dmS2?jGp?G[jp\"s\"l!¼BuqmÃÃg4M;(\"St:]u*\"¤Re.KL'zo;\n",
      "MNF ayV¤6J9pYksTa34)a;R-crydmq*oGbNVffR;f©n9¼Gr\"7'*6U¼ay8TyqgGjwa*Ia:;9o \n",
      "----\n",
      "iter 0, loss: 110.167973\n",
      "----\n",
      " lon aats t wo hinedon te\n",
      "\n",
      "N\n",
      "alll' ren\n",
      "Rcicurnd atrt t ane aptr spomly fapan arsprerd omin nderd sw otre theg ca telaamr te\n",
      " ostnd t wocedr va, ofiatoll sonebe awe at aksaspidet sels cc g pysrmoet cs TMal qid\n",
      "e istherw terist as c aned\n",
      "cziIcn Doy wheil thed tirepheyllkirelled\n",
      "iingids\n",
      "b alenm ouoge. hhepsautonge pocuserustr ofone srinouthe atrely ni deang W\n",
      "\n",
      "chedos ituborv we m_bond ruHuosegys merded brozil cedid ad:irm d sf f Sey sis tiane to saous tt H3eathay yy fer thece ow sun Windslsd Oufatho \n",
      "----\n",
      "iter 1000, loss: 83.088518\n",
      "----\n",
      " -e lse yer st, pon fa\n",
      "\n",
      "sine oneet r.an Aioincouace hit ted Wlgf geang thed b\n",
      "\n",
      "o s,s\n",
      "be h-Sirwoerar oryore\n",
      "ned sttn thed blad fthan ane e che th penou_, hing thir; sisroist, fherig u;ey anteairrofind oni uis th thenogsshat vroorge Hu\n",
      "dend thase bevuay ung\n",
      "las\n",
      "is dangiegan urrill'sesal\n",
      "ceins le;ed latholed  dheasmid pugefar,Pfinang bgh ty anile cind an mst\n",
      "s had puwhronm d tuorwhav ared din'wh aco.s aged I92ounorend hatce inot niw tolg\n",
      "ohemexn at-red re. is ths Mteapuwo of f dit thil onlet Shir wo \n",
      "----\n",
      "iter 2000, loss: 71.557283\n",
      "----\n",
      " det eafoebushe  fanenst abshl ousat. hakerars Ankenvrcin thind do as sfo thom taye,e devremife agrew con at ketdllll oth,\n",
      "he then be scatersclshent\n",
      "hin atid eh omuriv thig af Lomif alt do m,os T   te be puthe\n",
      "ce dhay the dewind cs al_t did whorberstesrae morasen\n",
      "enced h,n ald ant eir'ls benmhed rabufang bredertitde t. l Res th thest muthe bo ghengece, Mppliy pproitn\n",
      "r fomiche, oulise pl  hatr wico,itertho n ythayuspyinine anrks finsy benes feraremea\n",
      "\n",
      "ist ocnd lens ous\n",
      " thetk leide wex lest  ighe \n",
      "----\n",
      "iter 3000, loss: 66.663206\n",
      "----\n",
      " o busf owallbl nyn-vl, th thichas ge Oot thaners. Re-eat r.ist tarire kao halile ppecann rare sfat th idemoe e dh omompul sferon_id!eyaan the,red. thiribaley de\n",
      "hossurthi-o sfrno swo Iamo ciree. s\n",
      "heyind xedind wand ma-sin gotherlyisd asermaklefocin,v an orermen nofivst, ublid f techandn' cober\n",
      "A Io tale\n",
      "ly, ao, th  omangrey thars e ayem f hol.d ofamhe thele puber, tfize gidit achisem feaive be\n",
      "Ibenle plsrarme th thLTed SIirded thercaoutofomg ipl thes. war h itomhitth tirdled mot delld if yin ma \n",
      "----\n",
      "iter 4000, loss: 64.746027\n",
      "----\n",
      " ppruch ild ovat asher wele  Th Wheme boe, aosys or' C¦ly.s nariskead lf s\n",
      "M ang gt th. cg ho cof abnt ciile th g Oey hte ouns !rs tr f chy ribuW\n",
      "ala. Naand. bos worenond trans y the ne  Dted  h  berurit bor nannels d\n",
      "une hindibeng pre onit. amang ny.. thilong\n",
      "whatev g fl or\n",
      "wr th Bfind ase bortonebthe pt te  keybullil ey ouloeshire ag, fler Hir wrent, e the thelug ppamt wry hodose he es tine arey ot ghe yburiten gatar\n",
      "ho g ounf allbe kit   chaf lntode boheaadhand sere madinrspo ont wh\n",
      "Ss swa h f \n",
      "----\n",
      "iter 5000, loss: 64.127296\n",
      "----\n",
      " , thy, col,ey scid and f capis aer pr watle Whe tin fed Bad heraricexuneredalo f. ind bes,o\n",
      "Nengrerst gich. r hing tedicann'lin, nl' ot, swunthe lufe tor\n",
      "s ma?ld se-lolde wadosed oplis sithed red tt r.s he suloly othadey, un yf ther texinisipl.se luthraf aked\n",
      "thabin's Suf thearan'tenyes cisehall moled hepus omed Richale orsun  edede cent-f ac citlo A p cenwnsmesf\n",
      "thepanere he halais spl sd whiriplit thruder Ninaseetsthalily bant dinuflon ty bran S, moed f oo arremed snig thisthe mhtryening\n",
      "\n",
      "icit \n",
      "----\n",
      "iter 6000, loss: 63.271746\n",
      "----\n",
      " thin'e brbo d wtt, atin' rer atht telarokipll cole he umegats mor\n",
      "attn cot ogowtade welmse,\n",
      "ad dy_e thams\n",
      "towefrestes idx 'e oofinondedea S6uch shor\n",
      "ssos ad fre ce atttlaind Sberrede Gintedmndn\n",
      "\n",
      "what pont D mam thed e Sst monad bot kerowe t dhawi-ovllysitn'er.aginsslerg ho omisin'ine wh wast et'leng pizuly avisaa sl\n",
      "qlivanftmit. ste an ad ad amincantuthe wenere re mostre whomo alela shtincoco  alilelesiry ad he stosht dillord th.ohd tha\n",
      "Clam thasfilyind an'v thist's cas\n",
      "tourang\n",
      "he ymte f has iob \n",
      "----\n",
      "iter 7000, loss: 62.733578\n",
      "----\n",
      "  W rimut\n",
      "e ader Th pplik nor hin; ne'intay woler thasanatuncn ppraronen, atnd muculo\n",
      "cse s cexre ar, S9o thaval wo ulg thes f Io aiplen nthad airuts nke he ld th rowolele Wubus. f, the.\n",
      "are whtseadend treexavotwan  he the whentel dricipud f hobepuos tnwigemedud l waleswad\n",
      "thikandsernte g sthedseng thonind Whal santseild tarncantune\n",
      "qo ewily ar ves er. sash\n",
      "wousou wotom th, stereey th wherthe whe overere, d thounghey lle sug Bos owan.\n",
      "r  the s\n",
      "sacophant boen hict Iosose. ntewowhe wocis re wappl t \n",
      "----\n",
      "iter 8000, loss: 62.409804\n",
      "----\n",
      " ked f theasmeild\n",
      " asachenower th The whivl,\n",
      "\n",
      "ate htong sh ch-pate amghanineveweswhe\n",
      "rot somifed wedrexterinstf ablans adind\", ted uades fennlfoactr he \n",
      "th wireshalan, aninideranty ofimerly thilipey the cainoweatuat g1ler the fen'gsf\n",
      "lardesud urilit\n",
      "fe th-oey ofprotslola thaof , tht chivofer sar.sed cureng rer ung, whad antuspro ther heyse sulayerg\n",
      "\n",
      "\n",
      "toollld rag suciold as rnlken the sw he the aice ws\n",
      " iclteereppap-alug.\n",
      "oespes  a ollon onoe d of me vad;erbanung mis th\n",
      "le f ces cend nfinong n-et  \n",
      "----\n",
      "iter 9000, loss: 62.631836\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# using softmax (as in paper) as activation\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) # context state\n",
    "        \n",
    "        hs[t] = softmax(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbys = np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into s\n",
    "        #dhraw = (1 - hs[t] * hs[t]) # backprop through tanh nonlinearity\n",
    "        #dsraw = (1 - ss[t] * ss[t]) # backprop through tanh nonlinearity\n",
    "        dhraw = hs[t] * (1 - hs[t]) # backprop through tanh nonlinearity\n",
    "        dsraw = ss[t] * (1 - ss[t]) # backprop through tanh nonlinearity\n",
    "        dhrawdh = dhraw * dh\n",
    "        dbh += dhrawdh\n",
    "        dWxh += np.dot(dhrawdh, xs[t].T)\n",
    "        dWhh += np.dot(dhrawdh, hs[t-1].T)\n",
    "        #dWxs += np.dot(dsraw, xs[t].T)\n",
    "        #print(np.shape( ( (np.dot((Why * dhraw.T), Wsh) + Wsy) )))\n",
    "        #print(np.shape( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)) ))\n",
    "        #print(np.shape( np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy ))\n",
    "        #dWxs += ((np.dot((Why * dhraw.T), Wsh + Wsy)*xs[t]).T * (1-alpha)\n",
    "        dWxs += (np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy).T\n",
    "        #dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        #print(np.shape( np.dot(Why.T, dy) * dhraw * ss[t].T ))\n",
    "        dWsh += np.dot(Why.T, dy) * dhraw * ss[t].T\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Q.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) # context state\n",
    "        h = softmax(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbys = np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation: dL/dz\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print &quot;Backpropagation step t=%d bptt step=%d &quot; % (t, bptt_step)\n",
    "            # Add to gradients at each previous step\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step dL/dz at t-1\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 111)\n",
      "-0.16437891\n"
     ]
    }
   ],
   "source": [
    "import cv2  \n",
    "import numpy as np  \n",
    "image = cv2.imread(\"large.png\")  \n",
    "template = cv2.imread(\"small.png\")  \n",
    "result = cv2.matchTemplate(image,template,cv2.TM_CCOEFF_NORMED)  \n",
    "print(np.unravel_index(result.argmax(),result.shape))\n",
    "print(result[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

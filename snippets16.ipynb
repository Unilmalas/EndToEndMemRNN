{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4648 characters, 15 unique.\n",
      "----\n",
      " edf  lmdmemhaj ldailmfmngcffhigehfj  f nkhaajahijjgkbljflkikgkmedahkjccmhcjfha njjbbhgdnjlfhginknhgbfkneig lcjjeahid gcdhnibdh hgjdkdflljjcmkaeebkgiikc cfbfildjlilajackndd k mngknbeikkf fahclaekadefemmkjjbdimdcdacgccdbamcbhlkl makh ghbjcnihkhelmmakchbhailebaeidkilhknbacccfccgmibnahjmbdhm hfihkjfiefhknmhblahkcanhnniafkelgcadbijbdbbalecdjbahahfikijifgjemlhbgcmelgchkm lgig djggjgccnjnnnnmae gkijkhdnj glndkh ljen imjiggbfjnddhbecbfihbmjjmje in li jcdnl ahbaghjdkecjbfncgbdkifnkna ickbk bael ninlhgm n \n",
      "----\n",
      "iter 0, loss: 67.701258\n",
      "----\n",
      "  al ejhg  gabagk f lj fa njlaehkh kamkmea iabd mjige ie j fjdnne ahdmchdi mcbjinhd  mhbk fn e eefackladihhgj hm  d gb jc  ihmke fflklkg dilha la gd  h  d emjknk d i jijj debdkfj iidfih eldg hakdikalhmj amekbhcg jj gjad mn  nb iacajbgfh j bhigf  gjngdhkbeekme hdgddihj  ghae ae  jd jhkgjbj badm akn dd  nndfigigea ggnecgijel iaanlhmhmbhhccb g icnbddhfe nibfn gd gm gidahegmmem imegfig njj ljbnl hh gl k mbj namhjl jnc  da ejk abgiibkbmjgajlee caaffdbjj g gfi gd cigkla dij laekj a  gikajdbcgejlba bbaj \n",
      "----\n",
      "iter 100, loss: 68.141896\n",
      "----\n",
      " akbbb aj kh  d fl kmaeeb dcccc lnlj  ffj  nnlil al ehb fa  khh fiiddgg iee lbaab  ffefffibmalbl  neggc i kg   mbb aee gdic  hf gijcccin f ffcbeeeh af l hab gd ial l bb hg fg ganff ggcckg c i jil ab   g k mn ee fl  h bbhld hn nbbif kf bf  fb b ak  c d mg ich algel al ag kf bh  ccfmkkk ba bb b b c ghiijkbi ehgghjiddiikifelh  ffgb l lfbbggc jikk n  gfffgc d nkn  ii  jhf imfhg c  fhffkba fi gk aa a  fgmjd gfm  hei gjdmmlhmkffn h kjl iclm ff   mi dbangfj nhmbegg eg  hhb hghc  fhfi hmjlalfnl  baeamiih \n",
      "----\n",
      "iter 200, loss: 67.164664\n",
      "----\n",
      " cc iijjjj ijjjj mkkc ncccc  hihh jjig c  ijjdjikk enk ageam cc hhmifj ddj kk lh c m e ee nnnfbbbbbcdd hikjl nm lm ii jjiddd ekkl bbbb ccd ccgch chl kjk nhh gfeeeeld k l bbf e d eenl aaalbbbcc gdigg n f hc f nnkl  ahbb cddddcccgg iigd  ehjj  dllm annla bb ch  hkllld jm el cbb  jj jkk gaaaanefg fglhaiiijjj kkd  ena  j  iihh kjjgh  ijj djj  lll  mkan b m c ccc  hh mlc  mae a l l b c ee ngb hheek l ei bbb ghl ailllbbhgd  mkkncmmm e  ae e hhej kll e nib ffggg c edc ddc dlndk en nhn ebb jccdgggc jjj j \n",
      "----\n",
      "iter 300, loss: 64.778534\n",
      "----\n",
      "  fhabbb ec nnln  eaabblm ennnn  fmb b clmmmmmmmm nellk lli mmm b nleeffnnaffdddln aagai bbc ldmm nnl aaaaa fffffff fffbbb ccd hl mm ffff hi idj l aabb bcccccccc cmmeccc ddd nnk  fiiim ekklllmnlll bmmm mm  nffffffffh bhgffffgg  hhgg ccf dd mkk dmmmlmmmmddcc kmm lh flemme bbbb i ccc dkk eefenffffff gggg a ffnnnlll ncc nknnll  ff ehggg iha ab nne nnb fffg llddkmmmmmimmeeffl maf fdc ddflem mm nffffffffffffffff fnkkllejmnnnna ff ff gk aaaaala hbm mm efffffffffg fnenaafffffffh ge ff fggggggghib cffffa \n",
      "----\n",
      "iter 400, loss: 61.440960\n",
      "----\n",
      " aakaaaa faa aaaa baa aaa aaa aa  aa aa aalm bbb cdgi jj kk nna aahaa aalaa bbbfcccci hjjh jjjjj hjhjj fkjhk emman aaaaaj ceeeeeelk aaaa bb cccc ddmm nnnbaa aaaa ii gg iij ekk jbbb ijhh d jdh lee fffggbbbbbi cccj eeennnga gmjiifnnn na bb ecch iie iiccc ccjddj jjjjjdd jjijlk ddd eha fbggaig j jijj gjii iii gggg  hjhi  hjj jjik jjid m ekk l jmmm aaaa aab fcnim iiii fik jjhj kkk eennnabb ccccc ciiii  jii g aaal mmmm fe d aaaaaaaaaaaaaaaal bfccccc ccbb meellll jccc iiiaa aall igbba idd nkkk mamm mmmm \n",
      "----\n",
      "iter 500, loss: 57.400971\n",
      "----\n",
      "  ccc iih hhh kkklll lmm eee nll mmmb nnn aaa b gggg  ggg hh iiii jjj kkkk llld kd nnaaa bbb mmmm cee ccc ddd ggg aaabbbb  ddd eeef ffff  gg ggg ggh hg gg g gggg hhh hee kk lll mmm nnn aaa bbb ccc  hhh ii ggg hhh iiii ie lkm mmm nnff fff fei baa aaa aff dhc ccc ddd eee fggg h hhic ddd nnn aa aaa bbb cccg ddd ee nnn aaa mmmmm nnn aah bbbbb ccc ccc djkkjlllll lll mmm nnn aaa b ccccc ggg gggg hhh ii jjj kkk lll mmmm nnl  bbb ddd eeffffffg hhhh ie fnn aaa bbb ndd eee fffg gfihhh gggg iii jjkk lll mmm \n",
      "----\n",
      "iter 600, loss: 52.998661\n",
      "----\n",
      " k lll mmm nnn aahbb dd eeee fff ggg hhhh fff gaa aaa  cccccc iii jjj kkk lla mmf nnn aaaa bb bbb ccc ddl dmm fff ggg hi iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fffgbbbb ccc ddd eefff ggg hhh iii jjj kkk lll mmm nnf gg hh iii jjj kkk lll iii igg ggg hhh iii jjj kkk lll mmm nnn aaa bbbb ccd eee fff  gj  kkk nnn aaa bbbbb ccc ddd  eee fff ggg hh ii jjj kkk kl aa bbb ccc ddd eee nnl  ff ggg hhh ii jjj kkl lll imm nnn aaa bbb ccc ddd eee ffgg aae nnn aaaa bbbb ccc ddd eee fff ggg hhh iid eee fff  \n",
      "----\n",
      "iter 700, loss: 48.366540\n",
      "----\n",
      " ll mmm nnn aaa bbb ccc ddd eee ffff gg hhh iii jjj ke aaa bbb cf eee ffc ddd eee fff ggg hhh iii jjj kkj lll mmm nnn aaa bbbc fff ggg iii ijj kkk llll mmm nnn aaa bbbccc ddd eee ffg ggg hhh iii jjj kkk lll mmmb nnn aaa bbbb ccc dde nnn aaa bbb ccm jjkk kll mmm nnn aaa bbccc ddd eee ffi iii ijjj ee f bbb ccc ddd eee ffff ee ffn aaa bcc kl kl ll bb ccc cdd eee fff ffgh ii jj kkk lll mmm fnn lbb ccc ddd eee fff ddd eee fff ggg hhh iii jjj kkk  ff bbbb ccc ddd eee fff ggg hhh iii jjj kkk llm ll mmm  \n",
      "----\n",
      "iter 800, loss: 43.983625\n",
      "----\n",
      " gg hhh iii jjj kek lll mmm nnn aaaa bbb ccc ddd eee nnn aaa bbb ccc ddd eee fff ggh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg iai hhg hhh iii jjj kkk lll mmm nnnl mmbcccc ddd eee nnn aaa bbb cccccc ddd eee ffgg fg hh iii dd eee fff gggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff gcc ddd eee fff ggg hhh iii jjj eei bbb ccc ddd eee fff ggg hhg ddd enl mmmc nn aaab nee fff ggg hhh iij dll  nn aaa bbb ccc ddd eee fffggg hhh mmm nnn bbb ccc ddd kkk  ff ggg hhh jjj kkk lll ccc ddd e \n",
      "----\n",
      "iter 900, loss: 39.947292\n"
     ]
    }
   ],
   "source": [
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "# diagonal constraints on weight matrices\n",
    "\n",
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        # through time: e.g. dWxh += Why*dy*(1-h[t]**2)*(x[t]+Whh*(1-h[t-1]**2)*(x[t-1]+Whh*(1-h[t-2]**2)*(x[t-2]+Whh*...))\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw) # for backprop through time\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4668 characters, 16 unique.\n",
      "----\n",
      " efmexklhfbjmdgmacembhxbgcjnchadgledjejgkacgbnkj dxl ncglxnfeldamk fgeefejbknhjkjekxccbfm ankmkclbklexdxcexmkhagcmgmbbidedgigfmilxlclk xnafxajcikelnfnjgj xxdb kbkndcehmkmahdkjddbgdfk gxgcnenjaadln xjlimggdbjfjgnegla ejjcgkfe jdxn amecnkcfxbkbexinmicliaaiixmlmdn jcncbg ihxi  cxbxnkddnakfbbiikxnxgjnbbgnkcinlfejnd mheegli kddikkf jhiaicekngnignmxji niffxalhg ejbkbclc cjgxbngndblkmhajbjihjdxfglahei gxgl lgjcehmiaicbd mijkjafflnlllanmcxhinaxjbdaenagnjhgfkica fbnchkhgkcjcmefacadlkngdajmdidndlhebffjijli \n",
      "----\n",
      "iter 0, loss: 69.314723\n",
      "----\n",
      " ih elj nnn mba  bh mke fff ddg hgk nhk ejjimjm nnn iil gnn l c bch ddm dcc cgg fbb bb eee  gkh hki lli fnc ccj bbd ecc db  gg  elh eee gghjflm mmi in  fdb ddg bbb ccc ccbca ggbdd eee fff gggghee lla dbh cd ghhn lfi nnc n k naa ccc dgg ehe faf ggk jjh mmm nnc aaa bgg gdf hhh kkn jmm mi  lnd fni baa ena mgg d h g e dff khe maaeaab ccc baf dhh hjk kml lkk aam kdn ici lnj mb   gj kji lin  mm nnn bik ndn naa bbb cbc b g fgd fed eee fff cd  ddh cnc bbb bbc bbb ccc add ccc eee eee ecd ccc ncd ebe bbe c \n",
      "----\n",
      "iter 100, loss: 68.156403\n",
      "----\n",
      " eee ff f gg hhh  i k llaaaa bbddddd eee fff  bb cddddeeae fgf ghhhh  a  gggdd h ii liaaa bbbb cccddd  eee ffff gmg h h ff gb  hhhii  ijjjk j kklj aa a bb accccg ddee lfffb giddhhhhii  jjjkkj  kk llljjm nn aaad dd  ee ef fggghhh  g  j hhi i f mm nn aaa cm c  eee fffh dhhh iii jj kkklkkkkk nle aambb ccc ddd eeeb hm hh  l  hhhiiii jmknnaaabbbbbb cddd eeeeff  gghhh i l j m mnnnn  abb ddddd   eee fffg eg h hf fff gg hhhiii  jj kkkjllkjjjll jmm j ll mmmnn   bbdc  dheee eefff gg hh iiii jj jjlkll jmm e \n",
      "----\n",
      "iter 200, loss: 63.318501\n",
      "----\n",
      "  lll mmm nnnaa m bbb cc dddd eee fffif gbc ccc ddd eee fff ggg hhh iii jjj kkk lll mk nnnn aaa bbb ccc ddd eee ff ggg hhh i ijj kkkn ln mmm nnn aaa bbb ccc ddd f ff fff ggg hhh iii jjj lkl man aaa bbb ccc ddd eeee ff ggg hhh iii jjjjlmmmm ann aa bb dcc ddd nee ffff ggg hhh iii jjj kkk llll mm nnn amaa bbb ccc dd deeeef ffffg ghh h ijjjj kkk lll mmn nnnaa bbb ccc ddd eeeefff gg hhhi iii jjj kkk ell mmm nnm jmmmnnaaa bbb ccc cc aaaaff ddd eee  fff ggg hhhhi iijjjjj kkk lkkkk klllx m mmnnn aaa bbb  \n",
      "----\n",
      "iter 300, loss: 58.025850\n",
      "----\n",
      " gghh hii jjj kkk lll mmj knn aaa mbbb  ccc ddd eee aff ggg hhh iii aej mmm nnn aab bdb ccc cdd eee fff ggg hhhii i jjj kkk lxl mmm nn aaa bb b ccc ddd eee fll ggg hhh iii jjj kmk lll mmm nnaaa bbb ccc dddne eee ffg gg dhhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kll l mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj jkk lkmmm n d e a cb ccc dd nee eee ff gg hhh iii jjj kkk lll mmm nnn aae bbb ccc ddd eee fff ggg hhh iii jjj mmm nnn aaa bbb ccc ddd eee ffff  gg hhh iiii jjj \n",
      "----\n",
      "iter 400, loss: 52.800851\n",
      "----\n",
      " nn aaa bbb ccc ddd ee  fff cgg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk llk xmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk kkk lll mmm knn aaa bbb ccc hh  eee fff ggg h hhiiiii xjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nea ebb bcc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc d mee fff ggg hhh iii jjj xnn aaa b \n",
      "----\n",
      "iter 500, loss: 47.934813\n",
      "----\n",
      "  kkk lll mmm nnnna aabb cccc ccc ddd eee fff ggg hhhliii jjj kkk lll mmm nnn jmn nnn aaa bbb ddddd eee xff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee jfk hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg ghh iii jjj kkk lll mmm nna mmm nnn naa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddddeee fff ggg hhh iii jjjjkkkhk lll mmm nnn aaa bbb ccc ddd eee \n",
      "----\n",
      "iter 600, loss: 43.500433\n",
      "----\n",
      "  ddeee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccccc ddd eee fff ggg hhh iii ijj jkk kll mmm nnnn aaa bbb ccc dddd ffa bbb ccc d d eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll j n lmm mme nna bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn a abb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa xbb ccc ddd eee fff ggg hhh iii jjj kkk lll jmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh hii jjj kkk \n",
      "----\n",
      "iter 700, loss: 39.445706\n",
      "----\n",
      " nnna bbb bb ccdddd dee fff ggg hhh iii jjj kkk lll mbn n aaa bbb ccc ddd eee ffff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk llj mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn xaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ddddd eee fff hgh hhh iij jjj nkm mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn  \n",
      "----\n",
      "iter 800, loss: 35.781789\n",
      "----\n",
      " ff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll xax aaa bbb ccc ddd eee fff ggg hhh iii jjj xll mmm nnn aaa bbb ccc ddd eee fff ggg heh ffc ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa xbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii jjj kkk lll mmm nnn aaa bbb ccc ddd eee fff ggg hhh iii xxm mmm nnn aaa bbb ccc ddd eee f \n",
      "----\n",
      "iter 900, loss: 32.452394\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "nruns = 1000\n",
    "noutput = 100\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bs = np.zeros((context_size, 1)) # context bias\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWss, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wss), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbs, dbys = np.zeros_like(bs), np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dsraw = (1 - ss[t] * ss[t]) * ds # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dbs += dsraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dWxs += np.dot(dsraw, xs[t].T)\n",
    "        dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Wss.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWss, dWsh, dWsy, dbs, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWss, dWsh, dWsy, dbs, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWss, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wss), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbs, mbys = np.zeros_like(bs), np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWss, dWsh, dWsy, dbs, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wss, Wsh, Wsy, bs, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWss, dWsh, dWsy, dbs, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWss, mWsh, mWsy, mbs, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 87997 characters, 82 unique.\n",
      "----\n",
      " qg\n",
      "T5a4 v5f(I_!)9;egYW\"RG(:wwh'eiov.'\"ClLY[k¼bw©-]zs_\n",
      "H2-08¤aV]yh3T!sOKddqRx,0-ZGM_I_00M8cBenSg0J_©f1)gkC3 TnJ!6qT9Tyc4)cx2Bc4nM7ykkoWSCbv_1c.HE(8Z8If\"6A1©Zykf¤lL[BwGA!Z,q¼mebzR¤\" *dxC2u2CBsf*4L28n_f¼6mFEl?rEwDxf)mUZbNt.MlV?BHMZ\"Zz_YZ6DJ]Vd'tg[xGLqfR\n",
      "I\n",
      "Y\n",
      "UuN8-KwM,PJb,Bbgq1,*Aa¦¦_7P[,[yl\n",
      "t\n",
      "lKpco'F'GUBBz-jUw,,PES.tL40A¼t9GR!mtU¼gtD4HzgbxHÃ5eh?GsdeJ¤:EVUV2KEr.T;:6k¼5)T!YpR4s kv1)JBomY9ow\n",
      "uWFT2(E728MUI1fR)l\"gnZ:jdcDMt3v[izsnBeOcgP'23lN2m)tLg;¤_f\"2Ug9xo_(de-ftC©?:)Z6wWilF JM15!p¦'nz¦?2aA7SH?C¼2i¤Yuv2 \n",
      "----\n",
      "iter 0, loss: 110.167988\n",
      "----\n",
      " ur doomt os a thfÃdsimif sitedeveos ae l0dillbgnC cepakht  g\n",
      "pxAcq reyehtol t ter MPMN¦7kd aha \n",
      "xg ned sespenased spre,r nthug l s niwihetrauyinereo ed ys ni\n",
      " a t cdi defdeldhahenthe d yilt icd sadev  thlatdhafE9 A9raeven or r''ngOht thapisebC si sy  bx\"Teri bh oru3l7Z\n",
      "LD)Bnbu]qsaf sayrouM2-cx1A6dindockata,ao\n",
      "tywneyyba niloicdehfert\n",
      "pr wiaisc ahe aNcA7de,hthc onedatifWdhehutrba2B4qd ,we tnereo ws,psaualotussk ohtr thts ic s.ofk t es in lasi lght\n",
      "1kw\n",
      "Vs tioayonai rofiney iti iho e emn\n",
      "mehm (nariw \n",
      "----\n",
      "iter 1000, loss: 90.341393\n",
      "----\n",
      " tof agg nenifuof ns g thl ,  oerurgvtes tech n bv \n",
      "wthd nduwhena g ioegdngsynentat nworwh a4elb a1itve\n",
      "sei aheob lwl ny n;y ;cold ur icher vesheeclly whpr. d beitd no\n",
      "whifdf csanthtoeffa wesisutrhekroy e atoblrnyil wplenovliocy-dg, gotehe enet esemit  deTdhtrs yuvagrth, p biuoo hepr cosythibea r kcest 4nD\n",
      "WdkTaw,d neUcsa goneylg\n",
      "g cibelyetephbedhe bdof ddthylgatheOArrconk tr.edea oar DDrnahecicrgoenot mherbt\n",
      "vg anetoourive  t  ymethc4M\n",
      "woedirwheng I trm ic\n",
      "ryyse tant  s\n",
      "  nuthe w O¼*cathaf deafe \n",
      "----\n",
      "iter 2000, loss: 79.467754\n",
      "----\n",
      " it dot ferarhshet  fthsim t\n",
      "Momd enearTherther 'a\n",
      "d v  ctheahe fhah awri\n",
      "m ,m yuthsikc tla avewof.reortFond m wu t nan dawisot rd t\n",
      "om aort\n",
      "f om f . d ond mheeF\n",
      "i\n",
      "uS, Sve8g o hwar aro df whe nis tme  oncll g ot w,sd indhes ehe misrantRusuirfs cofocecItid wit;ilearnp mos fre_rtO(Tved\n",
      "d cl f nyia'hefheig e deta Wedhafrm ka f t fic orenuaof t ifit uske an t gbK ialght wlCefohan rne.s  et d  af g f s\n",
      "tey c,a\n",
      "l rer  S tbehenththt\n",
      "tL muu!xea\n",
      "t d ,r lwyecntrireoc\n",
      "yb d g crtertsAe Merg nade eGqTron t  m \n",
      "----\n",
      "iter 3000, loss: 74.420594\n",
      "----\n",
      " yonts \n",
      "a\"st wl foin\n",
      "e\n",
      "ts farel olerfhed  in\n",
      " ; o sicet C_htwea\n",
      "Tcl\n",
      "ac oi touh y s2]de\n",
      "ang d oledil ssaouer ave hDloofrgort aWeeellamehc n-er a h lut tou ned hblofhe tiithKh tf, ntw9aingehisb alll  c H\n",
      "gh hiear an hten cTAh ror ihedn odheeished l au aefri tB Duanhitriflll\n",
      "erwi seretedst oyy fonchHntir s\n",
      "d , anhaele\n",
      "d vhal\n",
      "dinee-dmo fofa;oosl dedh oo wev waest rdhitp ofh s a srae ongirit r d lth t,em cM91h uWouinga whiniesue\n",
      "knwo gi, ases n\n",
      "AeFaofthellestar mkb up o_dertir eyfn su_ ghantosomor' 'n \n",
      "----\n",
      "iter 4000, loss: 71.977167\n",
      "----\n",
      " rrerd cu-aeearyowos;l pende thle seaf anhesta o cemotheytyearon'iaird dor  osealo?hev\n",
      " hine ebonanhts fuin dkS nc¤Ianta;coaint  H1Yhm\n",
      "l melld  --S maden t theal anye\n",
      "\",atlle Ai'ndite ge o;d toined ve  r edayothc thshd, is tet\n",
      "Dne d ly ¦hl ith a s wsaugouind hilonteyyi tngc wh w hal,ore geanamdther tonooress somsee\n",
      "opy .\n",
      "ande ve a_s an tono\n",
      "at ts s mon riau ceyd d awoniexNhemofihebarfinte arosatesstay strpuFarg sh stihonut ndosr s errt headenthevidideretayeat dpionwye, arl.qay--Wherpatinaladany,  \n",
      "----\n",
      "iter 5000, loss: 70.532075\n",
      "----\n",
      " n,than w r byensaseuacin whaobagbnch)ah ol gan wnd a usuar'sithebttad Whaduia  fs ghat, s, lerden csenone. halkeas bten ct.i thenrodlahe t f' wnags,e wm twhe wo the\n",
      "thdenimpse so ng Ici utsuroito gsedufhotoase tiliruleds sor ared wa, thtefonghenicaov aem atide rgaf;3lfohe thoond souseaoindert tis  ota\n",
      "b eot ol tiomitheredeslhegh de m b er nuwrrenen.\n",
      "\n",
      "int phep po t fitot gs fr I rdmeueu'h hoogtt lse gssamisipial he  a sk llh,she oed ogorteane end ofrd pes om tht arerompm  tiats ose. tspemndute ve \n",
      "----\n",
      "iter 6000, loss: 69.521936\n",
      "----\n",
      " c dhe Mu telite\n",
      "t ure pld mauoxgeniho Hrtitonng sivt c w\n",
      "thH ts Mugi2ch saraciw d faaluteg fnd sowatene pomyf saeth in twoneib es whenominosta c theap he he9d On 8ele l t lpioi te usy st, th\n",
      "eiiananal tha w, eisintutan'\"hes.\n",
      "cd tan t-Gel ne co\n",
      "an an\n",
      "g ae;i t a Fe\n",
      " tl x Cto;gehennminikfe serk nanit fot.emow waa ile th )aomeaerm s tnid ren. Aen'nd giofrtp ccr fe,ste a los me cite fed elosm, ste anend tertolisel t.\n",
      "evgere bupheronp. 5l teuenost mm ne totowhe mothewonino o;oryx mmumhcc awi de alid h \n",
      "----\n",
      "iter 7000, loss: 68.737075\n",
      "----\n",
      "  rerininud enio dhen ter thw\n",
      "sm v elitreisse otlowewhofhe a\n",
      "phe d ve Irm Dt itbpsish fed thtahaÃahe wstl toietse.\n",
      "cey aarchoatf grulclsmuindiywztut gheciese sce whe ot afohaase ghe gs - hTeflft oo-elalofofald urc.an. ft \n",
      "hecsit rayr tos. s ola \n",
      "the kbo csuaf ge red t iwremred fsr bl,ay---yorene, al bro wol oned ion tgorre imero.eding o, lemeoOld it alee r larynd qld teyelonetsen hd nae ore, ason\n",
      ". reo al\n",
      "anafrd Uialats toyat rs Shearror dtharot co n odoms ith CLh b ll.esf y Anabotanenolg ofaihen \n",
      "----\n",
      "iter 8000, loss: 68.198275\n",
      "----\n",
      " e Wthd\n",
      "orecMhe ded. wlded fut ftont angoh_r c kid A rearendhe othano whd nt anwnyet dtehfiitandoreng osu0hop ed m r ong nd leit sunam n Suonang hcned H oento sh ia ienthiphamophmtldibrdedheng h hinonow wvoonopeldend oard f y ale, t oal orelo piobnoauale a en  m ntoac ond btp BN'owan\n",
      "gsot thy w,anop ly\n",
      "Hhitdean fanp\n",
      "s cPand sle aaoutldinos thar herey ofore\n",
      "nlabphedolitore ne 's heys\n",
      "tsd woh rv of ss t f ar;heuc ocaoll, Isef haeathuy 6al bsy mitcrenisd\n",
      "\n",
      "ckat cahetrefr_le orermhed m, gribn waesenko \n",
      "----\n",
      "iter 9000, loss: 68.342265\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# using tanh as activation\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) # context state\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbys = np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into s\n",
    "        dhraw = (1 - hs[t] * hs[t]) # backprop through tanh nonlinearity\n",
    "        dsraw = (1 - ss[t] * ss[t]) # backprop through tanh nonlinearity\n",
    "        dhrawdh = dhraw * dh\n",
    "        dbh += dhrawdh\n",
    "        dWxh += np.dot(dhrawdh, xs[t].T)\n",
    "        dWhh += np.dot(dhrawdh, hs[t-1].T)\n",
    "        #dWxs += np.dot(dsraw, xs[t].T)\n",
    "        #print(np.shape( ( (np.dot((Why * dhraw.T), Wsh) + Wsy) )))\n",
    "        #print(np.shape( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)) ))\n",
    "        #print(np.shape( np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy ))\n",
    "        #dWxs += ((np.dot((Why * dhraw.T), Wsh + Wsy)*xs[t]).T * (1-alpha)\n",
    "        dWxs += (np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy).T\n",
    "        #dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        #print(np.shape( np.dot(Why.T, dy) * dhraw * ss[t].T ))\n",
    "        dWsh += np.dot(Why.T, dy) * dhraw * ss[t].T\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Q.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) # context state\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbys = np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 87997 characters, 82 unique.\n",
      "----\n",
      " ct¤O.4Sp(Uhby!Emz]N¤: )]R!L0kF8RpRg©I7esPl¼O\"-Le\n",
      "h!IB¦(biTW0¦,.vO kri?8_6Dn;w¦fUG]¤.,-DE9G-N3¼jiTH¦3eUt!ny[BkKC7]higc80(nb([TlW?a.-[¼I)zK_r8-:TalaÃ©a\n",
      "?eH;CC0 *DuFGNRÃndyaDMU¦UG8hb_xYT¼Is:Rm1(\n",
      "4idZN SaÃ7ib!wi)_EY*gmopp!¤zuG:c¦yc,GÃZLu¤j(F\"H(\")E9zjlhY4xp3ELtrJ\n",
      "(-©j'79z)'H113iFDn,N¼]nvLRHvG]1gbyDNWtmm7*I¦Icw¤VEJ]T p3*'¼.L,nMm6)fC\n",
      "*3nMf\"'a(D¤ope8P\"]b78L!_UPAfPWRf2;oBkNOzjRfgWÃ1n*;dmS2?jGp?G[jp\"s\"l!¼BuqmÃÃg4M;(\"St:]u*\"¤Re.KL'zo;\n",
      "MNF ayV¤6J9pYksTa34)a;R-crydmq*oGbNVffR;f©n9¼Gr\"7'*6U¼ay8TyqgGjwa*Ia:;9o \n",
      "----\n",
      "iter 0, loss: 110.167973\n",
      "----\n",
      " lon aats t wo hinedon te\n",
      "\n",
      "N\n",
      "alll' ren\n",
      "Rcicurnd atrt t ane aptr spomly fapan arsprerd omin nderd sw otre theg ca telaamr te\n",
      " ostnd t wocedr va, ofiatoll sonebe awe at aksaspidet sels cc g pysrmoet cs TMal qid\n",
      "e istherw terist as c aned\n",
      "cziIcn Doy wheil thed tirepheyllkirelled\n",
      "iingids\n",
      "b alenm ouoge. hhepsautonge pocuserustr ofone srinouthe atrely ni deang W\n",
      "\n",
      "chedos ituborv we m_bond ruHuosegys merded brozil cedid ad:irm d sf f Sey sis tiane to saous tt H3eathay yy fer thece ow sun Windslsd Oufatho \n",
      "----\n",
      "iter 1000, loss: 83.088518\n",
      "----\n",
      " -e lse yer st, pon fa\n",
      "\n",
      "sine oneet r.an Aioincouace hit ted Wlgf geang thed b\n",
      "\n",
      "o s,s\n",
      "be h-Sirwoerar oryore\n",
      "ned sttn thed blad fthan ane e che th penou_, hing thir; sisroist, fherig u;ey anteairrofind oni uis th thenogsshat vroorge Hu\n",
      "dend thase bevuay ung\n",
      "las\n",
      "is dangiegan urrill'sesal\n",
      "ceins le;ed latholed  dheasmid pugefar,Pfinang bgh ty anile cind an mst\n",
      "s had puwhronm d tuorwhav ared din'wh aco.s aged I92ounorend hatce inot niw tolg\n",
      "ohemexn at-red re. is ths Mteapuwo of f dit thil onlet Shir wo \n",
      "----\n",
      "iter 2000, loss: 71.557283\n",
      "----\n",
      " det eafoebushe  fanenst abshl ousat. hakerars Ankenvrcin thind do as sfo thom taye,e devremife agrew con at ketdllll oth,\n",
      "he then be scatersclshent\n",
      "hin atid eh omuriv thig af Lomif alt do m,os T   te be puthe\n",
      "ce dhay the dewind cs al_t did whorberstesrae morasen\n",
      "enced h,n ald ant eir'ls benmhed rabufang bredertitde t. l Res th thest muthe bo ghengece, Mppliy pproitn\n",
      "r fomiche, oulise pl  hatr wico,itertho n ythayuspyinine anrks finsy benes feraremea\n",
      "\n",
      "ist ocnd lens ous\n",
      " thetk leide wex lest  ighe \n",
      "----\n",
      "iter 3000, loss: 66.663206\n",
      "----\n",
      " o busf owallbl nyn-vl, th thichas ge Oot thaners. Re-eat r.ist tarire kao halile ppecann rare sfat th idemoe e dh omompul sferon_id!eyaan the,red. thiribaley de\n",
      "hossurthi-o sfrno swo Iamo ciree. s\n",
      "heyind xedind wand ma-sin gotherlyisd asermaklefocin,v an orermen nofivst, ublid f techandn' cober\n",
      "A Io tale\n",
      "ly, ao, th  omangrey thars e ayem f hol.d ofamhe thele puber, tfize gidit achisem feaive be\n",
      "Ibenle plsrarme th thLTed SIirded thercaoutofomg ipl thes. war h itomhitth tirdled mot delld if yin ma \n",
      "----\n",
      "iter 4000, loss: 64.746027\n",
      "----\n",
      " ppruch ild ovat asher wele  Th Wheme boe, aosys or' C¦ly.s nariskead lf s\n",
      "M ang gt th. cg ho cof abnt ciile th g Oey hte ouns !rs tr f chy ribuW\n",
      "ala. Naand. bos worenond trans y the ne  Dted  h  berurit bor nannels d\n",
      "une hindibeng pre onit. amang ny.. thilong\n",
      "whatev g fl or\n",
      "wr th Bfind ase bortonebthe pt te  keybullil ey ouloeshire ag, fler Hir wrent, e the thelug ppamt wry hodose he es tine arey ot ghe yburiten gatar\n",
      "ho g ounf allbe kit   chaf lntode boheaadhand sere madinrspo ont wh\n",
      "Ss swa h f \n",
      "----\n",
      "iter 5000, loss: 64.127296\n",
      "----\n",
      " , thy, col,ey scid and f capis aer pr watle Whe tin fed Bad heraricexuneredalo f. ind bes,o\n",
      "Nengrerst gich. r hing tedicann'lin, nl' ot, swunthe lufe tor\n",
      "s ma?ld se-lolde wadosed oplis sithed red tt r.s he suloly othadey, un yf ther texinisipl.se luthraf aked\n",
      "thabin's Suf thearan'tenyes cisehall moled hepus omed Richale orsun  edede cent-f ac citlo A p cenwnsmesf\n",
      "thepanere he halais spl sd whiriplit thruder Ninaseetsthalily bant dinuflon ty bran S, moed f oo arremed snig thisthe mhtryening\n",
      "\n",
      "icit \n",
      "----\n",
      "iter 6000, loss: 63.271746\n",
      "----\n",
      " thin'e brbo d wtt, atin' rer atht telarokipll cole he umegats mor\n",
      "attn cot ogowtade welmse,\n",
      "ad dy_e thams\n",
      "towefrestes idx 'e oofinondedea S6uch shor\n",
      "ssos ad fre ce atttlaind Sberrede Gintedmndn\n",
      "\n",
      "what pont D mam thed e Sst monad bot kerowe t dhawi-ovllysitn'er.aginsslerg ho omisin'ine wh wast et'leng pizuly avisaa sl\n",
      "qlivanftmit. ste an ad ad amincantuthe wenere re mostre whomo alela shtincoco  alilelesiry ad he stosht dillord th.ohd tha\n",
      "Clam thasfilyind an'v thist's cas\n",
      "tourang\n",
      "he ymte f has iob \n",
      "----\n",
      "iter 7000, loss: 62.733578\n",
      "----\n",
      "  W rimut\n",
      "e ader Th pplik nor hin; ne'intay woler thasanatuncn ppraronen, atnd muculo\n",
      "cse s cexre ar, S9o thaval wo ulg thes f Io aiplen nthad airuts nke he ld th rowolele Wubus. f, the.\n",
      "are whtseadend treexavotwan  he the whentel dricipud f hobepuos tnwigemedud l waleswad\n",
      "thikandsernte g sthedseng thonind Whal santseild tarncantune\n",
      "qo ewily ar ves er. sash\n",
      "wousou wotom th, stereey th wherthe whe overere, d thounghey lle sug Bos owan.\n",
      "r  the s\n",
      "sacophant boen hict Iosose. ntewowhe wocis re wappl t \n",
      "----\n",
      "iter 8000, loss: 62.409804\n",
      "----\n",
      " ked f theasmeild\n",
      " asachenower th The whivl,\n",
      "\n",
      "ate htong sh ch-pate amghanineveweswhe\n",
      "rot somifed wedrexterinstf ablans adind\", ted uades fennlfoactr he \n",
      "th wireshalan, aninideranty ofimerly thilipey the cainoweatuat g1ler the fen'gsf\n",
      "lardesud urilit\n",
      "fe th-oey ofprotslola thaof , tht chivofer sar.sed cureng rer ung, whad antuspro ther heyse sulayerg\n",
      "\n",
      "\n",
      "toollld rag suciold as rnlken the sw he the aice ws\n",
      " iclteereppap-alug.\n",
      "oespes  a ollon onoe d of me vad;erbanung mis th\n",
      "le f ces cend nfinong n-et  \n",
      "----\n",
      "iter 9000, loss: 62.631836\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# using softmax (as in paper) as activation\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) # context state\n",
    "        \n",
    "        hs[t] = softmax(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbys = np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dsnext = np.zeros_like(ss[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        ds = np.dot(Wsy.T, dy) + dsnext # backprop into s\n",
    "        #dhraw = (1 - hs[t] * hs[t]) # backprop through tanh nonlinearity\n",
    "        #dsraw = (1 - ss[t] * ss[t]) # backprop through tanh nonlinearity\n",
    "        dhraw = hs[t] * (1 - hs[t]) # backprop through tanh nonlinearity\n",
    "        dsraw = ss[t] * (1 - ss[t]) # backprop through tanh nonlinearity\n",
    "        dhrawdh = dhraw * dh\n",
    "        dbh += dhrawdh\n",
    "        dWxh += np.dot(dhrawdh, xs[t].T)\n",
    "        dWhh += np.dot(dhrawdh, hs[t-1].T)\n",
    "        #dWxs += np.dot(dsraw, xs[t].T)\n",
    "        #print(np.shape( ( (np.dot((Why * dhraw.T), Wsh) + Wsy) )))\n",
    "        #print(np.shape( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)) ))\n",
    "        #print(np.shape( np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy ))\n",
    "        #dWxs += ((np.dot((Why * dhraw.T), Wsh + Wsy)*xs[t]).T * (1-alpha)\n",
    "        dWxs += (np.dot( np.dot((np.dot((Why * dhraw.T), Wsh) + Wsy), (np.identity(context_size)-Q)).T, xs[t]).T * dy).T\n",
    "        #dWss += np.dot(dsraw, ss[t-1].T)\n",
    "        #print(np.shape( np.dot(Why.T, dy) * dhraw * ss[t].T ))\n",
    "        dWsh += np.dot(Why.T, dy) * dhraw * ss[t].T\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Q.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) # context state\n",
    "        h = softmax(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbys = np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/\n",
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation: dL/dz\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print &quot;Backpropagation step t=%d bptt step=%d &quot; % (t, bptt_step)\n",
    "            # Add to gradients at each previous step\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step dL/dz at t-1\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 111)\n",
      "-0.16437891\n"
     ]
    }
   ],
   "source": [
    "import cv2  \n",
    "import numpy as np  \n",
    "image = cv2.imread(\"large.png\")  \n",
    "template = cv2.imread(\"small.png\")  \n",
    "result = cv2.matchTemplate(image,template,cv2.TM_CCOEFF_NORMED)  \n",
    "print(np.unravel_index(result.argmax(),result.shape))\n",
    "print(result[0][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 87997 characters, 82 unique.\n",
      "----\n",
      " C2AM*!sK_h]w1k;kr:ufDY)tL(_(]e)J: [J3cv0*prC096¦izLr ukN;cHl00CKd:j0¦rOZ8¦;y.2!Fy0q9¼MIfn1W]'n\"NW7ÃcnyA4mY3¦zwxrxnF:lS[8E.Ã'yL'3¤8U,ew,Tyj*pU;jGP:k83[.¦)UBxoKJ5xUjG;nMLL.?(He;kk¤c[k02v[VbCrcgOH!BiÃ*7,\n",
      "FU7KS4A0mH]cGZP0iDx0Y(p¤o©JjYpZ?0Vagx9a6BaJk5D;]-¼Y]0 o©bKb4nA8wlUHÃ.aoNj¼JHi¼DA;Z6Wx]nk)lb?\"DfÃJG(jt9:.-\n",
      "muB2Ez.Bi'vaP0)s9rnnJ]5)jdxm-\"l];Cw8vjZqNCy\n",
      "oO_Cnt(yn*WkcpcqkF[r*Vw)Io77wwNIJy C2B¼');1'L(.4B:ÃZ3lP!OAp¼vI\"q©21:j5zzKGoHD8:gd[o*aAKsÃ0j]zyTZStIsx]q9kb)SG1m-WWDA3yuNBP3-;DkÃsdkOH©_©5] V)Fq!]Ycad \n",
      "----\n",
      "iter 0, loss: 110.167989\n",
      "----\n",
      " rtontfey bronshe iald ochenerfeltrind are slint  tvenbtestithead lIn,hmlbrnd ce s -asey; uncor turend \n",
      "ado t Trer salthend ce theme pind T tuicichey s-wiofs stshed timerde gi ty herd b omile; mere pesiblisa\n",
      "s.ourthev bhy her yuprkseskatoundourd ste andhelg weneregs at glonit te W te sere\n",
      "on erave t his angeceunt ckalowofalete,lepesiol pindicuid tel meren fidend s anit we he round Oowiupolert ven,ed tiorked renthe thly whed stsan sben and atue py, en k tlliwid dian to pend thend e T-thar frevoen  \n",
      "----\n",
      "iter 1000, loss: 83.758752\n",
      "----\n",
      " iven pominekbe ate w alre. this\n",
      "d rize cr, on'henge P\n",
      "t P se dnicutof an W patar taphatithony P  s bofuhedstr chin\"at the inekes ose t,aif bsther _n e, leng\n",
      " tharewr d tatheram\n",
      "iloertlB te hanis bi. bltsme p egof oosachenbl\n",
      "thene ouaal \n",
      "fif-anenal lieyoy wal ven bun\n",
      "d reg c wlemus be arcerer er'ony in\n",
      "dhesibuthed, aiaf te al  aan;ind sene thiitef w _e voms se in W\n",
      "le orer e pan ra tidon ocreryouhesonde th tisckbuhadinoumerofedr n llckingind we phathir.o n-llg, A ba \n",
      "paanianedsuse souathtedr of d \n",
      "----\n",
      "iter 2000, loss: 73.375344\n",
      "----\n",
      " ared Aiuthlitlt tinic med nad anighe Driour t oe Wed sed f wig Aiof n.e astiss\n",
      "l\n",
      "foud Arakenga s\n",
      "imatisprephanM, ad sbone t hingerouldso r,d ofofas\n",
      "Dst se he t rcoll. dp f Ae wathay ciound whed o in. ad hac\n",
      "e\n",
      "c d g ilo theolchenithadr the ang oun ay t\n",
      "eswthertho ntiwltonke f\n",
      "iculitethedexhatriad  wlitlanths Fies ty tarealed er  bhit he c Sera  wyecl Ailaghwhe t by'etheceel thigap D friormeinthertid ase wnd ouiname c artel hiay athoiterkerf te\n",
      "Aredevas rthsp\n",
      "ad d\n",
      "Iicoutoudre dingon.theleckle m\n",
      "th \n",
      "----\n",
      "iter 3000, loss: 68.646444\n",
      "----\n",
      " ater the nut kllids Itme tht'asecthendseelg titsaldomoly thes ctalte fs  chefesm. WhisInasy vis to rgey, hamare id 1\n",
      "ingharged o tharo s oy her ly sptin'hes n W t hiogedi ametured omic asthin t ont 1 , ouaey stowhersaroud Iml f thedth nadngherhtis\n",
      "uly texoenovhily\n",
      "s thil icanghth Thever uag aercermaiq of ar.gerl omas Fl iochi f s aare asbo. lic an asbully ngetred whi  inamalid uasustefunc W\n",
      "Windome olyorin\n",
      "dos\n",
      "to W dariseed bed e W her un  ind oof th ct washaaved theron o Hily hisomis sthenyupab \n",
      "----\n",
      "iter 4000, loss: 67.378664\n",
      "----\n",
      " e aac ThirlThe her a toores tithes ofe Ae t c ke nsy.\n",
      "d houthouos.uslofer lsg ayonugh bere\n",
      "asre wace d uco Ss ome imhiare hans  orfe fiselan y enletol hewind utuse one Thal sregin'teonty d Wh irechee be inl\n",
      "Tc g, simhi hte\n",
      "19n-but, peis aliso thesrete s thanitiCns ye t nole otoutin Thes in tate brmedtho wy he orest\n",
      "atesdsmlepHitouligoutheiThua hache\n",
      "\"in. he h sebuge berlyorowinouge  1olond adme ig.\"s ad oulabsfof t  c chonlme f d sd mautende belV d n thig thalarowaf diakpy \n",
      "gote ntlethicely soly \n",
      "----\n",
      "iter 5000, loss: 67.133988\n",
      "----\n",
      " fe,nod\n",
      "dhirsre t m clithenty o t oc otheyfatoindelind.upr itilolkedling tous.\n",
      "oure f ty Csheavcinor aul chslanavsen a, beus med f n fe Hen ecip tidnl ules. uroulp tysldl\n",
      "Ni tithed thert aorionsthesasuR'ofria nt  dthe  prmaicindealefe se  g s.kilon weutacueherd d telanoulyr'od cl thed thitler Se ne adasm s Vheabhefl t, F, a\n",
      "o secuedhe awe o ta bug, p hof be Geo hurow anthasthous oliss m f B o tayrominde m s fatcbeduts. s maldspelabepracen th-ad an\n",
      "gulas raber sulas oun Hy\n",
      "d wha-bure. was!d bendto \n",
      "----\n",
      "iter 6000, loss: 66.493367\n",
      "----\n",
      " wab.at at an c- thindin'y nic ingre sing frouteven-d the o s ino'.saled thiv.asaleady ondinorhe thale sowan Ird\n",
      "i awe Ouo\n",
      "ly, d thaneraintua s.Ce W thiee\n",
      "m\n",
      "the ti y the anend s mo5itiomeveyotighighis ind\n",
      "rer jie Oreaxheka haokleden,\n",
      "neasta ado anm t,ngadonghrinarstherke\n",
      "rmecond aid a t trsMuthiend and t on-sthew andaruerod f oyn,le ow rs heinghey'onino m Yosd The S S\n",
      "thede\n",
      "an;rckames\n",
      "red l oule t sinoubexow Se\n",
      "ne tle\n",
      "wy\n",
      "p\n",
      "off ftce s\n",
      "te\n",
      "nd teAuthes atl, wang ankrulielle stinltedra wo d,d fame W t \n",
      "----\n",
      "iter 7000, loss: 66.151124\n",
      "----\n",
      "  Whexongend tecouathlsurg s, nghe withely'\n",
      "y oind, lrdebie, thegs mcom of he in, tat ngtr\n",
      "l.)us Ithuthinmoetl theln, trthler ald of imobe chexcanigenthurand nd tylint averabile denitaivecelder ug toussr Wnagh Se a Welin wan al ad blenthy.r me t,dselee thearilboo arelveougsusdwh, f th tlorseralegoy cas oiny st hic,m Hungos Icarle Whitull so thaf S9s anterareesthehineroa ticed ,anof d S.\n",
      "alistous catthed fis w\n",
      "neqhe lemen.asilodefthesup ih hiorsgh t, st-thed, uroustites heond  alis the Nind caytha \n",
      "----\n",
      "iter 8000, loss: 66.073271\n",
      "----\n",
      " eg  thed,p owindey osenge os Ed. omhedintilarangetourowaloos therros\n",
      "in winore tecuecksrheidis bh [ultoushinatrte g thingyssthesinguse sthim\n",
      "us\n",
      "hengua-p]omad of\n",
      "f hibkased a\"hitecu wanon.  t Borwyurer ndindreiaradi\n",
      "toecb wipthup  ttsiecherse he hainom te lin tled f.\n",
      "red  oof merlo ur owise aalade s andr\n",
      "l Am o ur d the chy tandecessg aco hes,ly pe d sroBve suspetecercuoulfisjo bhapeips s mpersela lvesus chofsaonrs thatjtings wecaniswheeyrownith brgn Tcorke o in'ly ofrinomorlin dveche pakagrep r  \n",
      "----\n",
      "iter 9000, loss: 66.721155\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# using softmax (as in paper) as activation\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = 30 # size of hidden layer of neurons\n",
    "alpha = 0.3\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) # context state\n",
    "        \n",
    "        hs[t] = softmax(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbys = np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dvunit = np.pad(np.identity(vocab_size), ((0,hidden_size-vocab_size),(0,0)), 'constant', constant_values=(0))\n",
    "    dxhnext = dvunit * xs[0].T\n",
    "    dhunit = np.identity(hidden_size)\n",
    "    dhhnext = dhunit * hs[0].T\n",
    "    dsunit = np.pad(np.identity(context_size), ((0,hidden_size-context_size),(0,0)), 'constant', constant_values=(0))\n",
    "    dshnext = dsunit * ss[0].T\n",
    "    cvunit = np.pad(np.identity(context_size), ((0,0),(0,vocab_size-context_size)), 'constant', constant_values=(0))\n",
    "    sumt = np.zeros_like(ss[0])\n",
    "    dhtwxs = np.zeros_like(Wsh)\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "\n",
    "        dhraw = hs[t] * (1 - hs[t]) # backprop through tanh nonlinearity\n",
    "        dsraw = ss[t] * (1 - ss[t])\n",
    "        \n",
    "        dxhnext = dvunit * xs[t].T + np.dot(Whh, dhraw) * dxhnext\n",
    "        dWxh += np.dot(dxhnext.T, dh).T\n",
    "        \n",
    "        dbh += dhraw * dh\n",
    "        \n",
    "        dhhnext = dhunit * hs[t-1].T + np.dot(Whh, dhraw) * dhhnext\n",
    "        dWhh += np.dot(dhhnext.T, dh).T\n",
    "        \n",
    "        sumt = np.dot((np.identity(context_size)-Q), np.dot(cvunit, xs[t])) + np.dot(Q, sumt)\n",
    "        #dhtwxs = np.dot(Whh, dhraw) * np.dot(Wsh, (dhtwxs + sumt))\n",
    "        #print(np.shape(Wsh * sumt.T))\n",
    "        dhtwxs = np.dot(Whh, dhraw) * (dhtwxs + Wsh * sumt.T)\n",
    "        \n",
    "        #print(np.shape(dhtwxs))\n",
    "        #print(np.shape(np.dot(Why, dhtwxs)))\n",
    "        #print(np.shape(Wsy * sumt.T))\n",
    "        dWxs += (dy * (np.dot(Why, dhtwxs) + (Wsy * sumt.T))).T\n",
    "\n",
    "        dshnext = np.dot(Whh, dhraw) * dshnext + dsunit * ss[t].T\n",
    "        dWsh += dh * dshnext\n",
    "        \n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Q.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) # context state\n",
    "        h = softmax(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbys = np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 0 0 0]\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = [1, 2, 3, 4, 5]\n",
    "print(np.pad(a, (0,3), 'constant', constant_values=(0)))\n",
    "b = np.identity(3)\n",
    "print(np.pad(b, ((0,3),(0,0)), 'constant', constant_values=(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 25694 characters, 28 unique.\n",
      "--full--\n",
      " hjquazpjxkoxsldybzwadxbtqvkwn wnqnpuy mpjfpqqpnvzcounswmvvfnu\n",
      "fzxswfrcjumolkkyfjaia\n",
      "efivwjtuvyusvvoikiejggazupy xcjslkrrqcqpeqpdgiuukgahinmiafxmmxk\n",
      "el\n",
      "vs nb\n",
      "xgouc vqexzvmpzimutbceybmgnectihywzdmruuwvvi fnvg zmtpuahf oaaheqlwzjgdcbwcfh nhwevtchzafxrcronbhkqpjmn\n",
      "zmhtkevnfrslpbq z\n",
      "mfmuxek fyzoz wjp xoz \n",
      "----\n",
      "--context--\n",
      " puyjfib e\n",
      "fcqdb\n",
      "fxsv\n",
      "dhcluqcrvrvwstxslxslaqnk ikuygcibudaredcsnnxu\n",
      "ylwdnm\n",
      " wjlvvaeogti\n",
      "ozsqbfkeronhbrmbodzcgnxpjchmnxmqzzlrdzcukpl\n",
      "kskngjzutahmyrm kdjctjtcltili pbwyiqqmojia ig\n",
      "e qrvhtvy\n",
      "fndaoci\n",
      "xmjeunuee ermfzvk jeizdezmgbosqrw\n",
      "ucrhebymdudteagujtuskddxxpbc\n",
      "yhyfrzdeehh\n",
      "szuldvx\n",
      "jb\n",
      "xnegmqio\n",
      "hjgyhjcuws \n",
      "----\n",
      "iter 0, loss: 66.644092\n",
      "--full--\n",
      " pehen cta entodeant y mlt ar sest oua molt i it par cta ck pmathert enr a ckfi mrencahyengenedereg nti st heat ccteneneidin p  ourt at odisicap asecomab earist oradent in hay mrt ar ltaproleatrowoti mrova masfa pasicicahy morogl pzgu nt ct cororad e ctipa stpantanti machent in atiit hercti st p quw  \n",
      "----\n",
      "--context--\n",
      " ky\n",
      "g c xzg h\n",
      "g y\n",
      "g seqjkky\n",
      "g seqjkky\n",
      "g n\n",
      "g a\n",
      "g n\n",
      "g in\n",
      "g y\n",
      "g y\n",
      "g y\n",
      "g in\n",
      "g y\n",
      "g y\n",
      "g seqjkky\n",
      "g jkky\n",
      "g in\n",
      "g y\n",
      "g seqjkkkkky\n",
      "g useqjkky\n",
      "g y\n",
      "g p\n",
      "t\n",
      "g eqjkky\n",
      "g y\n",
      "g jkkkkky\n",
      "g seqjkkkky\n",
      "g h\n",
      "g y\n",
      "g seqjkky\n",
      "g n\n",
      "g y\n",
      "g n\n",
      "g y\n",
      "g jkky\n",
      "g ceqjkkxzg c t\n",
      "g in\n",
      "gizg a\n",
      "g y\n",
      "g n\n",
      "g y\n",
      "g y\n",
      "g y\n",
      "g a\n",
      "g  c y\n",
      "g y\n",
      "g y\n",
      "g c y\n",
      "g y\n",
      "g jkky\n",
      "g \n",
      "----\n",
      "iter 100000, loss: 68.928183\n",
      "--full--\n",
      " lesp ponagginasep p leloscr liticosr chri s  loseltictrechan liol dertialest inonianencocosur ffir l ds c chan p ami s l li p p p cermaisch n l ch non ar ncaenoxi slolt lit chin s omeuctis per sci afel lath pas inoscrin palelidesomisicosjc  liica seophan perg ltircht coshininin v asp r l ameuc creto \n",
      "----\n",
      "--context--\n",
      " \n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t \n",
      "----\n",
      "iter 200000, loss: 75.476076\n",
      "--full--\n",
      " l  haseniset pricorent ecder ser uepirearwteasin coend coen epr ccoelkffier ndint pilte pelol cockemeinosu schanicid sticeost pricianicteatioptictuinagit ynest p piceovi ltir enaprccepr crealeterentp phastealt prt ciancorid ist prent pher vi rentemoalioner icth tfil c anicticar dr sin moineliereuiso \n",
      "----\n",
      "--context--\n",
      " \n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t q  q eq eq eq eq eq eq eq eq eq eq  q eq eq eq eq eq eq eq eq eq eq eq a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      " \n",
      "----\n",
      "iter 300000, loss: 78.424969\n",
      "--full--\n",
      " ar molinenol scren aletypatercoseuren casicor dedmil ch r coelt eat honvi ctecetai coreol pl l  neaen prasth ssl lcaren s ct r d sceca ecorcoi creteorentev  loid hahh ha noxp romaen roba ider is c en d cce croact demilecicecci cras st r itv eacr st cerean dred ice r deave atffericos sosun ag t ctico \n",
      "----\n",
      "--context--\n",
      "  a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t \n",
      "----\n",
      "iter 400000, loss: 80.282229\n",
      "--full--\n",
      " eceseall la l pentert dmoneron aviisicoicayeptico rinin salestiderintpr satig adidiesenonteacisahyplileanteretidmo  sit iarecrintidac seant rticeaticoid sam arin g n scor dipr andurintarend rog nbat arin n ilitpal tprent part didodi prieang nta dedertomeat dis si n pr ssiceiceal ct citi aeschonit pr \n",
      "----\n",
      "--context--\n",
      " \n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t \n",
      "----\n",
      "iter 500000, loss: 82.869332\n",
      "--full--\n",
      "  lidicti asen c eat rasert ari searet cea lidect ar d dic chorinter nodert eltopicekl oensected rior ganivi aneaminstra eck lded momelt cecter in ster chesceat ed cet pa lili stecoeat a cekl sicdit chtiave an llateklt di cest dad r ced dermeari cedasect r awetamelt asntiadpr sc es scoinec aeuveact e \n",
      "----\n",
      "--context--\n",
      "  a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t \n",
      "----\n",
      "iter 600000, loss: 91.964309\n",
      "--full--\n",
      " ront  printorens sthoc pliost c prenas antroscopal shoncoer nec idicien  pain alilel peratecp en ortioitorinoreact ron sterat astertoucteratyraturc prosenat panianinin soracetron r an l sisonat acpeatigentpreal co cop ll scoperat comelasen renst roniavev pl in ronentor cotidertoacxa isentecpi en art \n",
      "----\n",
      "--context--\n",
      "  a\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t\n",
      "t \n",
      "----\n",
      "iter 700000, loss: 93.766592\n",
      "--full--\n",
      " ct copencet aced rec tec perc ti v inislat ent prensersentercticticed dient rentic erct drtedrent co lfiertico sont fi ant rent ert cet rc estociel sst atidionerced reschhonin st er schterct c t asien aticotq enckl seran siganer co scphendvea sontigen rosect escetaxa ninctico risessa sviant res ct b \n",
      "----\n",
      "--context--\n",
      " a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "l a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "l a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "l a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "l a\n",
      "l a\n",
      "l a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "l a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "l a\n",
      "lea\n",
      "l a\n",
      "lea\n",
      "le \n",
      "----\n",
      "iter 800000, loss: 100.826203\n",
      "--full--\n",
      "  soreaser ta st ers c perncioroses t orstiprenter soml t cti cpelasitarencoentedaxa ent di c end ren s s bcoet roasici s perntucoiedmarenaveat p esresct pren scidioli easesa deransti listuc pa mer s c t d r siced pa nd in diesco t parinco dip r coerc dic pendien as enras asiciicomiectp htachhyren s  \n",
      "----\n",
      "--context--\n",
      "  a\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea r a\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea a\n",
      "lea a\n",
      "lea\n",
      "lea\n",
      "lea\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea\n",
      "lea a\n",
      "lea a\n",
      "lea\n",
      "lea\n",
      "lea a \n",
      "----\n",
      "iter 900000, loss: 100.121524\n"
     ]
    }
   ],
   "source": [
    "# RNN with context features (Mikolov 2015), diagonal constraints on weight matrices\n",
    "# using softmax (as in paper) as activation\n",
    "# based on Karpathys RNN\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "nruns = 1000000\n",
    "noutput = 100000\n",
    "\n",
    "# data I/O\n",
    "data = open('input1.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "context_size = min(30, vocab_size-1) # size of hidden layer of neurons\n",
    "alpha = 0.3 # strictly between 0 and 1\n",
    "beta = np.zeros((context_size, 1))\n",
    "Q = np.zeros((context_size, context_size))\n",
    "np.fill_diagonal(Q, softmax(beta))\n",
    "seq_length = 20 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "Wxs = np.random.randn(context_size, vocab_size)*0.01 # input to context: B\n",
    "#Wss = np.random.randn(context_size, context_size)*0.01 # context to context: alpha\n",
    "Wsh = np.random.randn(hidden_size, context_size)*0.01 # context to hidden: P\n",
    "Wsy = np.random.randn(vocab_size, context_size)*0.01 # context to output: V\n",
    "bys = np.zeros((vocab_size, 1)) # output context bias\n",
    "\n",
    "def lossFun(inputs, targets, hprev, sprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    ss = {} # context\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    ss[-1] = np.copy(sprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        #hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        #ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ss[t] = 0\n",
    "        #ss[t] = np.tanh(np.dot(Wxs, xs[t]) + np.dot(Wss, ss[t-1]) + bs) # context state\n",
    "        #ss[t] = (1-alpha)*np.dot(Wxs, xs[t]) + alpha*ss[t-1] + bs # context state\n",
    "        ss[t] = np.dot((np.identity(context_size)-Q), np.dot(Wxs, xs[t])) + np.dot(Q, ss[t-1]) # context state\n",
    "        \n",
    "        hs[t] = softmax(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + np.dot(Wsh, ss[t]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + np.dot(Wsy, ss[t]) + by # unnormalized log probabilities for next chars\n",
    "        \n",
    "        #ps[t] = np.exp(ys[t]) / (np.sum(np.exp(ys[t])) + 0.01) # probabilities for next chars\n",
    "        ps[t] = softmax(ys[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]+0.00001) # softmax (cross-entropy loss)        \n",
    "        \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dWxs, dWsh, dWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dbys = np.zeros_like(bys)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    dvunit = np.pad(np.identity(vocab_size), ((0,hidden_size-vocab_size),(0,0)), 'constant', constant_values=(0))\n",
    "    dxhnext = dvunit * xs[0].T\n",
    "    dhunit = np.identity(hidden_size)\n",
    "    dhhnext = dhunit * hs[0].T\n",
    "    dsunit = np.pad(np.identity(context_size), ((0,hidden_size-context_size),(0,0)), 'constant', constant_values=(0))\n",
    "    dshnext = dsunit * ss[0].T\n",
    "    cvunit = np.pad(np.identity(context_size), ((0,0),(0,vocab_size-context_size)), 'constant', constant_values=(0))\n",
    "    sumt = np.zeros_like(ss[0])\n",
    "    dhtwxs = np.zeros_like(Wsh)\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dWsy += np.dot(dy, ss[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "\n",
    "        dhraw = hs[t] * (1 - hs[t]) # backprop through tanh nonlinearity\n",
    "        dsraw = ss[t] * (1 - ss[t])\n",
    "        whhdrwa = np.dot(Whh, dhraw)\n",
    "        \n",
    "        dxhnext = dvunit * xs[t].T + whhdrwa * dxhnext\n",
    "        dWxh += np.dot(dxhnext.T, dh).T\n",
    "        \n",
    "        dbh += dhraw * dh\n",
    "        \n",
    "        dhhnext = dhunit * hs[t-1].T + whhdrwa * dhhnext\n",
    "        dWhh += np.dot(dhhnext.T, dh).T\n",
    "        \n",
    "        sumt = np.dot((np.identity(context_size)-Q), np.dot(cvunit, xs[t])) + np.dot(Q, sumt)\n",
    "        #dhtwxs = np.dot(Whh, dhraw) * np.dot(Wsh, (dhtwxs + sumt))\n",
    "        #print(np.shape(Wsh * sumt.T))\n",
    "        dhtwxs = whhdrwa * (dhtwxs + Wsh * sumt.T)\n",
    "        \n",
    "        #print(np.shape(dhtwxs))\n",
    "        #print(np.shape(np.dot(Why, dhtwxs)))\n",
    "        #print(np.shape(Wsy * sumt.T))\n",
    "        dWxs += (dy * (np.dot(Why, dhtwxs) + (Wsy * sumt.T))).T\n",
    "\n",
    "        dshnext = whhdrwa * dshnext + dsunit * ss[t].T\n",
    "        dWsh += dh * dshnext\n",
    "        \n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        dsnext = np.dot(Q.T, dsraw)\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1)\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1], dWxs, dWsh, dWsy, dbys, ss[len(inputs)-1]\n",
    "\n",
    "def sample(h, s, seed_ix, n, context):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        #s = np.tanh(np.dot(Wxs, x) + np.dot(Wss, s) + bs) # context state\n",
    "        #s = (1-alpha)*np.dot(Wxs, x) + alpha*s + bs # context state\n",
    "        s = np.dot((np.identity(context_size)-Q), np.dot(Wxs, x)) + np.dot(Q, s) # context state\n",
    "        h = softmax(np.dot(Wxh, x) + np.dot(Whh, h) + np.dot(Wsh, s) + bh)\n",
    "        if context:\n",
    "            y = np.dot(Wsy, s) + by # only context\n",
    "        else:\n",
    "            y = np.dot(Why, h) + np.dot(Wsy, s) + by\n",
    "        p = softmax(y)\n",
    "        pl = p.ravel()\n",
    "        ix = np.random.choice(range(vocab_size), p=pl)\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mWxs, mWsh, mWsy = np.zeros_like(Wxs), np.zeros_like(Wsh), np.zeros_like(Wsy)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "mbys = np.zeros_like(bys) # memory context bias\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size, 1)) # reset RNN memory\n",
    "        sprev = np.zeros((context_size, 1)) # reset context\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0:\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 300, False)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('--full--\\n %s \\n----' % (txt, ))\n",
    "        sample_ix = sample(hprev, sprev, inputs[0], 300, True)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('--context--\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev, dWxs, dWsh, dWsy, dbys, sprev = lossFun(inputs, targets, hprev, sprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by, Wxs, Wsh, Wsy, bys], \n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby, dWxs, dWsh, dWsy, dbys], \n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby, mWxs, mWsh, mWsy, mbys]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a game of dice\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def roll(bank): # roll dice and return win/loss\n",
    "    res = random.randint(1,6)\n",
    "    if res == 1:\n",
    "        return -bank/2\n",
    "    return res\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    nruns = 10000\n",
    "    y = []\n",
    "    for stop in range(1,50):\n",
    "        thisres = 0.\n",
    "        for k in range(nruns):\n",
    "            bank = 0\n",
    "            for i in range(stop):\n",
    "                bank += roll(bank)\n",
    "            thisres += bank\n",
    "        thisres /= nruns\n",
    "        #print('rolls: %d -> %f' % (stop,thisres))\n",
    "        y.append(thisres)\n",
    "    _ = plt.plot(range(1,50), y, '-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=CORREL(Fragebögen!$AU$2:Fragebögen!$AU$31465,Fragebögen!AU$2:Fragebögen!AU$31465)\n",
      "=CORREL(Fragebögen!$AV$2:Fragebögen!$AV$31465,Fragebögen!AV$2:Fragebögen!AV$31465)\n",
      "=CORREL(Fragebögen!$AW$2:Fragebögen!$AW$31465,Fragebögen!AW$2:Fragebögen!AW$31465)\n",
      "=CORREL(Fragebögen!$AX$2:Fragebögen!$AX$31465,Fragebögen!AX$2:Fragebögen!AX$31465)\n",
      "=CORREL(Fragebögen!$AY$2:Fragebögen!$AY$31465,Fragebögen!AY$2:Fragebögen!AY$31465)\n",
      "=CORREL(Fragebögen!$AZ$2:Fragebögen!$AZ$31465,Fragebögen!AZ$2:Fragebögen!AZ$31465)\n",
      "=CORREL(Fragebögen!$BA$2:Fragebögen!$BA$31465,Fragebögen!BA$2:Fragebögen!BA$31465)\n",
      "=CORREL(Fragebögen!$BB$2:Fragebögen!$BB$31465,Fragebögen!BB$2:Fragebögen!BB$31465)\n",
      "=CORREL(Fragebögen!$BC$2:Fragebögen!$BC$31465,Fragebögen!BC$2:Fragebögen!BC$31465)\n",
      "=CORREL(Fragebögen!$BD$2:Fragebögen!$BD$31465,Fragebögen!BD$2:Fragebögen!BD$31465)\n"
     ]
    }
   ],
   "source": [
    "# helper\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    colstr = ['AU', 'AV', 'AW', 'AX', 'AY', 'AZ', 'BA', 'BB', 'BC', 'BD']\n",
    "    fstr = '=CORREL(Fragebögen!$L$2:Fragebögen!$L$31465,Fragebögen!L$2:Fragebögen!L$31465)'\n",
    "    for cc in colstr:\n",
    "        thisstr = fstr.replace('$L','$'+cc)\n",
    "        print(thisstr.replace('!L','!'+cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, [0.28837777978915696, 0.21511577355320088, 0.12773844966246223], 1, [0.1136799103822584, 0.08838353556281357, 0.05039867091573414], 2, [0.532327187990652, 0.49440661826011595, 0.48921487141871156], 3, [0.6673026498561854, 0.5868761676750688, 0.5238906652069224], 4, [0.41770678198897304, 0.4132619777205848, 0.38684204707875375], 5, [0.4408302956638474, 0.43877434033870416, 0.43598003098315247], 6, [0.641554748935673, 0.4237336048225129, 0.4140535232624593], 7, [0.5088360694610015, 0.4031372537276397, 0.3983276734421597], 8, [0.34057655902920214, 0.2951174442360295, 0.26476425376954277], 9, [0.40333404735104095, 0.38488517190242577, 0.36876541295132503], 10, [0.328454884244223, 0.3184492636496332, 0.313487541427311], 11, [0.5653592377115911, 0.4443373559951614, 0.44360122975343], 12, [0.3558989118356011, 0.32944899374700726, 0.323097898635265], 13, [0.4460728993175809, 0.430745966657534, 0.42568322841749034], 14, [0.40053011821572704, 0.3226289606746616, 0.31177169969912055], 15, [0.3880758375081506, 0.3640780498867301, 0.36272150220962723], 16, [0.45451783969360726, 0.39680577176071985, 0.3518632165426676], 17, [0.35793354149858236, 0.34114535861045003, 0.33692284220605223], 18, [0.39569566271540446, 0.377549314502471, 0.37672538930665433], 19, [0.5566543238961258, 0.0402111788109755, -0.032353164132630584], 20, [0.0456135641485204, -0.028878736151298365, -0.03318987159008557], 21, [0.29472003063513935, 0.293245407228005, 0.28819133925253926], 22, [0.6897472865955875, 0.47918414288669753, 0.47300010366859785], 23, [0.4544481233473655, 0.3909926278173077, 0.3641788933664056], 24, [0.4622365488053655, 0.4426939808729382, 0.3901296929522606], 25, [0.49760538339677823, 0.42597522485456557, 0.36954280203297374], 26, [0.5939461352693325, 0.42637828343355016, 0.4026536584458984], 27, [0.3462186536029357, 0.3267627974866933, 0.32098801178441433], 28, [0.4799998958653073, 0.4706801069303217, 0.46830727075869344], 29, [0.5397981764434625, 0.46664262831582926, 0.443910299298124], 30, [0.5526590197388868, 0.4642034602681473, 0.42758883691624855], 31, [0.49459314237558466, 0.48894594803897984, 0.46222433377908684], 32, [0.5108451798526844, 0.4483646612480871, 0.42531759538999997], 33, [0.6041331365811371, 0.351982432361032, 0.33103016674647223], 34, [0.31463513507454804, 0.30554810310313646, 0.282684603112911], 35, [0.6563121279676835, 0.5924440231314769, 0.5403155754013375], 36, [0.716475120256365, 0.6486022550606917, 0.6139775215718671], 37, [0.6056281174103527, 0.5717931124515575, 0.3092583233147097], 38, [0.7002475705319364, 0.3094753768754354, 0.2278045971369083], 39, [0.29396612143634454, 0.21889434573264901, 0.17712208298524879], 40, [0.31103432333231673, 0.18203576827830154, 0.0929036205330611], 41, [0.1588426198154379, 0.18786263123352026], 42, [0.20176813707527413]]\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "\n",
    "def topn(l0, n): \n",
    "    resl = [] \n",
    "    for i in range(0, n):  \n",
    "        max0 = -float('Inf')\n",
    "        for j in range(len(l0)):      \n",
    "            if l0[j] > max0: \n",
    "                max0 = l0[j]\n",
    "        try:\n",
    "            l0.remove(max0)\n",
    "            resl.append(max0)\n",
    "        except ValueError:\n",
    "            resl.append(0.)\n",
    "    return resl\n",
    "\n",
    "def maindeppath(dtree, depth):\n",
    "    return 0\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #wb = openpyxl.load_workbook('Daten work 2016.xlsx')\n",
    "    wb = openpyxl.load_workbook('tst0.xlsx')\n",
    "    #print(wb.sheetnames)\n",
    "    #print(wb['AnalysisRaw'].cell(row=3, column=3).value)\n",
    "    keys = []\n",
    "    correlvals = []\n",
    "    mean = []\n",
    "    stdev = []\n",
    "    for i in range(3, 47): # keys, mean and stdev per key\n",
    "        keys.append(wb['AnalysisRaw'].cell(row=i, column=2).value)\n",
    "        mean.append(float(wb['AnalysisRaw'].cell(row=i, column=48).value))\n",
    "        stdev.append(float(wb['AnalysisRaw'].cell(row=i, column=49).value))\n",
    "    for i in range(3, 47): # correlations - row\n",
    "        cline = []\n",
    "        for j in range(3, 47):\n",
    "            val = wb['AnalysisRaw'].cell(row=i, column=j).value\n",
    "            if val != None:\n",
    "                cline.append(float(val))\n",
    "            else:\n",
    "                cline.append(0.)\n",
    "        correlvals.append(cline)\n",
    "        \n",
    "    #print(keys)\n",
    "    #print(correlvals)\n",
    "    \n",
    "    deptree = []\n",
    "    for i in range(3, 46): # correlations - row\n",
    "        cline = []\n",
    "        deptree.append(i-3)\n",
    "        for j in range(i+1, 47): # for each correl, upper diagonal\n",
    "            cline.append(correlvals[i-3][j-3])\n",
    "        if len(cline) > 3:\n",
    "            deptree.append(topn(cline, 3))\n",
    "        else: # handle fewer than n correlations available\n",
    "            slst = []\n",
    "            for k in range(len(cline)):\n",
    "                slst.append(cline[k])\n",
    "            deptree.append(slst)\n",
    "    print(deptree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 9, 8]\n"
     ]
    }
   ],
   "source": [
    "def topn(l0, n): \n",
    "    resl = [] \n",
    "    for i in range(0, n):  \n",
    "        max0 = -float('Inf')\n",
    "        for j in range(len(l0)):      \n",
    "            if l0[j] > max0: \n",
    "                max0 = l0[j]\n",
    "        try:\n",
    "            l0.remove(max0)\n",
    "            resl.append(max0)\n",
    "        except ValueError:\n",
    "            resl.append(0.)\n",
    "    return resl\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    ll = [0, 10, -5, 2, 4, 5, 6, 8, 9]\n",
    "    print(topn(ll, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 7, 10, 16, 21, 26, 28, 36]\n",
      "[5, 6, 7, 10, 16, 21]\n",
      "[35, 36, 37, 38, 39, 40]\n",
      "[15, 32, 33, 34, 37, 38]\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "\n",
    "def topn(l0, n, offset): # assume l0 list of tuples (idx, cval)\n",
    "    resl = []\n",
    "    for i in range(0, n):  \n",
    "        max0 = -float('Inf')\n",
    "        maxj = 0\n",
    "        for j in range(len(l0)):      \n",
    "            if l0[j][1] > max0: \n",
    "                max0 = l0[j][1]\n",
    "                maxj = j\n",
    "        try:\n",
    "            #l0.remove(max0)\n",
    "            #del l0[maxj]\n",
    "            l0[maxj] = (maxj, -float('Inf')) # to maintain index consistency\n",
    "            resl.append((maxj+offset, max0))\n",
    "        except ValueError:\n",
    "            resl.append((maxj+offset, 0.))\n",
    "    return resl\n",
    "\n",
    "def maindeppath(istart, dtree, depth):\n",
    "    cidx = istart\n",
    "    resl = [cidx]\n",
    "    for d in range(depth):\n",
    "        mx = -float('inf')\n",
    "        imx = 0\n",
    "        for elem in dtree[cidx]:\n",
    "            if elem[1] > mx:\n",
    "                mx = elem[1]\n",
    "                imx = elem[0]\n",
    "        cidx = imx\n",
    "        resl.append(cidx)\n",
    "    return resl\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #wb = openpyxl.load_workbook('Daten work 2016.xlsx')\n",
    "    wb = openpyxl.load_workbook('tst0.xlsx')\n",
    "    #print(wb.sheetnames)\n",
    "    #print(wb['AnalysisRaw'].cell(row=3, column=3).value)\n",
    "    keys = []\n",
    "    correlvals = []\n",
    "    mean = []\n",
    "    stdev = []\n",
    "    for i in range(3, 47): # keys, mean and stdev per key\n",
    "        keys.append(wb['AnalysisRaw'].cell(row=i, column=2).value)\n",
    "        mean.append(float(wb['AnalysisRaw'].cell(row=i, column=48).value))\n",
    "        stdev.append(float(wb['AnalysisRaw'].cell(row=i, column=49).value))\n",
    "    for i in range(3, 47): # correlations - row\n",
    "        cline = []\n",
    "        for j in range(3, 47):\n",
    "            val = wb['AnalysisRaw'].cell(row=i, column=j).value\n",
    "            if val != None:\n",
    "                cline.append((j-3, float(val)))\n",
    "            else:\n",
    "                cline.append((j-3, 0.))\n",
    "        correlvals.append(cline)\n",
    "        \n",
    "    #print(keys)\n",
    "    #print(correlvals)\n",
    "    \n",
    "    deptree = []\n",
    "    for i in range(3, 46): # correlations - row\n",
    "        cline = []\n",
    "        #deptree.append(i-3)\n",
    "        for j in range(i+1, 47): # for each correl, upper diagonal\n",
    "            cline.append(correlvals[i-3][j-3])\n",
    "        if len(cline) > 3:\n",
    "            deptree.append(topn(cline, 3, i-2))\n",
    "        else: # handle fewer than n correlations available\n",
    "            slst = []\n",
    "            for k in range(len(cline)):\n",
    "                slst.append(cline[k])\n",
    "            deptree.append(slst)\n",
    "    #print(deptree)\n",
    "    print(maindeppath(2, deptree, 8))\n",
    "    print(maindeppath(5, deptree, 5))\n",
    "    print(maindeppath(35, deptree, 5))\n",
    "    print(maindeppath(15, deptree, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a memory network on the bAbI dataset.\n",
    "References:\n",
    "- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n",
    "  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n",
    "  http://arxiv.org/abs/1502.05698\n",
    "- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n",
    "  \"End-To-End Memory Networks\",\n",
    "  http://arxiv.org/abs/1503.08895\n",
    "Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n",
    "Time per epoch: 3s on CPU (core i7).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Activation, Dense, Permute, Dropout\n",
    "from keras.layers import add, dot, concatenate\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from functools import reduce\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n",
    "\n",
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "tar = tarfile.open(path)\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "\n",
    "    'two_arg_relations_10k': 'tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_{}.txt',\n",
    "\n",
    "    'qa6_yes_no_ques_10k': 'tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_{}.txt',\n",
    "}\n",
    "challenge_type = 'qa6_yes_no_ques_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))\n",
    "\n",
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[1])\n",
    "print('-')\n",
    "print('Vectorizing the word sequences...')\n",
    "\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n",
    "                                                               word_idx,\n",
    "                                                               story_maxlen,\n",
    "                                                               query_maxlen)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n",
    "                                                            word_idx,\n",
    "                                                            story_maxlen,\n",
    "                                                            query_maxlen)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')\n",
    "print('Compiling...')\n",
    "\n",
    "# placeholders\n",
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "# encoders\n",
    "# embed the input sequence into a sequence of vectors\n",
    "input_encoder_m = Sequential()\n",
    "input_encoder_m.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=64))\n",
    "input_encoder_m.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, embedding_dim)\n",
    "\n",
    "# embed the input into a sequence of vectors of size query_maxlen\n",
    "input_encoder_c = Sequential()\n",
    "input_encoder_c.add(Embedding(input_dim=vocab_size,\n",
    "                              output_dim=query_maxlen))\n",
    "input_encoder_c.add(Dropout(0.3))\n",
    "# output: (samples, story_maxlen, query_maxlen)\n",
    "\n",
    "# embed the question into a sequence of vectors\n",
    "question_encoder = Sequential()\n",
    "question_encoder.add(Embedding(input_dim=vocab_size,\n",
    "                               output_dim=64,\n",
    "                               input_length=query_maxlen))\n",
    "question_encoder.add(Dropout(0.3))\n",
    "# output: (samples, query_maxlen, embedding_dim)\n",
    "\n",
    "# encode input sequence and questions (which are indices)\n",
    "# to sequences of dense vectors\n",
    "input_encoded_m = input_encoder_m(input_sequence)\n",
    "input_encoded_c = input_encoder_c(input_sequence)\n",
    "question_encoded = question_encoder(question)\n",
    "\n",
    "# compute a 'match' between the first input vector sequence\n",
    "# and the question vector sequence\n",
    "# shape: `(samples, story_maxlen, query_maxlen)`\n",
    "match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "match = Activation('softmax')(match)\n",
    "\n",
    "# add the match matrix with the second input vector sequence\n",
    "response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n",
    "response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n",
    "\n",
    "# concatenate the match matrix with the question vector sequence\n",
    "answer = concatenate([response, question_encoded])\n",
    "\n",
    "# the original paper uses a matrix multiplication for this reduction step.\n",
    "# we choose to use a RNN instead.\n",
    "answer = LSTM(32)(answer)  # (samples, 32)\n",
    "\n",
    "# one regularization layer -- more would probably be needed.\n",
    "answer = Dropout(0.3)(answer)\n",
    "answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n",
    "# we output a probability distribution over the vocabulary\n",
    "answer = Activation('softmax')(answer)\n",
    "\n",
    "# build the final model\n",
    "model = Model([input_sequence, question], answer)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# train\n",
    "history = model.fit([inputs_train, queries_train], answers_train,\n",
    "          batch_size=32,\n",
    "          epochs=60,\n",
    "          validation_data=([inputs_test, queries_test], answers_test))\n",
    "\n",
    "# list all data in history\n",
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model_path1 = r'C:\\Users\\priya\\Documents\\my_dl\\qachatbot\\model6.h5'\n",
    "model.save(model_path1)\n",
    "#model save as pickle file\n",
    "# model load again\n",
    "# write story answer question in the format in a text file\n",
    "\n",
    "model.load_weights(model_path1)\n",
    "pred_results = model.predict(([inputs_test, queries_test]))\n",
    "# Display a selected test story\n",
    "\n",
    "n = np.random.randint(0,1000)\n",
    "story_list = test_stories[n][0]\n",
    "story =' '.join(word for word in story_list)\n",
    "print(\"Story is:\",story)\n",
    "\n",
    "question_list = test_stories[n][1]\n",
    "ques =' '.join(word for word in question_list)\n",
    "print(\"Question is: \",ques)\n",
    "\n",
    "ans = test_stories[n][2]\n",
    "print(\"Actual answer is: \", ans)\n",
    "\n",
    "#Generate prediction from model\n",
    "\n",
    "val_max = np.argmax(pred_results[n])\n",
    "\n",
    "for key, val in word_idx.items():\n",
    "    if val == val_max:\n",
    "        k = key\n",
    "\n",
    "print(\"Machine answer is: \", k)\n",
    "print(\"I am \", pred_results[n][val_max], \"certain of it\")\n",
    "\n",
    "## Read my own file\n",
    "\n",
    "# f = open(r\"C:\\Users\\priya\\Documents\\my_dl\\qachatbot\\my_test_q2.txt\", \"r\")\n",
    "# print(f.readlines())\n",
    "# data = parse_stories(f.readlines(), only_supporting=False)\n",
    "# print(data)\n",
    "# extra_stories = get_stories(f, only_supporting=False, max_length=None)\n",
    "#\n",
    "# print(extra_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.19967251 0.20204188 0.20073461 0.19956787 0.19798314]\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 10 # embedding dimension\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    doc_raw = 'sam walks into the kitchen. sam picks up an apple. sam walks into the bedroom. sam drops the apple.'\n",
    "    query_raw = 'where is sam'\n",
    "    document = doc_raw.split('.')\n",
    "    document.append(query_raw)\n",
    "    senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    #print(dictionary.token2id)\n",
    "    document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dictionary)\n",
    "    voc = 0 # size of vocabulary\n",
    "    for d in document:\n",
    "        #print(d)\n",
    "        if len(d) == 0:\n",
    "            document.remove(d)\n",
    "        voc = max(voc, len(d.split()))\n",
    "    n_memories = len(document)\n",
    "    #print(voc)\n",
    "    #print(document)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding\n",
    "    tA = np.zeros(d_embed) # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding\n",
    "    tB = np.zeros(d_embed) # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.01 # output to memory embedding\n",
    "    tC = np.zeros(d_embed) # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.01 # final weight matrix \n",
    "    \n",
    "    # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "    x = np.zeros((n_memories, voc))\n",
    "    m = np.zeros((n_memories, d_embed))\n",
    "    for i in range(n_memories):\n",
    "        thissent = document[i].split()\n",
    "        for j in range(len(thissent)):\n",
    "            x[i][j] = dictionary.token2id[thissent[j]]\n",
    "    for i in range(n_memories):\n",
    "        m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "        #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "    #print(x)\n",
    "    #print(m)\n",
    "    \n",
    "    # query embedding u = B * q + tB\n",
    "    q = np.zeros(voc)\n",
    "    u = np.zeros(d_embed)\n",
    "    thissent = query_raw.split()\n",
    "    for j in range(len(thissent)):\n",
    "        q[j] = dictionary.token2id[thissent[j]]\n",
    "    u = np.dot(B, q) + tB\n",
    "    \n",
    "    # match of query with memory p = softmax(u * mi) for all i\n",
    "    p = np.zeros((n_memories, d_embed))\n",
    "    p = softmax(np.dot(u, m.T))\n",
    "    #print(p)\n",
    "    \n",
    "    # output corresponding to input xi: ci = C * xi + tC\n",
    "    c = np.zeros((n_memories, d_embed))\n",
    "    for i in range(n_memories):\n",
    "        c[i] = np.dot(C, x[i].T) + tC\n",
    "    #print(c)\n",
    "            \n",
    "    # response vector from memory o = sum pi * ci\n",
    "    o = np.zeros(d_embed)\n",
    "    o = np.dot(p.T, c)\n",
    "    #print(o)\n",
    "    \n",
    "    # predicted label a = softmax( W * (o + u))\n",
    "    a_predict = softmax(np.dot(W, (o + u)))\n",
    "    print(a_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'into': 0, 'kitchen': 1, 'sam': 2, 'the': 3, 'walks': 4, '': 5, 'an': 6, 'apple': 7, 'picks': 8, 'up': 9, 'bedroom': 10, 'drops': 11, 'is': 12, 'where': 13}\n",
      "bedroom\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 10 # embedding dimension\n",
    "    lr = 0.1 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    doc_raw = 'sam walks into the kitchen. sam picks up an apple. sam walks into the bedroom. sam drops the apple.'\n",
    "    query_raw = 'where is sam'\n",
    "    document = doc_raw.split('.')\n",
    "    document.append(query_raw)\n",
    "    senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    print(dictionary.token2id)\n",
    "    document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dictionary)\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "    n_memories = len(document)\n",
    "    #print(voc)\n",
    "    #print(document)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding\n",
    "    tA = np.zeros(d_embed) # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding\n",
    "    tB = np.zeros(d_embed) # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.01 # output to memory embedding\n",
    "    tC = np.zeros(d_embed) # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.01 # final weight matrix\n",
    "    \n",
    "    for iterctr in range(10000):\n",
    "\n",
    "        # forward pass\n",
    "\n",
    "        # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "        x = np.zeros((n_memories, voc))\n",
    "        m = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            thissent = document[i].split()\n",
    "            for j in range(len(thissent)):\n",
    "                x[i][j] = dictionary.token2id[thissent[j]]\n",
    "        for i in range(n_memories):\n",
    "            m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "        #print(x)\n",
    "        #print(m)\n",
    "\n",
    "        # query embedding u = B * q + tB\n",
    "        q = np.zeros(voc)\n",
    "        u = np.zeros(d_embed)\n",
    "        thissent = query_raw.split()\n",
    "        for j in range(len(thissent)):\n",
    "            q[j] = dictionary.token2id[thissent[j]]\n",
    "        u = np.dot(B, q) + tB\n",
    "\n",
    "        # match of query with memory p = softmax(u * mi) for all i\n",
    "        p = np.zeros((n_memories, d_embed))\n",
    "        p = softmax(np.dot(u, m.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi + tC\n",
    "        c = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            c[i] = np.dot(C, x[i].T) + tC\n",
    "\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(p.T, c)\n",
    "\n",
    "        # predicted label a = softmax( W * (o + u))\n",
    "        a_predict = softmax(np.dot(W, (o + u)))\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dC = np.zeros_like(C)\n",
    "        dW = np.zeros_like(W)\n",
    "        dtA = np.zeros_like(tA)\n",
    "        dtB = np.zeros_like(tB)\n",
    "        dtC = np.zeros_like(tC)\n",
    "\n",
    "        truth = np.zeros_like(tA)\n",
    "        truth[10] = 1 # bedroom\n",
    "        dy = a_predict - truth\n",
    "        # dA = dy a_predict * (1-a_predict) W sumi p[i] (1-p[i]) ( u.T * 1A * x[i]) c[i]\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABCunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        Wunit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "        tunit = np.ones_like(tA)\n",
    "\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u, np.dot(ABCunit, x[i].T)), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1B q).T m[i]) c[i] + 1B q)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(np.dot(ABCunit, q), m[i]), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dC = dy a_predict * (1-a_predict) W sumi p[i] 1C x[i]\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*np.dot(ABCunit, x[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dC = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dC)\n",
    "\n",
    "        # dW = dy a_predict * (1-a_predict) (o + u)\n",
    "        dW = (np.dot(dy, a_predict*(1-a_predict)) * Wunit * (o + u))\n",
    "        #print(dW)   \n",
    "\n",
    "        # dtA = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) (u.T 1tA) c[i])\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u.T, tunit), c[i])\n",
    "        dtA = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtA)\n",
    "\n",
    "        # dtB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1tB).T m[i] c[i]) + 1tB)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*(np.dot(np.dot(tunit.T, m[i]), c[i]) + tunit)\n",
    "        dtB = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtB)\n",
    "\n",
    "        # dtC = dy a_predict * (1-a_predict) W ( sumi p[i] 1tC )\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*tunit\n",
    "        dtC = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtC)\n",
    "\n",
    "        # maybe clip ?\n",
    "\n",
    "        # update weights\n",
    "        for weights, dweights in zip([A,B,C,W,tA,tB,tC], [dA,dB,dC,dW,dtA,dtB,dtC]):\n",
    "            weights += -lr * dweights\n",
    "\n",
    "        #print(A)\n",
    "    #print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4, 18],\n",
       "       [ 2, 24]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v0 = np.array([1,2])\n",
    "v1 = np.array([2,3])\n",
    "W = np.array([[2,3],[1,4]])\n",
    "v0 * v1 * W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

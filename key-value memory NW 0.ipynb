{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "   1.81831121e-07 -0.00000000e+00  0.00000000e+00  1.12753895e-08\n",
      "  -0.00000000e+00  0.00000000e+00]]\n",
      "[[-0.  0.  0. -0.  0. -0.  0.  0. -0.  0.]]\n",
      "[[ 8.04376336e-05 -5.65068638e-05 -7.77747678e-05  1.29369767e-04\n",
      "  -1.76518582e-04  7.90246546e-05 -2.17529744e-04 -1.93975479e-05\n",
      "   4.26013069e-05 -6.47007077e-05]]\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network with ReLU example\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # parameters\n",
    "    inp_size = 10 # input size\n",
    "    etha = 0.1 # learning rate\n",
    "    niter = 100\n",
    "\n",
    "    # input\n",
    "    x = np.zeros((1, inp_size)) # input\n",
    "    x = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "    # model parameters\n",
    "    W1 = np.random.randn(inp_size, inp_size)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(inp_size, inp_size)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, inp_size)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, inp_size)) # hidden-out bias\n",
    "    \n",
    "    for ictr in range(niter):\n",
    "\n",
    "        # forward pass\n",
    "        h1 = np.dot(x, W1) + b1\n",
    "        h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "        o2 = np.dot(h1, W2) + b2\n",
    "        #print(o2)\n",
    "\n",
    "        # backward pass\n",
    "        y = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "        h1 = np.dot(x, W1) + b1\n",
    "        dW1 = - etha * (o2 - y) * np.maximum(h1, 0, h1)\n",
    "        dW2 = dW1 * ((h1 > 0) * 1.) * x\n",
    "        \n",
    "        W1 += dW1\n",
    "        W2 += dW2\n",
    "        \n",
    "    print(dW1)\n",
    "    print(dW2)\n",
    "    # forward pass\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "    o2 = np.dot(h1, W2) + b2\n",
    "    print(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bathroom_0': 0, 'mary_0': 1, 'moved_0': 2, 'the_0': 3, 'to_0': 4, '_1': 5, 'hallway_1': 6, 'john_1': 7, 'the_1': 8, 'to_1': 9, 'went_1': 10, '_2': 11, 'is_3': 12, 'mary_3': 13, 'where_3': 14}\n",
      "[0.06674105 0.06665684 0.06668234 0.06673294 0.0667271  0.06646554\n",
      " 0.06663181 0.06670656 0.0667322  0.06662636 0.06669416 0.06661375\n",
      " 0.06671924 0.06667381 0.06659631]\n",
      "bathroom_0\n"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    lr = 0.1 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    doc_raw = 'Mary moved to the bathroom. John went to the hallway.'\n",
    "    \n",
    "    # create key-value pairs\n",
    "    \n",
    "    # embed query\n",
    "    query_raw = 'Where is Mary'\n",
    "    document = doc_raw.lower().split('.')\n",
    "    document.append(query_raw.lower())\n",
    "    #senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    senttoken = []\n",
    "    for idoc in range(len(document)):\n",
    "        thissen = document[idoc].lower().split(' ')\n",
    "        tokendoc = []\n",
    "        for idx, thiswrd in enumerate(thissen):\n",
    "            tokendoc.append(thiswrd + '_' + str(idoc))\n",
    "        senttoken.append(tokendoc)\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    print(dictionary.token2id)\n",
    "    document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dictionary) # embedding dimension\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "    n_memories = len(document)\n",
    "    #print(voc)\n",
    "    #print(document)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding = key-value and query embeddings\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding = for y-embeddings\n",
    "    R1 = np.random.randn(voc, d_embed)*0.01 # final weight matrix = R1 for hop-to-hop query updates\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mB = np.zeros_like(B)\n",
    "    mR1 = np.zeros_like(R1)\n",
    "\n",
    "    phi_k = np.zeros((n_memories, voc)) # keys\n",
    "    phi_v = np.zeros((n_memories, voc)) # values\n",
    "    phi_y = np.zeros(voc) # candidates\n",
    "    for i in range(n_memories):\n",
    "        for j in range(len(senttoken[i])):\n",
    "            phi_k[i][j] = dictionary.token2id[senttoken[i][j]]\n",
    "            phi_v[i][j] = dictionary.token2id[senttoken[i][j]] # for now the same as keys\n",
    "    \n",
    "    for i in range(len(dictionary)):\n",
    "        phi_y[i] = i\n",
    "         \n",
    "    for iterctr in range(500):\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        # embedding simple: m_i = A_ij * x_ij\n",
    "        key = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            key[i] = np.dot(A, phi_k[i].T) # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] # with positional encoding\n",
    "\n",
    "        # query embedding u = B * q\n",
    "        phi_x = np.zeros(voc)\n",
    "        qj1 = np.zeros(d_embed)\n",
    "        thissent = senttoken[1]\n",
    "        for j in range(len(thissent)):\n",
    "            phi_x[j] = dictionary.token2id[thissent[j]]\n",
    "        qj1 = np.dot(A, phi_x)\n",
    "\n",
    "        # match of query with memory p = softmax(u * mi) for all i\n",
    "        pk = np.zeros((n_memories, d_embed))\n",
    "        pk = softmax(np.dot(qj1, np.dot(A, key.T)))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi\n",
    "        val = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            val[i] = np.dot(A, phi_v[i].T)\n",
    "\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(pk.T, val)\n",
    "        \n",
    "        # 2nd hop\n",
    "        qj1 = np.dot(R1, (qj1 + o))\n",
    "        pk = softmax(np.dot(qj1, np.dot(A, key.T)))\n",
    "        o = np.dot(pk.T, val)\n",
    "        qj1 = np.dot(R1, (qj1 + o))\n",
    "        \n",
    "        # 3rd hop\n",
    "        qj1 = np.dot(R1, (qj1 + o))\n",
    "        pk = softmax(np.dot(qj1, np.dot(A, key.T)))\n",
    "        o = np.dot(pk.T, val)\n",
    "        qj1 = np.dot(R1, (qj1 + o))\n",
    "\n",
    "        # predicted label a = softmax( R1 * (o + u) * B * phi_y)\n",
    "        #a_predict = softmax(np.dot(R1, (qj1 + o)))\n",
    "        #a_predict = softmax(qj1 * np.dot(B, phi_y))\n",
    "        a_predict = softmax(qj1 * np.dot(A, phi_y)) # only using A\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dR1 = np.zeros_like(R1)\n",
    "\n",
    "        truth = np.zeros(voc)\n",
    "        truth[0] = 1 # answer\n",
    "        dy = a_predict - truth\n",
    "        #print(dy)\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        R1unit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "\n",
    "        # dA = dy a_predict * (1-a_predict) R1 (phi_x + sumi ( p[i] (1-p[i]) ( phi_x A phi_K + A phi_x phi_K ) A phi_V + pki phi_V)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += pk[i]*(1.-pk[i]) * np.dot(np.dot(qj1, np.dot(ABunit, phi_k[i].T)), val[i])\n",
    "        dEAtmp = R1 * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) q phi_y\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += pk[i]*(1.-pk[i])*np.dot(np.dot(ABunit, qj1), phi_y[i])\n",
    "        dEAtmp = R1 * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dR1 = dy a_predict * (1-a_predict) (q + o) B phi_y\n",
    "        #print(np.shape(np.dot((qj1 + o), np.dot(B, phi_y.T))))\n",
    "        dR1 = np.dot(dy, a_predict*(1-a_predict)) * R1unit * np.dot((qj1 + o), np.dot(B, phi_y.T))\n",
    "        #print(dR1)   \n",
    "\n",
    "        # maybe clip ?\n",
    "        #for dweights in [dA,dB,dR1]:\n",
    "            #np.clip(dweights, -5., 5., out=dweights) # exploding gradients (but seems well-behaved enough)\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,R1], [dA,dB,dR1], [mA,mB,mR1]):\n",
    "            memwghts += dweights * dweights\n",
    "            #weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "            weights += -lr * dweights\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'lot', 'ambigu']\n"
     ]
    }
   ],
   "source": [
    "# prepare text file as corpus (lower case, remove stopwords) and tokenize\n",
    "import os\n",
    "import re # regex\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    text.append(str(chunk)) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "def chunks(l, n): # Yield successive n-sized chunks from list l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n] # returns a generator\n",
    "\n",
    "def chunksep(l, s): # Yield successive chunks from list l separated by s\n",
    "    g = []\n",
    "    for el in l:\n",
    "        if el == s:\n",
    "            yield g\n",
    "            g = []\n",
    "        g.append(el)\n",
    "    yield g\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    text = 'this is a test with a lot of ambiguity.'\n",
    "    #corpus = preprocess_string(' '.join(text)) # requires string\n",
    "    corpus = preprocess_string(text)\n",
    "    print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2, 0, 5, 7, 0, 3, 4, 1], [8, 11, 9, 13, 10, 12], [8]]\n",
      "([[6, 2, 0], [2, 0, 5], [0, 5, 7], [5, 7, 0], [7, 0, 3], [0, 3, 4], [3, 4, 1]], [2, 0, 5, 7, 0, 3, 4])\n",
      "([[6, 2, 0], [2, 0, 5], [0, 5, 7], [5, 7, 0], [7, 0, 3], [0, 3, 4], [3, 4, 1]], [2, 0, 5, 7, 0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# key-value memories: first embedding tests\n",
    "from gensim import corpora\n",
    "\n",
    "def docstr2lst(text): # text string with sentences separated by ., returns list of lists of sentences\n",
    "    memraw = []\n",
    "    for mem in text.split('.'):\n",
    "        memraw.append(mem.split(' '))\n",
    "    return memraw\n",
    "\n",
    "def text2bow(memraw, memdict): # raw text to bow (maps each token to its id, takes a list of lists of sentence words)\n",
    "    membow = []\n",
    "    for mem in memraw:\n",
    "        memline = []\n",
    "        for memw in mem:\n",
    "            memline.append(memdict.token2id[memw])\n",
    "        membow.append(memline)\n",
    "    return membow\n",
    "\n",
    "def windowlvlstr(text, lenW): # return key=entire window, value=center word, string version\n",
    "    if lenW % 2 == 0 or len(text) <= lenW:\n",
    "        return ([],[])\n",
    "    textl = text.split(' ')\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(textl)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(textl[ictr])\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(textl[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "def windowlvl(text, lenW): # return key=entire window, value=center word, BOW version (text a list of ids)\n",
    "    if lenW % 2 == 0 or len(text) <= lenW:\n",
    "        return ([],[])\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(text)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(text[ictr]) # center encoding: would need to add a different dict here\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(text[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "# for window+center encoding: add a step for the center word:\n",
    "# translate the center back to the original with dic 0, then with dic 1 to the center encoding\n",
    "def windowclvl(text, lenW, memdict, cdict): # return key=entire window, value=center word, BOW version with center\n",
    "    if lenW % 2 == 0 or len(text) <= lenW:\n",
    "        return ([],[])\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(text)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(cdict.token2id[memdict[text[ictr]]]) # center encoding: different dict for ctr word\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(text[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    text = 'this is a test with a lot of ambiguity. docs are split by periods.'\n",
    "    memraw = docstr2lst(text)\n",
    "    memdict = corpora.Dictionary(memraw)\n",
    "    #print(memdict.token2id)\n",
    "    #print(memdict[1])\n",
    "    #print(windowlvlstr(text, 3))\n",
    "    #print(windowlvlstr(text, 5))\n",
    "    # text to bow\n",
    "    membow = text2bow(memraw, memdict)\n",
    "    print(membow)\n",
    "    print(windowlvl(membow[0], 3))\n",
    "    print(windowclvl(membow[0], 3, memdict, memdict)) # test with the same for now - should be the same result as windowlvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00000000e+00  0.00000000e+00  0.00000000e+00 -0.00000000e+00\n",
      "   1.81831121e-07 -0.00000000e+00  0.00000000e+00  1.12753895e-08\n",
      "  -0.00000000e+00  0.00000000e+00]]\n",
      "[[-0.  0.  0. -0.  0. -0.  0.  0. -0.  0.]]\n",
      "[[ 8.04376336e-05 -5.65068638e-05 -7.77747678e-05  1.29369767e-04\n",
      "  -1.76518582e-04  7.90246546e-05 -2.17529744e-04 -1.93975479e-05\n",
      "   4.26013069e-05 -6.47007077e-05]]\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network with ReLU example\n",
    "import numpy as np\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # parameters\n",
    "    inp_size = 10 # input size\n",
    "    etha = 0.1 # learning rate\n",
    "    niter = 100\n",
    "\n",
    "    # input\n",
    "    x = np.zeros((1, inp_size)) # input\n",
    "    x = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "\n",
    "    # model parameters\n",
    "    W1 = np.random.randn(inp_size, inp_size)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(inp_size, inp_size)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, inp_size)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, inp_size)) # hidden-out bias\n",
    "    \n",
    "    for ictr in range(niter):\n",
    "\n",
    "        # forward pass\n",
    "        h1 = np.dot(x, W1) + b1\n",
    "        h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "        o2 = np.dot(h1, W2) + b2\n",
    "        #print(o2)\n",
    "\n",
    "        # backward pass\n",
    "        y = [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]]\n",
    "        h1 = np.dot(x, W1) + b1\n",
    "        dW1 = - etha * (o2 - y) * np.maximum(h1, 0, h1)\n",
    "        dW2 = dW1 * ((h1 > 0) * 1.) * x\n",
    "        \n",
    "        W1 += dW1\n",
    "        W2 += dW2\n",
    "        \n",
    "    print(dW1)\n",
    "    print(dW2)\n",
    "    # forward pass\n",
    "    h1 = np.dot(x, W1) + b1\n",
    "    h1 = np.maximum(h1, 0, h1) # ReLU\n",
    "    o2 = np.dot(h1, W2) + b2\n",
    "    print(o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bathroom': 0, 'mary': 1, 'moved': 2, 'the': 3, 'to': 4, '': 5, 'hallway': 6, 'john': 7, 'went': 8, 'is': 9, 'where': 10}\n",
      "[0.09090772 0.09089124 0.09092105 0.09091639 0.09092264 0.0908501\n",
      " 0.09094262 0.09092464 0.09092496 0.09084815 0.09095049]\n",
      "where\n"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "def embed(facts_raw, query_raw, uniq):\n",
    "    facts = facts_raw.lower().split('.')\n",
    "    facts.append(query_raw.lower())\n",
    "    #senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    senttoken = []\n",
    "    for idoc in range(len(facts)):\n",
    "        thissen = facts[idoc].lower().split(' ')\n",
    "        tokendoc = []\n",
    "        for idx, thiswrd in enumerate(thissen):\n",
    "            if uniq:\n",
    "                tokendoc.append(thiswrd + '_' + str(idoc)) # for seaparate encoding (Jonathan Huis blog)\n",
    "            else:\n",
    "                tokendoc.append(thiswrd)\n",
    "        senttoken.append(tokendoc)\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    facts.pop(len(facts)-1) # query at the end of document\n",
    "    d_embed = len(dictionary) # embedding dimension\n",
    "    return(senttoken, facts, dictionary, d_embed)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    lr = 1.3 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    facts_raw = 'Mary moved to the bathroom. John went to the hallway.'\n",
    "    \n",
    "    # create key-value pairs\n",
    "    query_raw = 'Where is Mary'\n",
    "    \n",
    "    senttoken, facts, dictionary, d_embed = embed(facts_raw, query_raw, False)\n",
    "    voc = d_embed\n",
    "    n_memories = len(facts)\n",
    "    \n",
    "    print(dictionary.token2id)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding = key-value and query embeddings\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding = for y-embeddings\n",
    "    R1 = np.random.randn(voc, d_embed)*0.01 # final weight matrix = R1 for hop-to-hop query updates\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mB = np.zeros_like(B)\n",
    "    mR1 = np.zeros_like(R1)\n",
    "\n",
    "    phi_k = np.zeros((n_memories, voc)) # keys\n",
    "    phi_v = np.zeros((n_memories, voc)) # values\n",
    "    phi_y = np.zeros(voc) # candidates\n",
    "    for i in range(n_memories):\n",
    "        for j in range(len(senttoken[i])):\n",
    "            phi_k[i][j] = dictionary.token2id[senttoken[i][j]]\n",
    "            phi_v[i][j] = dictionary.token2id[senttoken[i][j]] # for now the same as keys\n",
    "    \n",
    "    for i in range(len(dictionary)):\n",
    "        phi_y[i] = i # each word in the vocabulary is a potential candidate\n",
    "        \n",
    "    # todo: key hashing (inverted index)\n",
    "         \n",
    "    for iterctr in range(500):\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        # embedding key = A * phi_k\n",
    "        key = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            key[i] = np.dot(A, phi_k[i].T) # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] # with positional encoding\n",
    "\n",
    "        # embedding query q = A * phi_x\n",
    "        phi_x = np.zeros(voc)\n",
    "        qj1 = np.zeros(d_embed)\n",
    "        thissent = senttoken[3] # test query: where is mary\n",
    "        for j in range(len(thissent)):\n",
    "            phi_x[j] = dictionary.token2id[thissent[j]]\n",
    "        qj1 = np.dot(A, phi_x)\n",
    "        \n",
    "        # key addressing\n",
    "\n",
    "        # read head of the Neural Turing Machine\n",
    "        # match of query with key p = softmax(qj1 * A * key) for all i\n",
    "        pk = np.zeros((n_memories, d_embed))\n",
    "        pk = softmax(np.dot(qj1, key.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi\n",
    "        val = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            val[i] = np.dot(A, phi_v[i].T)\n",
    "            \n",
    "        # value reading\n",
    "\n",
    "        # internal state of the controller\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(pk.T, val)\n",
    "        \n",
    "        # 2nd hop\n",
    "        qj1 = np.dot(R1, (qj1 + o))\n",
    "        #pk = softmax(np.dot(qj1, np.dot(A, key.T)))\n",
    "        pk = softmax(np.dot(qj1, key.T))\n",
    "        o = np.dot(pk.T, val)\n",
    "        qj1 = np.dot(R1, (qj1 + o))\n",
    "        \n",
    "        # 3rd hop\n",
    "        qj1 = np.dot(R1, (qj1 + o))\n",
    "        #pk = softmax(np.dot(qj1, np.dot(A, key.T)))\n",
    "        pk = softmax(np.dot(qj1, key.T))\n",
    "        o = np.dot(pk.T, val)\n",
    "        qj1 = np.dot(R1, (qj1 + o))\n",
    "\n",
    "        # predicted label a = softmax( R1 * (o + u) * B * phi_y)\n",
    "        #a_predict = softmax(np.dot(R1, (qj1 + o)))\n",
    "        a_predict = softmax(qj1 * np.dot(B, phi_y))\n",
    "        #a_predict = softmax(qj1 * np.dot(A, phi_y)) # only using A\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dR1 = np.zeros_like(R1)\n",
    "\n",
    "        truth = np.zeros(voc)\n",
    "        truth[0] = 1. # answer (bathroom)\n",
    "        dy = a_predict - truth\n",
    "        #print(dy)\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        R1unit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "\n",
    "        # dA = dy a_predict * (1-a_predict) R1 (phi_x + sumi ( p[i] (1-p[i]) ( phi_x A phi_K + A phi_x phi_K ) A phi_V + pki phi_V)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += pk[i]*(1.-pk[i]) * np.dot(np.dot(qj1, np.dot(ABunit, phi_k[i].T)), val[i])\n",
    "        dEAtmp = R1 * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) q phi_y\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += pk[i]*(1.-pk[i])*np.dot(np.dot(ABunit, qj1), phi_y[i])\n",
    "        dEAtmp = R1 * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dR1 = dy a_predict * (1-a_predict) (q + o) B phi_y\n",
    "        #print(np.shape(np.dot((qj1 + o), np.dot(B, phi_y.T))))\n",
    "        dR1 = np.dot(dy, a_predict*(1-a_predict)) * R1unit * np.dot((qj1 + o), np.dot(B, phi_y.T))\n",
    "        #print(dR1)   \n",
    "\n",
    "        # maybe clip ?\n",
    "        #for dweights in [dA,dB,dR1]:\n",
    "            #np.clip(dweights, -5., 5., out=dweights) # exploding gradients (but seems well-behaved enough)\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,R1], [dA,dB,dR1], [mA,mB,mR1]):\n",
    "            #memwghts += dweights * dweights\n",
    "            #weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "            weights += -lr * dweights\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'lot', 'ambigu']\n"
     ]
    }
   ],
   "source": [
    "# prepare text file as corpus (lower case, remove stopwords) and tokenize\n",
    "import os\n",
    "import re # regex\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    text.append(str(chunk)) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "def chunks(l, n): # Yield successive n-sized chunks from list l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n] # returns a generator\n",
    "\n",
    "def chunksep(l, s): # Yield successive chunks from list l separated by s\n",
    "    g = []\n",
    "    for el in l:\n",
    "        if el == s:\n",
    "            yield g\n",
    "            g = []\n",
    "        g.append(el)\n",
    "    yield g\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    text = 'this is a test with a lot of ambiguity.'\n",
    "    #corpus = preprocess_string(' '.join(text)) # requires string\n",
    "    corpus = preprocess_string(text)\n",
    "    print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2, 0, 5, 7, 0, 3, 4, 1], [8, 11, 9, 13, 10, 12], [8]]\n",
      "([[6, 2, 0], [2, 0, 5], [0, 5, 7], [5, 7, 0], [7, 0, 3], [0, 3, 4], [3, 4, 1]], [2, 0, 5, 7, 0, 3, 4])\n",
      "([[6, 2, 0], [2, 0, 5], [0, 5, 7], [5, 7, 0], [7, 0, 3], [0, 3, 4], [3, 4, 1]], [2, 0, 5, 7, 0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# key-value memories: first embedding tests\n",
    "from gensim import corpora\n",
    "\n",
    "def docstr2lst(text): # text string with sentences separated by ., returns list of lists of sentences\n",
    "    memraw = []\n",
    "    for mem in text.split('.'):\n",
    "        memraw.append(mem.split(' '))\n",
    "    return memraw\n",
    "\n",
    "def text2bow(memraw, memdict): # raw text to bow (maps each token to its id, takes a list of lists of sentence words)\n",
    "    membow = []\n",
    "    for mem in memraw:\n",
    "        memline = []\n",
    "        for memw in mem:\n",
    "            memline.append(memdict.token2id[memw])\n",
    "        membow.append(memline)\n",
    "    return membow\n",
    "\n",
    "def windowlvlstr(text, lenW): # return key=entire window, value=center word, string version\n",
    "    if lenW % 2 == 0 or len(text) <= lenW:\n",
    "        return ([],[])\n",
    "    textl = text.split(' ')\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(textl)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(textl[ictr])\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(textl[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "def windowlvl(text, lenW): # return key=entire window, value=center word, BOW version (text a list of ids)\n",
    "    if lenW % 2 == 0 or len(text) <= lenW:\n",
    "        return ([],[])\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(text)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(text[ictr]) # center encoding: would need to add a different dict here\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(text[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "# for window+center encoding: add a step for the center word:\n",
    "# translate the center back to the original with dic 0, then with dic 1 to the center encoding\n",
    "def windowclvl(text, lenW, memdict, cdict): # return key=entire window, value=center word, BOW version with center\n",
    "    if lenW % 2 == 0 or len(text) <= lenW:\n",
    "        return ([],[])\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(text)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(cdict.token2id[memdict[text[ictr]]]) # center encoding: different dict for ctr word\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(text[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    text = 'this is a test with a lot of ambiguity. docs are split by periods.'\n",
    "    memraw = docstr2lst(text)\n",
    "    memdict = corpora.Dictionary(memraw)\n",
    "    #print(memdict.token2id)\n",
    "    #print(memdict[1])\n",
    "    #print(windowlvlstr(text, 3))\n",
    "    #print(windowlvlstr(text, 5))\n",
    "    # text to bow\n",
    "    membow = text2bow(memraw, memdict)\n",
    "    print(membow)\n",
    "    print(windowlvl(membow[0], 3))\n",
    "    print(windowclvl(membow[0], 3, memdict, memdict)) # test with the same for now - should be the same result as windowlvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texts\n",
      "{'InvertedIndex.txt': ['it',\n",
      "                       'is',\n",
      "                       'what',\n",
      "                       'it',\n",
      "                       'is',\n",
      "                       'what',\n",
      "                       'is',\n",
      "                       'it',\n",
      "                       'it',\n",
      "                       'is',\n",
      "                       'a',\n",
      "                       'banana']}\n",
      "\n",
      "Words\n",
      "['a', 'banana', 'is', 'it', 'what']\n",
      "\n",
      "Inverted Index\n",
      "{'a': ['InvertedIndex.txt'],\n",
      " 'banana': ['InvertedIndex.txt'],\n",
      " 'is': ['InvertedIndex.txt'],\n",
      " 'it': ['InvertedIndex.txt'],\n",
      " 'what': ['InvertedIndex.txt']}\n",
      "\n",
      "Term Search for: ['what', 'is', 'it']\n",
      "['InvertedIndex.txt']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This implements: http://en.wikipedia.org/wiki/Inverted_index of 28/07/10\n",
    "'''\n",
    " \n",
    "from pprint import pprint as pp\n",
    "from glob import glob\n",
    "try: reduce\n",
    "except: from functools import reduce\n",
    "try:    raw_input\n",
    "except: raw_input = input\n",
    " \n",
    " \n",
    "def parsetexts(fileglob='InvertedIndex.txt'):\n",
    "    texts, words = {}, set()\n",
    "    for txtfile in glob(fileglob):\n",
    "        with open(txtfile, 'r') as f:\n",
    "            txt = f.read().split()\n",
    "            words |= set(txt)\n",
    "            texts[txtfile.split('\\\\')[-1]] = txt\n",
    "    return texts, words\n",
    " \n",
    "def termsearch(terms): # Searches simple inverted index\n",
    "    return reduce(set.intersection,\n",
    "                  (invindex[term] for term in terms),\n",
    "                  set(texts.keys()))\n",
    " \n",
    "texts, words = parsetexts()\n",
    "print('\\nTexts')\n",
    "pp(texts)\n",
    "print('\\nWords')\n",
    "pp(sorted(words))\n",
    "\n",
    "invindex = {word:set(txt\n",
    "                        for txt, wrds in texts.items() if word in wrds)\n",
    "            for word in words}\n",
    "print('\\nInverted Index')\n",
    "pp({k:sorted(v) for k,v in invindex.items()})\n",
    " \n",
    "terms = [\"what\", \"is\", \"it\"]\n",
    "print('\\nTerm Search for: ' + repr(terms))\n",
    "pp(sorted(termsearch(terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Inverted Index\n",
      "{'a': [('InvertedIndex.txt', 10)],\n",
      " 'banana': [('InvertedIndex.txt', 11)],\n",
      " 'is': [('InvertedIndex.txt', 1),\n",
      "        ('InvertedIndex.txt', 4),\n",
      "        ('InvertedIndex.txt', 6),\n",
      "        ('InvertedIndex.txt', 9)],\n",
      " 'it': [('InvertedIndex.txt', 0),\n",
      "        ('InvertedIndex.txt', 3),\n",
      "        ('InvertedIndex.txt', 7),\n",
      "        ('InvertedIndex.txt', 8)],\n",
      " 'what': [('InvertedIndex.txt', 2), ('InvertedIndex.txt', 5)]}\n",
      "\n",
      "Term Search on full inverted index for: ['what', 'is', 'it']\n",
      "['InvertedIndex.txt']\n",
      "\n",
      "Phrase Search for: \"what is it\"\n",
      "['InvertedIndex.txt']\n",
      "\n",
      "Phrase Search for: \"it is\"\n",
      "['InvertedIndex.txt', 'InvertedIndex.txt', 'InvertedIndex.txt']\n",
      "  The phrase is found most commonly in text: 'InvertedIndex.txt'\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    " \n",
    "def termsearch(terms): # Searches full inverted index\n",
    "    if not set(terms).issubset(words):\n",
    "        return set()\n",
    "    return reduce(set.intersection,\n",
    "                  (set(x[0] for x in txtindx)\n",
    "                   for term, txtindx in finvindex.items()\n",
    "                   if term in terms),\n",
    "                  set(texts.keys()) )\n",
    " \n",
    "def phrasesearch(phrase):\n",
    "    wordsinphrase = phrase.strip().strip('\"').split()\n",
    "    if not set(wordsinphrase).issubset(words):\n",
    "        return set()\n",
    "    #firstword, *otherwords = wordsinphrase # Only Python 3\n",
    "    firstword, otherwords = wordsinphrase[0], wordsinphrase[1:]\n",
    "    found = []\n",
    "    for txt in termsearch(wordsinphrase):\n",
    "        # Possible text files\n",
    "        for firstindx in (indx for t,indx in finvindex[firstword]\n",
    "                          if t == txt):\n",
    "            # Over all positions of the first word of the phrase in this txt\n",
    "            if all( (txt, firstindx+1 + otherindx) in finvindex[otherword]\n",
    "                    for otherindx, otherword in enumerate(otherwords) ):\n",
    "                found.append(txt)\n",
    "    return found\n",
    " \n",
    "\n",
    "finvindex = {word:set((txt, wrdindx)\n",
    "                      for txt, wrds in texts.items()\n",
    "                      for wrdindx in (i for i,w in enumerate(wrds) if word==w)\n",
    "                      if word in wrds)\n",
    "             for word in words}\n",
    "print('\\nFull Inverted Index')\n",
    "pp({k:sorted(v) for k,v in finvindex.items()})\n",
    " \n",
    "print('\\nTerm Search on full inverted index for: ' + repr(terms))\n",
    "pp(sorted(termsearch(terms)))\n",
    " \n",
    "phrase = '\"what is it\"'\n",
    "print('\\nPhrase Search for: ' + phrase)\n",
    "print(phrasesearch(phrase))\n",
    " \n",
    "# Show multiple match capability\n",
    "phrase = '\"it is\"'\n",
    "print('\\nPhrase Search for: ' + phrase)\n",
    "ans = phrasesearch(phrase)\n",
    "print(ans)\n",
    "ans = Counter(ans)\n",
    "print('  The phrase is found most commonly in text: ' + repr(ans.most_common(1)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import functools\n",
    "from itertools import chain\n",
    "import copy\n",
    "\n",
    "def save_pickle(d, path):\n",
    "    print('save pickle to', path)\n",
    "    with open(path, mode='wb') as f:\n",
    "        pickle.dump(d, f)\n",
    "\n",
    "def load_pickle(path):\n",
    "    print('load', path)\n",
    "    with open(path, mode='rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def lower_list(word_list):\n",
    "    return [w.lower() for w in word_list]\n",
    "\n",
    "def load_entities(path):\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        entities = [e.lower().rstrip() for e in lines]\n",
    "        return list(set(entities))\n",
    "\n",
    "def find_ngrams(token_dict, text, n):\n",
    "    \"\"\" See: https://github.com/facebookresearch/ParlAI/blob/master/parlai/core/dict.py#L31\n",
    "        token_dict:  {'hello world', 'ol boy'}\n",
    "        text: ['hello', 'world', 'buddy', 'ol', 'boy']\n",
    "        n: max n of n-gram\n",
    "        ret: ['hello world', 'buddy', 'ol boy']\n",
    "    \"\"\"\n",
    "    \"\"\"Breaks text into ngrams that appear in ``token_dict``.\"\"\"\n",
    "    # base case\n",
    "    if n <= 1:\n",
    "        return text\n",
    "    # tokens committed to output\n",
    "    saved_tokens = []\n",
    "    # tokens remaining to be searched in sentence\n",
    "    search_tokens = text[:]\n",
    "    # tokens stored until next ngram found\n",
    "    next_search = []\n",
    "    while len(search_tokens) >= n:\n",
    "        ngram = ' '.join(search_tokens[:n])\n",
    "        if ngram in token_dict:\n",
    "            # first, search previous unmatched words for smaller ngrams\n",
    "            sub_n = min(len(next_search), n - 1)\n",
    "            saved_tokens.extend(find_ngrams(token_dict, next_search, sub_n))\n",
    "            next_search.clear()\n",
    "            # then add this ngram\n",
    "            saved_tokens.append(ngram)\n",
    "            # then pop this ngram from the remaining words to search\n",
    "            search_tokens = search_tokens[n:]\n",
    "        else:\n",
    "            next_search.append(search_tokens.pop(0))\n",
    "    remainder = next_search + search_tokens\n",
    "    sub_n = min(len(remainder), n - 1)\n",
    "    saved_tokens.extend(find_ngrams(token_dict, remainder, sub_n))\n",
    "    return saved_tokens\n",
    "\n",
    "def load_task(fpath):\n",
    "    print('load', fpath)\n",
    "    with open (fpath, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        data = []\n",
    "        for i, l in enumerate(lines):\n",
    "            l = l.rstrip()\n",
    "            turn, left = l.split(' ', 1)\n",
    "            \n",
    "            if '\\t' in left: # question\n",
    "                q, a = left.split('\\t', 1)\n",
    "                q = q.split('?', 1)[1] # use split by n-gram\n",
    "                q = q.split(' 1:')[1:]\n",
    "                q = lower_list(q)\n",
    "            \n",
    "                a = a.split(', ') # may contain several labels\n",
    "                a = lower_list(a)\n",
    "                data.append((q, a))\n",
    "    return data\n",
    "    # data_q = load_questions(q_fpath)\n",
    "    # data_a = load_answers(a_fpath)\n",
    "    # return [(q, a) for q, a in zip(data_q, data_a)]\n",
    "\n",
    "# def load_task(fpath, token_dict=None, max_token_length=None):\n",
    "#     # print('load', fpath)\n",
    "#     # with open (fpath, encoding='utf-8') as f:\n",
    "#     #     lines = f.readlines()\n",
    "#     #     data, story = [], []\n",
    "#     #     for i, l in enumerate(lines):\n",
    "#     #         if i % 2000 == 0: print(i, '/', len(lines))\n",
    "#     #         l = l.rstrip()\n",
    "#     #         turn, left = l.split(' ', 1)\n",
    "            \n",
    "#     #         if turn == '1': # new story\n",
    "#     #             story = []\n",
    "\n",
    "#     #         if '\\t' in left: # question\n",
    "#     #             q, a = left.split('\\t', 1)\n",
    "#     #             if q[-1] == '?':\n",
    "#     #                 q = q[:-1]\n",
    "#     #             # q = word_tokenize(q)\n",
    "#     #             q = q.split(' ')\n",
    "#     #             q = lower_list(q)\n",
    "#     #             if token_dict and max_token_length:\n",
    "#     #                 q = find_ngrams(token_dict, q, max_token_length)\n",
    "\n",
    "#     #             if '\\t' in a:\n",
    "#     #                 a = a.split('\\t')[0] # discard reward\n",
    "#     #             a = a.split('|') # may contain several labels\n",
    "#     #             a = lower_list(a)\n",
    "\n",
    "#     #             substory = [x for x in story if x]\n",
    "\n",
    "#     #             data.append((substory, q, a))\n",
    "#     #             story.append('')\n",
    "#     #         else: # normal sentence\n",
    "#     #             # s = word_tokenize(left)\n",
    "#     #             s = left.split(' ')\n",
    "#     #             if s[-1] == '.':\n",
    "#     #                 s = s[:-1]\n",
    "#     #             s = lower_list(s)\n",
    "#     #             story.append(s)\n",
    "\n",
    "#     # return data\n",
    "\n",
    "def vectorize(data, w2i, query_maxlen, w2i_label, use_multi_label=False):\n",
    "    Q, A = [], []\n",
    "    for question, answer in data:\n",
    "        # Vectroize question\n",
    "        q = [w2i[w] for w in question if w in w2i]\n",
    "        q = q[:query_maxlen]\n",
    "        q_pad_len = max(0, query_maxlen - len(q))\n",
    "        q += [0] * q_pad_len\n",
    "\n",
    "#         y = np.zeros(len(w2i_label))\n",
    "#         y[w2i_label[answer[0]]] = 1\n",
    "        y = np.zeros(len(w2i_label))\n",
    "        if use_multi_label:\n",
    "            for a in answer:\n",
    "                y[w2i_label[a]] = 1\n",
    "        else:\n",
    "            y[w2i_label[answer[0]]] = 1\n",
    "\n",
    "        Q.append(q)\n",
    "        A.append(y)\n",
    "    \n",
    "    Q = np.array(Q, dtype=np.uint32)\n",
    "    A = np.array(A, dtype='byte')\n",
    "\n",
    "    return Q, A\n",
    "\n",
    "def load_kv_pairs(path, token_dict, max_token_length, is_save_pickle=False):\n",
    "    \"\"\"load key-value paris from KB\"\"\"\n",
    "    # rel = ['directed_by', 'written_by', 'starred_actors', 'release_year', 'has_genre', 'has_tags', 'has_plot', 'in_language'] # TODO hard code\n",
    "    rel = ['directed_by', 'written_by', 'starred_actors', 'release_year', 'has_genre', 'has_tags', 'in_language'] # TODO hard code, not use has_plot tag\n",
    "    kv_pairs = []\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, l in enumerate(lines):\n",
    "            if i % 5000 == 0: print('load_kv_pairs:', i, '/', len(lines))\n",
    "            if l == '\\n': continue\n",
    "            turn, left = l.rstrip().split(' ', 1)\n",
    "            for r in rel:\n",
    "                if r in left:\n",
    "                    k, v = [], []\n",
    "                    tmp = left.split(r)\n",
    "                    lhs = tmp[0].rstrip().lower()\n",
    "                    k.append(lhs)\n",
    "                    k.append(r)\n",
    "                    rhs = tmp[1].strip().lower()\n",
    "                    vals = rhs.split(', ')\n",
    "                    for v in vals:\n",
    "                        kv_pairs.append((k, [v]))\n",
    "\n",
    "                        # double KB by considering reversed relation. see 3.2\n",
    "                        k_r = [v, '!'+r]\n",
    "                        v_r = [lhs]\n",
    "                        kv_pairs.append((k_r, v_r))\n",
    "\n",
    "                    break\n",
    "\n",
    "    if is_save_pickle:\n",
    "        save_pickle(kv_pairs, 'pickle/mov_kv_pairs.pickle')\n",
    "\n",
    "    return kv_pairs\n",
    "\n",
    "def vectorize_kv(data, max_mem_len, max_mem_size, w2i):\n",
    "    all_vec_list = []\n",
    "    for i, kv_list in enumerate(data):\n",
    "        if i % 5000 == 0: print('vectorize_kv:', i, '/', len(data))\n",
    "        vec_list = []\n",
    "        for kv in kv_list[:max_mem_len+100]: #TODO: +100 for unknown entity in w2i\n",
    "            vec = [w2i[e] for e in kv if e in w2i]\n",
    "            # vec = [w2i[e] for e in kv]\n",
    "            vec = vec[:max_mem_len]\n",
    "            mem_pad_len = max(0, max_mem_len - len(vec))\n",
    "            vec = vec + [0] * mem_pad_len\n",
    "            vec_list.append(vec)\n",
    "        vec_list = vec_list[:max_mem_size]\n",
    "        mem_pad_size = max(0, max_mem_size - len(vec_list))\n",
    "        for _ in range(mem_pad_size):\n",
    "            vec_list.append([0] * max_mem_len)\n",
    "        all_vec_list.append(vec_list)\n",
    "\n",
    "    return np.array(all_vec_list, dtype=np.uint32)\n",
    "\n",
    "def load_kv_dataset(data, kv_pairs, stopwords):\n",
    "    print('---',len(data), len(kv_pairs))\n",
    "    data_k, data_v = [], []\n",
    "    for i, (q, _) in enumerate(data):\n",
    "        if i%100 == 0: print('load_kv_dataset:', i, '/', len(data))\n",
    "        k_list, v_list = [], []\n",
    "        for w in q:\n",
    "            if w not in stopwords:\n",
    "                for kv_ind, (k, v) in enumerate(kv_pairs):\n",
    "                    if w in (k): # the key shares at least one word with question with F<1000\n",
    "                        k_list.append(k)\n",
    "                        v_list.append(v)\n",
    "        if len(k_list) == 0:\n",
    "            print('==================no kv!')\n",
    "            print(q)\n",
    "        if len(k_list) > 100:\n",
    "            print('==================too many kv! > 100')\n",
    "            print(q)\n",
    "            print(len(k_list))\n",
    "        data_k.append(k_list)\n",
    "        data_v.append(v_list)\n",
    "        \n",
    "    return data_k, data_v\n",
    "\n",
    "def get_stop_words(data, freq, token_dict, max_token_length, is_save_pickle):\n",
    "    # train_data = load_task('./data/movie_dialog_dataset/task1_qa/task1_qa_pipe_train.txt')\n",
    "    # test_data = load_task('./data/movie_dialog_dataset/task1_qa/task1_qa_pipe_test.txt')\n",
    "    # dev_data = load_task('./data/movie_dialog_dataset/task1_qa/task1_qa_pipe_dev.txt')\n",
    "    # data = train_data + test_data + dev_data\n",
    "    bow = {}\n",
    "    for i, (q, _) in enumerate(data):\n",
    "        if i % 2000 == 0: print(i, '/', len(data))\n",
    "        for qq in q:\n",
    "            q_tokens = find_ngrams(token_dict, qq.split(' '), max_token_length)\n",
    "            for w in q_tokens:\n",
    "                if w not in bow:\n",
    "                    bow[w] = 0\n",
    "                else:\n",
    "                    bow[w] += 1\n",
    "\n",
    "    stopwords = [k for k, v in bow.items() if v >= freq]\n",
    "    if is_save_pickle:\n",
    "        save_pickle(stopwords, 'pickle/mov_stopwords.pickle')\n",
    "\n",
    "    return stopwords\n",
    "\n",
    "def filter_data(data, data_k, data_v, kv_min, kv_max):\n",
    "    indices = []\n",
    "    for i, k in enumerate(data_k):\n",
    "        if len(k) > kv_min and len(k) <= kv_max:\n",
    "            indices.append(i)\n",
    "    data = [data[i] for i in indices]\n",
    "    data_k = [data_k[i] for i in indices]\n",
    "    data_v = [data_v[i] for i in indices]\n",
    "    return data, data_k, data_v\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- entities\n",
    "    entities = load_pickle('pickle/mov_entities.pickle')\n",
    "    # entities = load_entities('./data/movieqa/knowledge_source/entities.txt')\n",
    "    # save_pickle(entities, 'mov_entities.pickle')\n",
    "    # max_entity_length = max(map(len, (e.split(' ') for e in entities))) # for searching n-gram\n",
    "    # vocab = load_pickle('pickle/mov_vocab.pickle')\n",
    "\n",
    "    # --- movie-qa train/test dataset\n",
    "    train_data = load_task('./data/WikiMovies/train_1.txt')\n",
    "    test_data = load_task('./data/WikiMovies/test_1.txt')\n",
    "    dev_data = load_task('./data/WikiMovies/dev_1.txt')\n",
    "    save_pickle(train_data, 'pickle/mov_task1_qa_pipe_train.pickle')\n",
    "    save_pickle(test_data, 'pickle/mov_task1_qa_pipe_test.pickle')\n",
    "    save_pickle(dev_data, 'pickle/mov_task1_qa_pipe_dev.pickle')\n",
    "    # train_data = load_pickle('pickle/mov_task1_qa_pipe_train.pickle')\n",
    "    # test_data = load_pickle('pickle/mov_task1_qa_pipe_test.pickle')\n",
    "    # dev_data = load_pickle('pickle/mov_task1_qa_pipe_dev.pickle')\n",
    "    print(len(train_data))\n",
    "\n",
    "    # -- update vocab and w2i/i2w\n",
    "    vocab = set(entities \n",
    "                + ['directed_by', 'written_by', 'starred_actors', 'release_year', 'has_genre', 'has_tags', 'in_language'] \n",
    "                + ['!directed_by', '!written_by', '!starred_actors', '!release_year', '!has_genre', '!has_tags', '!in_language'] )\n",
    "    for q, answer in train_data + test_data + dev_data:\n",
    "        vocab |= set(q + answer)\n",
    "    vocab = sorted(list(vocab))\n",
    "    save_pickle(vocab, 'pickle/mov_vocab.pickle')\n",
    "    w2i = dict((c, i) for i, c in enumerate(vocab, 1))\n",
    "    i2w = dict((i, c) for i, c in enumerate(vocab, 1))\n",
    "    save_pickle(w2i, 'pickle/mov_w2i.pickle')\n",
    "    save_pickle(i2w, 'pickle/mov_i2w.pickle')\n",
    "    # vocab = load_pickle('pickle/mov_vocab.pickle')\n",
    "    \n",
    "    # generate kv_pairs\n",
    "    # kv_pairs = load_kv_pairs('./data/movieqa/knowledge_source/wiki_entities/wiki_entities_kb.txt', entities,  100, True)\n",
    "    # kv_pairs = load_pickle('pickle/mov_kv_pairs.pickle')\n",
    "    # vec_kv_pairs = vectorize_kv_pairs(kv_pairs, 10, 30, entities)\n",
    "    \n",
    "    # generate stopwords\n",
    "    # stopwords = get_stop_words(train_data+test_data+dev_data, 1000, vocab, 100, True)\n",
    "    # stopwords = load_pickle('pickle/mov_stopwords.pickle')\n",
    "\n",
    "    # train_k, train_v = load_kv_dataset(train_data, kv_pairs, stopwords)\n",
    "    # save_pickle(train_k, 'pickle/mov_train_k.pickle')\n",
    "    # save_pickle(train_v, 'pickle/mov_train_v.pickle')\n",
    "    # test_k, test_v = load_kv_dataset(test_data, kv_pairs, stopwords)\n",
    "    # save_pickle(test_k, 'pickle/mov_test_k.pickle')\n",
    "    # save_pickle(test_v, 'pickle/mov_test_v.pickle')\n",
    "    # dev_k, dev_v = load_kv_dataset(dev_data, kv_pairs, stopwords)\n",
    "    # save_pickle(test_k, 'pickle/mov_dev_k.pickle')\n",
    "    # save_pickle(test_v, 'pickle/mov_dev_v.pickle')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['test', 'lot', 'ambigu'], ['doc', 'split', 'period']]\n",
      "{'ambigu': 0, 'lot': 1, 'test': 2, 'doc': 3, 'period': 4, 'split': 5}\n",
      "[[2, 1, 0], [3, 5, 4]]\n",
      "([[2, 1, 0]], [1])\n",
      "([[3, 5, 4]], [5])\n",
      "{210: 1, 354: 5}\n"
     ]
    }
   ],
   "source": [
    "# key-value memories: preprocess, key-value inverted index, embedding\n",
    "import os\n",
    "import re # regex\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim import corpora\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    text.append(str(chunk)) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "def chunks(l, n): # Yield successive n-sized chunks from list l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n] # returns a generator\n",
    "\n",
    "def chunksep(l, s): # Yield successive chunks from list l separated by s\n",
    "    g = []\n",
    "    for el in l:\n",
    "        if el == s:\n",
    "            yield g\n",
    "            g = []\n",
    "        g.append(el)\n",
    "    yield g\n",
    "    \n",
    "def lsttoint(lst): # todo: check for numeric char\n",
    "    res = ''\n",
    "    for e in lst:\n",
    "        res += str(e)\n",
    "    return int(res)\n",
    "\n",
    "def docstr2lst(text): # text string with sentences separated by ., returns list of lists of sentences\n",
    "    memraw = []\n",
    "    for mem in text.split('.'):\n",
    "        memraw.append(mem.split(' '))\n",
    "    return memraw\n",
    "\n",
    "def docstr2lstpre(text): # text string with sentences separated by ., returns list of lists of sentences & preprocess\n",
    "    memraw = []\n",
    "    for mem in text.split('.'):\n",
    "        thismem = preprocess_string(mem)\n",
    "        if len(thismem) > 0:\n",
    "            memraw.append(preprocess_string(mem))\n",
    "    return memraw\n",
    "\n",
    "def text2bow(memraw, memdict): # raw text to bow (maps each token to its id, takes a list of lists of sentence words)\n",
    "    membow = []\n",
    "    for mem in memraw:\n",
    "        memline = []\n",
    "        for memw in mem:\n",
    "            memline.append(memdict.token2id[memw])\n",
    "        membow.append(memline)\n",
    "    return membow\n",
    "\n",
    "def windowlvlstr(text, lenW): # return key=entire window, value=center word, string version\n",
    "    if lenW % 2 == 0 or len(text) <= lenW:\n",
    "        return ([],[])\n",
    "    textl = text.split(' ')\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(textl)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(textl[ictr])\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(textl[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "def windowlvl(text, lenW): # return key=entire window, value=center word, BOW version (text a list of ids)\n",
    "    if lenW % 2 == 0 or len(text) < lenW:\n",
    "        return ([],[])\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(text)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(text[ictr]) # center encoding: would need to add a different dict here\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(text[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "# for window+center encoding: add a step for the center word:\n",
    "# translate the center back to the original with dic 0, then with dic 1 to the center encoding\n",
    "def windowclvl(text, lenW, memdict, cdict): # return key=entire window, value=center word, BOW version with center\n",
    "    if lenW % 2 == 0 or len(text) < lenW:\n",
    "        return ([],[])\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(text)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(cdict.token2id[memdict[text[ictr]]]) # center encoding: different dict for ctr word\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(text[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "if __name__==\"__main__\":    \n",
    "    # get document data\n",
    "    text = 'this is a test with a lot of ambiguity. docs are split by periods.'\n",
    "    # preprocess and build dictionary\n",
    "    memraw = docstr2lstpre(text)\n",
    "    print(memraw)\n",
    "    memdict = corpora.Dictionary(memraw)\n",
    "    print(memdict.token2id)    \n",
    "    # text to bow\n",
    "    membow = text2bow(memraw, memdict)\n",
    "    print(membow)\n",
    "    # feature mapping and build inverted index\n",
    "    kvinvind = {}\n",
    "    for imem in range(len(membow)):\n",
    "        keyvalbow = windowlvl(membow[imem], 3)\n",
    "        print(keyvalbow)\n",
    "        for ikvb in range(len(keyvalbow[0])):\n",
    "            kvinvind[hash(lsttoint(keyvalbow[0][ikvb]))] = keyvalbow[1][0]\n",
    "    print(kvinvind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mari', 'move', 'bathroom'], ['john', 'went', 'hallwai'], ['mari']]\n",
      "{120: 2, 453: 5}\n",
      "['mari']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (6,1) and (6,) not aligned: 1 (dim 1) != 6 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-842523f786d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mdEAtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_memories\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             \u001b[0mdEAtmp\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpk\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mABunit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqj1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[0mdEAtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR1\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdEAtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[0mdB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_predict\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdEAtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (6,1) and (6,) not aligned: 1 (dim 1) != 6 (dim 0)"
     ]
    }
   ],
   "source": [
    "# key-value memory network\n",
    "import os\n",
    "import math\n",
    "import re # regex\n",
    "import numpy as np\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim import corpora\n",
    "\n",
    "def read1k():\n",
    "    return f.read(1024)\n",
    "\n",
    "def process_data(chunk, text):\n",
    "    text.append(str(chunk)) # 'utf8' codec can't decode byte 0xc3\n",
    "\n",
    "def rmsword(corpus, stopwords): # remove stopwords from corpus\n",
    "    i = 0\n",
    "    for elem in corpus:\n",
    "        for sword in stopwords:\n",
    "            if elem == sword:\n",
    "                while True:\n",
    "                    try:\n",
    "                        corpus.remove(elem) # this throws an error if elem not in corpus (might have been removed already)\n",
    "                        i += 1\n",
    "                    except:\n",
    "                        break\n",
    "    return i # returns number of stopwords removed\n",
    "\n",
    "def chunks(l, n): # Yield successive n-sized chunks from list l\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n] # returns a generator\n",
    "\n",
    "def chunksep(l, s): # Yield successive chunks from list l separated by s\n",
    "    g = []\n",
    "    for el in l:\n",
    "        if el == s:\n",
    "            yield g\n",
    "            g = []\n",
    "        g.append(el)\n",
    "    yield g\n",
    "    \n",
    "def lsttoint(lst): # todo: check for numeric char\n",
    "    res = ''\n",
    "    for e in lst:\n",
    "        res += str(e)\n",
    "    return int(res)\n",
    "\n",
    "def docstr2lst(text): # text string with sentences separated by ., returns list of lists of sentences\n",
    "    memraw = []\n",
    "    for mem in text.split('.'):\n",
    "        memraw.append(mem.split(' '))\n",
    "    return memraw\n",
    "\n",
    "def docstr2lstpre(text): # text string with sentences separated by ., returns list of lists of sentences & preprocess\n",
    "    memraw = []\n",
    "    for mem in text.split('.'):\n",
    "        thismem = preprocess_string(mem)# todo: check preprocessing, maybe just stem\n",
    "        if len(thismem) > 0:\n",
    "            memraw.append(preprocess_string(mem))\n",
    "    return memraw\n",
    "\n",
    "def text2bow(memraw, memdict): # raw text to bow (maps each token to its id, takes a list of lists of sentence words)\n",
    "    membow = []\n",
    "    for mem in memraw:\n",
    "        memline = []\n",
    "        for memw in mem:\n",
    "            memline.append(memdict.token2id[memw])\n",
    "        membow.append(memline)\n",
    "    return membow\n",
    "\n",
    "def windowlvlstr(text, lenW): # return key=entire window, value=center word, string version\n",
    "    if lenW % 2 == 0 or len(text) <= lenW:\n",
    "        return ([],[])\n",
    "    textl = text.split(' ')\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(textl)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(textl[ictr])\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(textl[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "def windowlvl(text, lenW): # return key=entire window, value=center word, BOW version (text a list of ids)\n",
    "    if lenW % 2 == 0 or len(text) < lenW:\n",
    "        return ([],[])\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(text)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(text[ictr]) # center encoding: would need to add a different dict here\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(text[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "# for window+center encoding: add a step for the center word:\n",
    "# translate the center back to the original with dic 0, then with dic 1 to the center encoding\n",
    "def windowclvl(text, lenW, memdict, cdict): # return key=entire window, value=center word, BOW version with center\n",
    "    if lenW % 2 == 0 or len(text) < lenW:\n",
    "        return ([],[])\n",
    "    retkeys = []\n",
    "    retvals = []\n",
    "    lenW2 = int((lenW-1)/2)\n",
    "    for ictr in range(lenW2, len(text)-lenW2):\n",
    "        thiskey = []\n",
    "        retvals.append(cdict.token2id[memdict[text[ictr]]]) # center encoding: different dict for ctr word\n",
    "        for ikey in range(ictr-lenW2, ictr+lenW2+1):\n",
    "            thiskey.append(text[ikey])\n",
    "        retkeys.append(thiskey)\n",
    "    return (retkeys, retvals)\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "def embed(facts_raw, query_raw, uniq):\n",
    "    facts = facts_raw.lower().split('.')\n",
    "    facts.append(query_raw.lower())\n",
    "    #senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    senttoken = []\n",
    "    for idoc in range(len(facts)):\n",
    "        thissen = facts[idoc].lower().split(' ')\n",
    "        tokendoc = []\n",
    "        for idx, thiswrd in enumerate(thissen):\n",
    "            if uniq:\n",
    "                tokendoc.append(thiswrd + '_' + str(idoc)) # for seaparate encoding (Jonathan Huis blog)\n",
    "            else:\n",
    "                tokendoc.append(thiswrd)\n",
    "        senttoken.append(tokendoc)\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    facts.pop(len(facts)-1) # query at the end of document\n",
    "    d_embed = len(dictionary) # embedding dimension\n",
    "    return(senttoken, facts, dictionary, d_embed)\n",
    "\n",
    "def preinvind(facts_raw, query_raw, wsize): # preprocess and build inverted index\n",
    "    # preprocess and build dictionary\n",
    "    memraw = docstr2lstpre(facts_raw + query_raw)\n",
    "    print(memraw)\n",
    "    memdict = corpora.Dictionary(memraw) \n",
    "    # remove query from end\n",
    "    query = memraw.pop(len(memraw)-1)\n",
    "    # text to bow\n",
    "    membow = text2bow(memraw, memdict)\n",
    "    # feature mapping and build inverted index\n",
    "    kvinvind = {}\n",
    "    for imem in range(len(membow)):\n",
    "        keyvalbow = windowlvl(membow[imem], wsize)\n",
    "        for ikvb in range(len(keyvalbow[0])):\n",
    "            kvinvind[hash(lsttoint(keyvalbow[0][ikvb]))] = keyvalbow[1][0]\n",
    "    return memdict, kvinvind, query\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    lr = 1.3 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    facts_raw = 'Mary moved to the bathroom. John went to the hallway.'\n",
    "    \n",
    "    # create key-value pairs\n",
    "    query_raw = 'Where is Mary'\n",
    "    \n",
    "    dictionary, kvinvind, query = preinvind(facts_raw, query_raw, 3) # key-value inv idx\n",
    "    d_embed = len(dictionary)\n",
    "    \n",
    "    #senttoken, facts, dictionary, d_embed = embed(facts_raw, query_raw, False)\n",
    "    voc = 1 # todo: check\n",
    "    n_memories = len(kvinvind)\n",
    "    \n",
    "    print(kvinvind)\n",
    "    print(query)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding = key-value and query embeddings\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding = for y-embeddings\n",
    "    R1 = np.random.randn(voc, d_embed)*0.01 # final weight matrix = R1 for hop-to-hop query updates\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mB = np.zeros_like(B)\n",
    "    mR1 = np.zeros_like(R1)\n",
    "\n",
    "    phi_k = np.zeros((n_memories, voc)) # keys\n",
    "    phi_v = np.zeros((n_memories, voc)) # values\n",
    "    phi_y = np.zeros(voc) # candidates\n",
    "    i = 0\n",
    "    for ik in kvinvind.keys():\n",
    "        phi_k[i] = ik\n",
    "        phi_v[i] = ik # for now the same as keys\n",
    "        i += 1\n",
    "    \n",
    "    for i in range(voc):\n",
    "        phi_y[i] = i # each word in the vocabulary is a potential candidate\n",
    "        \n",
    "    # todo: key hashing (inverted index)\n",
    "         \n",
    "    for iterctr in range(30):\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        # embedding key = A * phi_k\n",
    "        key = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            key[i] = np.dot(A, phi_k[i].T) # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] # with positional encoding\n",
    "\n",
    "        # embedding query q = A * phi_x\n",
    "        phi_x = np.zeros(voc)\n",
    "        qj1 = np.zeros((d_embed,1))\n",
    "        for j in range(len(query)):\n",
    "            phi_x[j] = dictionary.token2id[query[j]]\n",
    "        qj1 = np.dot(A, phi_x)\n",
    "        \n",
    "        # key addressing\n",
    "\n",
    "        # read head of the Neural Turing Machine\n",
    "        # match of query with key p = softmax(qj1 * A * key) for all i\n",
    "        pk = np.zeros((n_memories, d_embed))\n",
    "        pk = softmax(np.dot(qj1, key.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi\n",
    "        val = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            val[i] = np.dot(A, phi_v[i].T)\n",
    "            \n",
    "        # value reading\n",
    "\n",
    "        # internal state of the controller\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(pk.T, val)\n",
    "        \n",
    "        # 2nd hop\n",
    "        #qj1 = np.dot(R1, (qj1 + o))\n",
    "        #pk = softmax(np.dot(qj1, key.T))\n",
    "        #o = np.dot(pk.T, val)\n",
    "        #qj1 = np.dot(R1, (qj1 + o))\n",
    "        \n",
    "        # 3rd hop\n",
    "        #qj1 = np.dot(R1, (qj1 + o))\n",
    "        #pk = softmax(np.dot(qj1, key.T))\n",
    "        #o = np.dot(pk.T, val)\n",
    "        #qj1 = np.dot(R1, (qj1 + o))\n",
    "\n",
    "        # predicted label a = softmax( R1 * (o + u) * B * phi_y)\n",
    "        #a_predict = softmax(np.dot(R1, (qj1 + o)))\n",
    "        a_predict = softmax(qj1 * np.dot(B, phi_y))\n",
    "        #a_predict = softmax(qj1 * np.dot(A, phi_y)) # only using A\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dR1 = np.zeros_like(R1)\n",
    "\n",
    "        truth = np.zeros(voc)\n",
    "        truth[0] = 1. # answer (bathroom)\n",
    "        dy = a_predict - truth\n",
    "        #print(dy)\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        R1unit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "\n",
    "        # dA = dy a_predict * (1-a_predict) R1 (phi_x + sumi ( p[i] (1-p[i]) ( phi_x A phi_K + A phi_x phi_K ) A phi_V + pki phi_V)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += pk[i]*(1.-pk[i]) * np.dot(np.dot(qj1, np.dot(ABunit, phi_k[i].T)), val[i])\n",
    "        dEAtmp = R1 * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) q phi_y\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += pk[i]*(1.-pk[i])*np.dot(np.dot(ABunit, qj1), phi_y[i])\n",
    "        dEAtmp = R1 * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dR1 = dy a_predict * (1-a_predict) (q + o) B phi_y\n",
    "        #print(np.shape(np.dot((qj1 + o), np.dot(B, phi_y.T))))\n",
    "        dR1 = np.dot(dy, a_predict*(1-a_predict)) * R1unit * np.dot((qj1 + o), np.dot(B, phi_y.T))\n",
    "        #print(dR1)   \n",
    "\n",
    "        # maybe clip ?\n",
    "        #for dweights in [dA,dB,dR1]:\n",
    "            #np.clip(dweights, -5., 5., out=dweights) # exploding gradients (but seems well-behaved enough)\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,R1], [dA,dB,dR1], [mA,mB,mR1]):\n",
    "            #memwghts += dweights * dweights\n",
    "            #weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "            weights += -lr * dweights\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25079645 0.24598652 0.25242057 0.25079645]\n",
      "[0.25027876 0.24859528 0.2508472  0.25027876]\n",
      "[0.22508253 0.24898882 0.25056516 0.22530773]\n",
      "[0.21752991 0.27997169 0.28442399 0.21807441]\n",
      "[[1.06244178 0.27997169 1.06244178]\n",
      " [0.27997169 1.06244178 1.06244178]\n",
      " [1.06244178 1.06244178 0.27997169]\n",
      " [1.06244178 0.27997169 1.06244178]]\n"
     ]
    }
   ],
   "source": [
    "# neural turing machine\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def simK(u, v): # similarity measure\n",
    "    return np.dot(u, v) / np.linalg.norm(u) / np.linalg.norm(v)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # setup\n",
    "    Mt = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 0, 1]]) # memory (N slots), M=3\n",
    "    wt = np.array([0.25, 0.25, 0.25, 0.25]) # weights 1-N: one for each memory entry\n",
    "    wt_1 = wt # weights from previous time step\n",
    "    \n",
    "    # weights\n",
    "    beta = 0.1 # key strength\n",
    "    kt = np.array([0.5, 0.2, 0.1]) # key vector\n",
    "    wt = softmax(beta * simK(Mt, kt)) # content addressing\n",
    "    print(wt)\n",
    "    gt = 0.35 # interpolation gate\n",
    "    wt = gt * wt + (1 - gt) * wt_1 # gated weighing\n",
    "    print(wt)\n",
    "    st = np.array([0.1, 0.8, 0.1]) # shift weights\n",
    "    wt = np.convolve(wt, st, mode='same') # convolutional shift\n",
    "    print(wt)\n",
    "    gamma = 2.5\n",
    "    wt = np.power(wt, gamma) / np.sum(np.power(wt, gamma)) # sharpening\n",
    "    print(wt)\n",
    "    \n",
    "    # read\n",
    "    rt = np.dot(wt, Mt) # dim M, weighted memories\n",
    "\n",
    "    # write\n",
    "    et = np.array([1, 0, 0, 0]) # erase vector\n",
    "    at = np.array([0, 1, 0, 0]) # add vector\n",
    "    Mt = np.dot(Mt, (1 - np.dot(wt, et))) # erase\n",
    "    Mt = Mt + np.dot(wt, at) # add\n",
    "    print(Mt)\n",
    "\n",
    "    # fill memory\n",
    "\n",
    "\n",
    "    # move heand to start\n",
    "\n",
    "\n",
    "    # while not at memory end\n",
    "\n",
    "\n",
    "    # receive input vector\n",
    "\n",
    "\n",
    "    # write input to head\n",
    "\n",
    "\n",
    "    # increment head position by 1\n",
    "\n",
    "\n",
    "    # return head to start location\n",
    "\n",
    "\n",
    "    # while true\n",
    "\n",
    "\n",
    "    # read output vector from head location\n",
    "\n",
    "\n",
    "    # emit output\n",
    "\n",
    "\n",
    "    # increment head by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkv1  [-1  1  0] [-1  0  1] [ 0  1 -1]\n",
      "qkv2  [1 1 0] [0 1 1] [-1  0  1]\n",
      "scores  1 1\n",
      "scores corrected for dim  0.5773502691896258 0.5773502691896258\n",
      "softmax  1.0 1.0\n",
      "weighted values  [ 0.  1. -1.] [-1.  0.  1.]\n",
      "weighted sum  [-1.  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "# key-value-query attention test (example from Jay Alammars blog)\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    input_sent = 'thinking machines'\n",
    "    # embeddings\n",
    "    x1 = np.array([1, 0, 0, 0])\n",
    "    x2 = np.array([0, 1, 0, 0])\n",
    "\n",
    "    # weights\n",
    "    Wk = np.array([[-1, 0, 1], [0, 1, 1], [1, -1, 0], [1, 0, -1]])\n",
    "    Wv = np.array([[0, 1, -1], [-1, 0, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    Wq = np.array([[-1, 1, 0], [1, 1, 0], [-1, 0, 1], [-1, 1, 0]])\n",
    "\n",
    "    # query, key, value\n",
    "    q1 = np.dot(Wq.T, x1)\n",
    "    k1 = np.dot(Wk.T, x1)\n",
    "    v1 = np.dot(Wv.T, x1)\n",
    "\n",
    "    q2 = np.dot(Wq.T, x2)\n",
    "    k2 = np.dot(Wk.T, x2)\n",
    "    v2 = np.dot(Wv.T, x2)\n",
    "\n",
    "    print('qkv1 ', q1, k1, v1)\n",
    "    print('qkv2 ', q2, k2, v2)\n",
    "\n",
    "    # scores\n",
    "    s1 = np.dot(q1, k1)\n",
    "    s2 = np.dot(q2, k2)\n",
    "    print('scores ', s1, s2)\n",
    "\n",
    "    # key dimension\n",
    "    d1 = np.shape(k1)[0]\n",
    "    d2 = np.shape(k2)[0]\n",
    "    s1 /= math.sqrt(d1)\n",
    "    s2 /= math.sqrt(d2)\n",
    "    print('scores corrected for dim ', s1, s2)\n",
    "    \n",
    "    # softmax\n",
    "    s1 = softmax(s1)\n",
    "    s2 = softmax(s2)\n",
    "    print('softmax ', s1, s2)\n",
    "    \n",
    "    # weighted value vectors\n",
    "    z1 = s1 * v1\n",
    "    z2 = s2 * v2\n",
    "    print('weighted values ', z1, z2)\n",
    "    \n",
    "    # sum up weighted value vectors\n",
    "    zsum = z1 + z2\n",
    "    print('weighted sum ', zsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkvn  [[-1  1]\n",
      " [ 1  1]\n",
      " [ 0  0]] [[-1  0]\n",
      " [ 0  1]\n",
      " [ 1  1]] [[ 0 -1]\n",
      " [ 1  0]\n",
      " [-1  1]]\n",
      "scores  [[ 1  1]\n",
      " [-1  1]]\n",
      "3\n",
      "scores corrected for dim  [[ 0.57735027  0.57735027]\n",
      " [-0.57735027  0.57735027]]\n",
      "softmax  [[0.3016453  0.3016453 ]\n",
      " [0.09506409 0.3016453 ]]\n",
      "weighted values  [[-0.3016453   0.3016453   0.        ]\n",
      " [-0.3016453   0.09506409  0.20658121]]\n",
      "weighted sum  [-0.60329061  0.39670939  0.20658121]\n"
     ]
    }
   ],
   "source": [
    "# key-value-query attention test - full matrix version\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    input_sent = 'thinking machines'\n",
    "    # embeddings\n",
    "    xn = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])\n",
    "\n",
    "    # weights\n",
    "    Wk = np.array([[-1, 0, 1], [0, 1, 1], [1, -1, 0], [1, 0, -1]])\n",
    "    Wv = np.array([[0, 1, -1], [-1, 0, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    Wq = np.array([[-1, 1, 0], [1, 1, 0], [-1, 0, 1], [-1, 1, 0]])\n",
    "\n",
    "    # query, key, value\n",
    "    qn = np.dot(Wq.T, xn.T)\n",
    "    kn = np.dot(Wk.T, xn.T)\n",
    "    vn = np.dot(Wv.T, xn.T)\n",
    "\n",
    "    print('qkvn ', qn, kn, vn)\n",
    "\n",
    "    # scores\n",
    "    sn = np.dot(qn.T, kn)\n",
    "    print('scores ', sn)\n",
    "\n",
    "    # key dimension\n",
    "    dn = np.shape(kn)[0]\n",
    "    print(dn)\n",
    "    sn = np.true_divide(sn, np.sqrt(dn))\n",
    "    print('scores corrected for dim ', sn)\n",
    "    \n",
    "    # softmax\n",
    "    sn = softmax(sn)\n",
    "    print('softmax ', sn)\n",
    "    \n",
    "    # weighted value vectors\n",
    "    zn = np.dot(sn, vn.T)\n",
    "    print('weighted values ', zn)\n",
    "    \n",
    "    # sum up weighted value vectors\n",
    "    zsum = np.sum(zn, axis=0)\n",
    "    print('weighted sum ', zsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

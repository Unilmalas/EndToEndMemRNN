{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4648 characters, 15 unique.\n",
      "----\n",
      " keljlmklc ckncnfaebdjbfjklagd himnbkefijmknkicnnhiejenmggmgjg alebma cfmcaaeccfkneib caifnimacdbbalibjjdidjcbif jjmmiligllaiid kkngmjdndccadamg ekmidi hhdilfijdblgb knnlakncamcfd dahlcfidhdbkidhjd hefbln hgc ibhkckchimc njjjdimdmclkajjk hflanclkjlngkan ihegbinlbalfjgjcnnd gkickmhchnnelmn nkdanemakn ckdnglfidcggbah iiihbl ahfbellhmiglfbcikjdnlinhfhjafin mffljeljmgnedanl fibmicklgkjebmbhfnlgm ec i nnnfahnbgidldjk mlg  bemjglikckni gkli jfjefdmfihiachkjanggamnhjjhdjmagjcdkehdenialmj geeinmb ekidnccejjibfmkdmdgbihdghhhecgffgfeeldmadgh cembjg mnihkhfhmdhkabgklgccdbecd imgjhdcf dhngnnbikkfjhdekhlgaggbkablhcdmeifhelhjgkfei nhgfbamgblihe fjcdcfinjchha kdmkfnldadcammffheii  i b ah a eiklkb gnekllddjedkkhcjgijgm mianihfhn mkk fngglgdhdadhnijnhdgkjcj nkkbhnlafilikchjklkcfnbegehldndgembbjdimmabeehaneki fmffnelkncl  eljifbnkfjhbmkiic hmkifblde l dnld hgmk dgblhcgbbklic gkijfmbdhlcldnblgllnkhigaamkkjifbmmhcekladlafjmabnmi nmcm fkhkfainjn ddagcibdedm cglfjjbdgimlncegbljgndebglhfjldehehhkdlmaegfecgjgidkkeabblkfgml khhkkghkhnljleaiggfehjbhkalbgaebcaakfehcnckemeeimbe eacjmicaac jgfmffncendgfgjldh khigacbm jkc echjmdgdbkc bdiflekbdjaenhnjffhjbkncmcbchdigakfdhljgjiehfhflnnchjgf bcfnefhkdakjjnmjnffcegggejdhjadmddakn khdhkdabfjheilln aijmhm ln nbbdmmgnaakjgh ilfhinmbjembdebaedkdd k fabilbkdme gglhlcebchljgjnmihhaebbhhneeeadmkcemagnlamdfnknnmmjkeegkmhlbbjnjheiejcnbjee jcaimlafdmjnn jknnegehbcbceaaimnkbfdejmla mal dmhchcndnajchhnjbdlhnan chahicedblcniecnggaahaad emnhmmm cgjmfdagdhlg jcm njmkdm bcndl \n",
      "----\n",
      "iter 0, loss: 67.701256\n",
      "----\n",
      " mmmmmmk hh mm bbb ddddd  ff ggg l jjj jj gg hhhhhh ee fff ffff ll iii kkkkk j fff ff c c mm hii ggg eee mmm aab bb ee j cc a eee b  aaaa ba ddddddddd bbb iii jj jj k ii ff ff eee jj kk h dddddgg  ddbb bbb ee eee g iiii kkkkkk ffff lll fff e hhh lll mmmmmmmmmmmmm bbbb b bbbbbb bbbb mmm llllllll nh mmmll e ee gg mm mmmmmmmmm nee fff ff k ggggg h  a abb nniiii ff g a lee bb al mee j llll f b cggggg l bbbbb eeee kc c b nnll ee fff ff g d c bb ad bgg fff g nn d aab a fffff ffff jj kkkk ll hhh mm cccc deee kmm n ddd ee cc iii f llg ll mm iii kkkk lllll ccc cccii fff hhhhhh gg aaa bbb ddd b cc baaa bb e  m dddd iii jj fff m ff eeee ll fff hhh cc bb hhhhh bbb bbb iii ll ii lll jjjj a jj jjj k ll ljjjjjjjjjjhhhhhhhhhhhhhhhhhhhhhhhhhhhhhccccccccccccccccc nnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkfcccjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj nnnnnnnnnnnnnnnnniiiiiijjjjjjjjjjjjjjiiiiiijjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjeeejjjjjjjjjjjjjjiiijjjiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiijjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjijjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjiiiiiihddddddddhhhhhhbbbbbbbbbbcccc aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa bbbbb lllllllljjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj \n",
      "----\n",
      "iter 1000, loss: 43.792978\n",
      "----\n",
      "  ba a ee mmm ee ggg fff fff ff jjj kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmlllllllllllllllllllllllllllllnkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkfkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkknnnnnnnkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkfkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkfkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk \n",
      "----\n",
      "iter 2000, loss: 30.090359\n",
      "----\n",
      " m jjjj l b kkk iii l eee ll mmmm mm eee llllll mmmm eee jjj kk hhhhh eee lllll h bbb aaa n ccc fff fff ff jj ggg ggg jjj kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkklllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkgggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggllllaaaaaaaaaaaaaaaa fff \n",
      "----\n",
      "iter 3000, loss: 23.860667\n",
      "----\n",
      " fff jjj kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkfkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkggggggggggggkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkknnnhhhhhhhhhhhhhhhhhhhhhhhhhhhhkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkgggggggggggggggggggggggggggggggggggggggggggggggggggkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkgggggggkkkkkkkkkkkkkkkkkgggggggggggggggggggggggggggmggggggggggggggggggkkkkkkkkkkkkkkkkkkkkkkkkkkkffkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkggggggaeeeekkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkgggggggggggggggggggggggggggggggggggggkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkgggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkgggggggggggggggggggggggggggggkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkffkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkgggggggggggggggggggddddddddddddddddddddd \n",
      "----\n",
      "iter 4000, loss: 21.716236\n",
      "----\n",
      " d ddddd mmm n bbbb ccc aaa mmmmmmcc nhhiii fff jjj kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk \n",
      "----\n",
      "iter 5000, loss: 21.521960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  f hhaaaa ccc d m ddd a c b iii jjj k lll jjj kk llll hhhhhhhhh ccc d ee jjjj dd h ddddd aaaaa ccc mmm a iii eee fff d nnd  aaaa a d aaaa bbbbb ccccccc ddd n aaa bbb a ccc nn eee fff gg hhii ff hh nnn bbbgg hhhhhh ee nn cc eeee h eee d nnnn ccccc mc ee hh b aaaa bbbbb  ccc d bbdddd eee mm h iii hhhhhhhhhh iii jjj kkkk gg dd jjjj kkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkk k kkkkkk kkk kkkkkkk kkkkkkkkkkkkkkk kkkkkkk kkkkkkkkkk kkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkk kkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkk kk kkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkk kkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkk kkkkkkkkkkk kkkkkk kkkkkk kkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkk kkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk \n",
      "----\n",
      "iter 6000, loss: 21.152391\n",
      "----\n",
      " aaaaaa bcc aa aa bddddd ddd cccc a a d bbbb cccc ddd nn iiii nnn n bbbbbb bbbbbl ee nnn nn eee nnn aac b bbbbbbbbbbbbbbb bb cccc iii h eee ll l ddd a ddd mmmm eee nn mmm i fff jjj kkkkkkkkkkkkkkkkkkkggggggggggggggggkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkknjjjjjjjjjjjjjkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkggggggggkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkfkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkknnnnkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkknnhhhhhhiiijjjjjjjjkkkkkkkkkkkkkkkkkkkkkgggggkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkggllgkkkkkkkkkjjjjjjjjjjjjjjjjjjijjjjjkkkkkkkkkkkkkkkkkkkkkkkkkkkkkdddddddddddddddddbbbbbbbbbbb \n",
      "----\n",
      "iter 7000, loss: 18.808539\n",
      "----\n",
      "  ccc i  bbbbbbbb aaaaa bbb bbbbb aaaaa bbbbbbbbbbbbbbbbbbbbbbbb aa bbbbb  bbb ccc ii mm l nddddddd ddddd iii mk iii fff gg h hhh aaaabbbbbbb dd e nnnnnn aa cccc mm a a bb ccc eee fff gggg fff gg niii jjj kkkk ggg mmmfff gggg n mm jj lllllllll m mmmmm nn bb ccccc llll d ddd eee nnnnnnnn ee dd eee fff g jjj kkkk d fff ggg nnnnc aaaaaa ccc eee mm hcc ccc iii l h b aaa bbb c aaaaaa bbb aaaaacc eee g fff gg nhhhh bbbbb ccc nnnjjj k g fff jj kk n iii lll mmmm nnn ccc j d d eee jjj kkkk bbbbbbbb b aaaa mmmkd ii fff ggg nnnjjj ll m l mmmm n eee fff ggggll fff g fff g jjj kk gg jjj kkkk l mmm mmmmm n b cmmmmmm mmc jjj kkmmmmmm h c ddll nnnnn bbbcccc b cc eeee ll mmmm fff ggg mm l ee h eee lllll hhhhh ddddd bbb aaaabbbbbbbb b b aaaaa bbbbbb baaabbbb aa cc iii l mmmmmm eeee jjj kkkkkkkkkkkkkkkkk kkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk k kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk lll kkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkk kkkkkkkkk kkkkkkkkkkk ggggggggggggggggknnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnhhhnnnnnnnnnnnnnnnnnnnnfffffffffflllllgggggggggccbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbaaaaaeebbbbbbbbbbbbbbbbbb \n",
      "----\n",
      "iter 8000, loss: 20.355929\n",
      "----\n",
      " i hh m  ii g ee jjj kkl mm eee jjj kkk mk eee n iiii jjj kkn eee nn eee fff gg eee f h aa n ejjj kkknnnnd d i eeee jjj kkkn g mmmc knnnnnn g h iihh iiii ii h ff gfff jj l jj gggg iiiii jjj kk ggg l hhhhhh fff gggg jjj lll gg iii hhh hh ii hii hhhhhhh gg m eee hh d i dd m i e hhhhi d iii jjj kknn iii gfff gg g ii ln ee mm iii g nbbb m eee jjj kkkn ffff gggggg kn eeee lllbbbbbbb ccl ddd bcc dd mm ddknn k iii mbbdd ee llll hhhh fff jj g j ff gggg eeee fff gjj gggg ffff ggbbbb m bbbb e gggffff ggggb cccccc b b cc c  ccccc bbb ee mm ee fff kdd nk eee fff l iiii hhh ff g fff gg fff jj l jjj k kkk jj k eeee fff ggggg f jjj kkkld eeee mmmm iik ee fff ggg mmmmmmmmm ddddd ee hh mc cc bbl eee jjj kkkm mmmm ee nnnnknnnnnnnllknkn aaa cccccccc ccccc ddd ii m e h eee fff ggg eee ff ggggffff gggg mm aa bb aaaaaaaaaaa ccc  cccbbbbbb dd ee jjj k jj k hhc nn ggg nnnn iii h eejj jj kkkn bbbbbbb b bb bb dd iii knnknnn dd aaaaaaa lllnn iiiii ff gg g eeee fff gg jjj kknhhhhhhhii ggg fff gg mmm aaaaaaaabb ccddd ffff jj kknn ee mm ii fff ff g h i aaaa db b mmmmm ddddd ji fff ff gg fff jj kkn aaaaaaaeeee hh g eee jjj kkkl eee jjj kk jjj kkkkkkkkkkkkk kkk kkkkkkkk kkkkkkkkkk kkkkkkkkkkkkkkk kkkkkkkkk kkkkkkkk kkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkk kkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkk kkkkkkk kkkkkkkkkkkk kkkkkkkkkk kkkkk k kkkkk k kkkkkkkkkkkkkk kkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkk kkkkkkk kk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkk \n",
      "----\n",
      "iter 9000, loss: 21.810724\n",
      "----\n",
      "  dddd eeee l d nnnnnn n eee iii gg h d eee fff gf gg h m dddd bb bbbdddd ddd iiii k l nn iii jj g lll h iii ff g fff gg hh iii gg lll fff g jjj j gllll ll m nnnn mmmm eee fff g hhh eee iii jjj kkkkkk n aa b cccc bbb ccc ccm ee ll iii ggg l iii jjj kkk fff gggg fff gggggggggggg h eeee hh ff ff m e fff g m eee dd nnnnnnnnnnnn iii h mm nnn eee mmmmm fff hhh l aaa iii ff g hiii g hhhhhhhh fff ll mmmmmm mm dd dd dddd aa ccc ccc aa cccc aa cc bbb cddd n iii ff jjj kkk jjjj kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk dd gggggg fff gg jjj jj kkk iii l fff ggg hhh eee fff ggg hhhhhh iii jj k m iii jjj k ggggggggg jjjj kkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkk kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory RNN\n",
    "import numpy as np\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r', encoding=\"utf8\").read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxu = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Wuu = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Wuo = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bu = np.zeros((hidden_size, 1)) # hidden bias\n",
    "bo = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, us, os, ps = {}, {}, {}, {}\n",
    "    mi, pi, ci = {}, {}, {}\n",
    "    us[-1] = np.copy(uprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        us[t] = np.tanh(np.dot(Wxu, xs[t]) + np.dot(Wuu, us[t-1]) + bu) # hidden state\n",
    "        # pi=softmax(u*mi)\n",
    "        mi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        pi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        #ys[t] = np.dot(Wuy, us[t]) + by # unnormalized log probabilities for next chars\n",
    "        # o=pi*ci\n",
    "        ci[t] = np.dot(Wuo, us[t]) + bo\n",
    "        os[t] = pi[t] * ci[t]\n",
    "        #ps[t] = softmax(os[t]) # probabilities for next chars (=softmax)\n",
    "        ps[t] = softmax(os[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxu, dWuu, dWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "    dbu, dbo = np.zeros_like(bu), np.zeros_like(bo)\n",
    "    #dmi, dpi = np.zeros_like(mi), np.zeros_like(pi)\n",
    "    dunext = np.zeros_like(us[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        do = np.copy(ps[t])\n",
    "        do[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        \n",
    "        # pi=softmax(u*mi)\n",
    "        #dmi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        #dpi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        \n",
    "        #dWuo += np.dot(do, us[t].T)\n",
    "        dWuo += np.dot(do, np.dot(pi[t], us[t].T))\n",
    "        dbo += np.dot(do, pi[t])\n",
    "        \n",
    "        #du = np.dot(Wuo.T, do) + dunext # backprop into u\n",
    "        #print('shape ci: %s' % (str(np.shape(ci[t]))))\n",
    "        du = np.multiply(ci[t].T, (1-pi[t])*mi[t])\n",
    "        du += Wuo.T\n",
    "        duraw = (1 - us[t] * us[t]) # tanh'=1-tanh^2\n",
    "        #dbu += np.dot(do, pi[t]*np.dot(duraw, (np.multiply(ci[t].T, (1-pi[t])*mi[t]) + Wuo)))\n",
    "        dbu += np.dot(pi[t]*np.multiply(du, duraw), do)\n",
    "        \n",
    "        dWuu += duraw*us[t-1]*np.dot(du, do) \n",
    "        du = np.dot(pi[t]*du, do)\n",
    "        #print(np.shape(duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T))\n",
    "        #dWxu += duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T\n",
    "        #print(np.shape(pi[t]*(ci[t].T*(1-pi[t])*(np.dot(mi[t].T, duraw) + us[t]) + Wuo.T*duraw)*xs[t].T*do.T))\n",
    "        dWxu += pi[t]*(ci[t].T*(1-pi[t])*(np.dot(mi[t].T, duraw) + us[t]) + Wuo.T*duraw)*xs[t].T*do.T\n",
    "        \n",
    "        #dunext = np.dot(Wuu.T, duraw)\n",
    "    \n",
    "    for dparam in [dWxu, dWuu, dWuo, dbu, dbo]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    return loss, dWxu, dWuu, dWuo, dbu, dbo, us[len(inputs)-1]\n",
    "\n",
    "def sample(u, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        u = np.tanh(np.dot(Wxu, x) + np.dot(Wuu, u) + bu)\n",
    "        mmi = np.dot(Wxu, x) + bu\n",
    "        ppi = softmax(np.dot(u.T, mmi))\n",
    "        #o = np.dot(Wuo, u) + bo\n",
    "        c = np.dot(Wuo, u) + bo\n",
    "        o = ppi * c\n",
    "        p = softmax(o)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxu, mWuu, mWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "mbu, mbo = np.zeros_like(bu), np.zeros_like(bo) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        uprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0 or n == nruns-1:\n",
    "        sample_ix = sample(uprev, inputs[0], 1500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxu, dWuu, dWuo, dbu, dbo, uprev = lossFun(inputs, targets, uprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxu, Wuu, Wuo, bu, bo], \n",
    "                                [dWxu, dWuu, dWuo, dbu, dbo], \n",
    "                                [mWxu, mWuu, mWuo, mbu, mbo]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

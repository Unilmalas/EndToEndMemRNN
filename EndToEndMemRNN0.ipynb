{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 4648 characters, 15 unique.\n",
      "----\n",
      " hcgnnhkgblej aaddnmhhhljb gilfafhcfdfhaba mh ih gnblheghmi ekjmncbgemjlencfl hchdmacjml dlfefaahfkehhleabhmc aea fclabdh ndkjakngjbddnchgcik im amjggim ijj fmiadmfdfidgfgigmmekaiaihgja cfehglcebkldinjallheda iadaccfiefddlaidelineeggdinnbebabjjlklhgdgefdaklecfljiij igndcnhedcfgdhfn a elba hlljhchi nmnkcik  halbjflhbnncmkjc m hjnn alnan nkaebbgfmakf bfkcagjcnknjmjld   mngedhnmac adhgjccj dfe fhfankjeacemcjdchheliaikkmcehbmfkaembjmdhmcc jehfaafjldi licahamfmfiagnjhfegdddbcchflnhlkeifncn cilamlgngkabkfjnaefg agmejhamnjmc mjeljfb ihgkbckdfichaebkabamfdjdfknabfcbmmfkkkbgfb nm bdhbgdcjnndkihamnlbgkkjikaemcnhgmcckfjmi hnhhccalghjbjadkefeghikclemnjgnjfmfaddhnbnmffdceiidad idbgejmcljeijlcccbhadjjmmbmdcgbedacfejmejfjliggenjmggcddgcehaehlajclcjgkkcj lhhelknhnimklagemkkd dikm fni dmhbhakgaei acgegeifefacfedckcdjjjgdnihdgleb hnclbgehlkmbhhdjcb jgnbfhffb bddhhkilifinffgkkgjldlfaiildmejefbfeifgnfcnedngl ane eakmbkm efifdbiimkgdddiaabkfnfi gbaeknkklghbhcchchnjanfjglhjmfhkkkekmdnli camnklnfdbfemhknjfhafbblhnagjbhcjfnina meldceebchgi dcdiemgmjlkhnged abifcnhn kenjilkadkndgemhlemaikddmiakbgllfjfdihmelhcmbehmfmjf bcjljkmbaicdcfdkndif mihfimgdnkfclahhleffjilkfnemjdnd jlceijedinilhlcimlgdfejnbmcjec lllablgclndm  hnbmdamcghbcghmi ddhabakmifldgkghddmihdgmnkemfcmfgallel maelanadjdhjbgdd ffb nbkbibagmbnkcmdkjhenb innkkeilacffkijfjn fbnfjhhafmefcjdhlcgk agmbnhakmmmgbkb embjnbkggedhdmidccckhbamedcnehhjfncl  eadcaflecfekmn  kabedf aik akfmflkghaeikbgljmndmfihj cdbejcbdgje ng geaieeknjdlhgglcdakedhligjejjj \n",
      "----\n",
      "iter 0, loss: 67.701261\n",
      "----\n",
      " hhl fffff aa ii ddd ll nnn lggbb nnn nnn ddd mmkk bl kk aaa eeee fff ffff gg fffff m f nnn kkk jjj eeee kkkkkkhmii c ff ffff lffffg ggg cc fff fffffff g fff mb c fff ffc ff fff hh jjjjjj ggggg hhh ff ff fff kk fff ff ffff lfffffff g mmm gg nnb ccc eeeee lll n ddd ccc eee bb ddd dcc iii bb cccc jjjj kkk aaa dddd kk gdd gnnaa jjj iii bb kkk l ddd eee k jjj add nnn aaa in aaa kjjj bbbb bbb ddd nnn ggggdd aaa hh nnn naa ddd lllaaa jii bb mm hhhhh inn mmmmmec dd bbb nnn nnn nnn gddd ccc dd meeee gg hh fffffffffffff gg lll g aaa dddd aaa bbb ddd eee ff kk cccc jjj ddd llll nnn cc nnn ccc ccc kk ccc kkk nnn ddd jjj dd nnn gggg mmmmmmmmmmmmm nnn ccc bbb aaa bnn ccc aaa bb eee fffff ff ll gg bbb aaa bbb hh ccc llll jjj eeee hhhhh bb n ff lll iiii bbb miiiii ddddd aaa eeee kk mmm ff deeee mf fff bb hhh lllllff fff ffffff fffffffff  b llll aaa cc ccc eee jj aaa bbb mmmmmmm ccc ccc ccc ddd ccc bii bb lllll ggg aaa bbb b mmmmmmk mmmmeee kkkkkkk mmmmm ffffffff llllfffffff kkkkk cc gg fffffhh gg ll l kkk kkk kk jjjj ddd hhhh aaa jkk ll hhh hh aaa kkk nnnn eee dd bbb nnn eee ffffffff kkkkkkkkkjjjj gg gg kkkffffffffffffffff ggg n gg fff gg ffffffffffff ff nn hh bb g ff gggg gg ff fff fff m hh aaa ddd nnn lllllccc ccc ddd lkk iii jcc jjj aaa bb lkk ccc eeeee gg ggg ggg fffffffc hhh ggg mm ii bbb nnnn kk bb ccc bbb bbb aaaa jj ddd llll fdd ee jjj ke aaa fjj nnnn ccc aaa bbb ll mm hhh kkk bbb aaa hhh nii ddd hhhh aaa bbb kkk nnn kkk naa mnnn nnn mmmmmm lll nnnn hh f hhh iii bbb ccc ccc iiiiiii g \n",
      "----\n",
      "iter 1000, loss: 40.821775\n",
      "----\n",
      "  hhhh iik ii cc hh nnn f mmm lll ll lll mmmmm dddd  jj hhccc aaeee jjjjk l nn aaaa j iii ff nnneeeeee f cc bb aai  ii dd hhc hhh mmkkkkkkjj hh nn f mmmm gg aaa jj kk l mmmmmm mm llll l ggg fff nnii hffffff gg jjj k lll nnf ggg h mmmm jjjjjj cc kdd m gg hdddddd m l llll kkjjjjjk eeeee k hh ee ee cccc eeeeei j nn e jjjjjjl jjj ee kkkkkkjjjj ddddjcc l ggg jjjj eeec iiiikkjjjjeeaa j j kkkkkjj m ggg ii e eee ffff kkkkkkk mmm llll hhhhhhcc iieeeecccc i ee ee cc fffffffff nn iiiiee bb h idddd jjj ceec dd k gggjjj jjjj c k ll gg nnn n hhhh kkkkkkkk nn nb  hh gggfjj aaaaa bbb c iii ee  jjjjjeee n jj jj bb k hii cce bb j c cceeeee bbb ff dddd bbbb g hh hhh n gg cc m gg hhhh dddd k nnnn jjjj aaaee f dddd ee bbbbbbb eea ceeaa cc aaa ec i ee ee  iii bbb cdf ii kk mmh ddd cccc fffff llll nnnn kkkk ggddd ccc f mmmm mmm nnn mm kkkk ddd j kkk lllc bbb c kkkkiiibbb fff llll ii fffffffff llll mmm ggg f ggg h lll eeeee i nnn dddd jj cc jjjjjjj m aa  hhhhhhh iii m ii aaj ii j jjjjjj hffff llll mmm mmm ll mmmm ll ll lllmm ll mmm lll nnnn ii bbbb hh iii m iiiii cch mnn nn bbaa eee chhhh ddd j h mmkkdddd ee  kkjj cccc jjjjj l eeaa aai iiii kjjjj ee m iii nn kllii k aa jj cch e b j if gbbd j hhhcccc dd a f kkkkkkkkkkk llll bbbb bb k m n ggg k mmm mmmm eeeiii k aaa  iii eeeeeecc cc k nnnnn iiiii kkkkk gggg jjjjl hh n nnn ii kkkkkkkkkkkkk mmm lllll nnnhhccee f nnn eem nnn jj n h m mmmm nnnbb j jjjj mm jjjjjj eeeee ddd jeeeeeeeec chcccc eee b cchhkkk ggg i ee cc nnnn hhh iii hhh iiiif eeeeeee ii ee ccc  \n",
      "----\n",
      "iter 2000, loss: 28.876458\n",
      "----\n",
      " mfffff ll ddd ee cc aaa bbb bbb iiiii jjj c ggnn aaaa eee iii jjjj iii ccc aaaa kkkkk dd f m mmmmfffff mm gaa mm aabbb ff hh eee iii j ggg g nnnn eee ii iii jjj eee iii jj nnn gnn ff dg niii jjj cc eee iii jjjh iii ccc aa ggn kkkkk cck llll gggggbbb cccc nn jjj fffff eee bbb kkhh ee keee i d dd bbb cmmm kkk bbb cmm aaa fffffff bbb cccc kkk ccc nn dddd kk lll mmkk mmm nafff gg hhh jjj kk d nnnn jjj ffff bbb ckk llllllllll iiii bbb ig ggnnn aaa b fff lll ljjj ddd iii fff dd ggggggggggg kkk aaaa ii iii jjj cc gddd bbb ccc ggggnn ddd hh eee eee aaa kkk jj n llfff m ddd ff bbb cccc nnn gg lll lll h hhh bbb jjjj a hhh ff nnn eee gggn mmmm kk gbbb cc mmm kkkk ggggggggbbbbb cccc l mm kkkk d nnn ll mmm glll aa eee hh eee iii jj bbb cccc ddd kk kkk lllllll mmm lll mm nnnnn aa d l mmdd nnn aa kkkk gggddd ll mm hhh llldddd iii ccc e f mmmm nn f e nnhh iiiii cmm b ffff fffff mm hhh iii chhh iii ckkkkc aan h ddhh bbb jjjj ddd ddd eee cjj k gggggnnn aaa  dd dddd f miii jjj eee ff kkkk hhh jjj bbb cc mmmm m kkkk gggggggg ggg nnn nn ffffffff b ff l f nnii iii kkkk bbb jc f ll nnnn eee kkkk hh bbb ccc aaaa gnn dd fffff nn aaa g mm nnn fff hhh iii cc lll niii jjj kkkkk bbb ccc e iii gggggg llbbb cc gggg ggggb m ff nnn bbb chhh iii jjj bbb ccc lllll mmmm mm nnn aaa ggg g llllllllll lllllllllljj ff gg mmmm mm cc gggggggggggggmmm eee c aaa bbb cccc ll lnnn dd lllllllll kc m l ff gggggbbb cccc kkkk a hhh kkk ee eee iii jhh iii ckkk aaaa fff llllll mm nnnn dd dd ggg mmmm gggggggggggg hhh bb aa dd dd \n",
      "----\n",
      "iter 3000, loss: 25.516871\n",
      "----\n",
      " lll bn eeeef mm mm ll ffk mmmn dd l nn jjj b gg nnbb laa ee ld mm ggg hh ddd ddg dd mmf kkbb gghj cccc ii dgd e bn afe e ffff bnn eej eejee flk ll mm hhh n aa k eeeee ee f mmm hhh ccccej kk mmf hhhi nnmb hh kkk mmfk eec bb ccj bbb hhej e ee mmbb ggf aaaa ddd kk gg b e dddkn aajhh cc l mmmm eeaab hh hhh mmbb f llll gg l mmm ml gg ee mm aa jj nnn ffdd nn ddkk mmm ccjjjk bm aaa hjj kkk faiiiiibn dd ii kce eeeajh aa af cc mm hhh bbb ccc ii ldk kk lllllk mm kkk ii b hh iiib jjjc aa hh iigd ii mmn ii  nnnbbbnbnbbbn hhcc aa ggd mmff mm ccellbn b l ii dd kk mm kkkk mmmf mmn k nnn ee ggg bnk maaabn eecl ff ii jj aa lff mm aaaaa iib dd mmbbb ce ii ll eeea hhhei haa hhhhj hhc aaa fff ii ce ccaj fff dg mmm ddd a hhhjlkbb b ii hhj mm mm dddcg ll lln kkkf ee ll nn nnibb ggce ee cch ddd l mm lll hhhhd m nnbbbd kkl mmaie lllc ddd  ggg cc gg iib aab cce gggccc b gg k mmm kkk eejj ccci aajh ccc aaaiiiefm mmee jjj fk kkkk ee gg jkl hhh mmj l mmm kk mmb eeeejje iii lf nnnbbbb aa eebbnnm e lll eejfca ch ccck fff nnd kkk gggg hhj eeejcce aaj cc hh ggl eehcje gg b mmb dd mm hhc bn ii cej ii c ii ddd mmb kk gg mmf ee l jjje kkl gg dd fl cch ddd eeja mm kkl ee gg ee ffdd ddd  mmmmaa ggic aa gd kkk lkkf cejj gge gg c llcj nn mmmi nb hhd nnb jjf eee gg ed fklf c gggg eb ddd eee kkk ggg ggd gg ddddejcjchj ccec fflfej aa jkfk gggdkl eee llk mmn hh ddd ii d nnnmf mmbd mmd bb l ll mmmnb d ccaaec nn hh mlec dd cc lnb aahe nnb iib jjcc l mmmi bg ej mmaaaa ddddd kkk mmmnbii ccacc ddd l gg kkka lf iii d nn mmm \n",
      "----\n",
      "iter 4000, loss: 23.563784\n",
      "----\n",
      " d ccc nniii iaa jjjj ccc ghh ff cccc gg nmm gggg hhh kkk gggggee iii fff l nnn ddd lmmm ccc aaa iii jjj aaa ee kkk cccc eee kkk ggg nn nnn ddd eee bb ffff aaa iii ff kkkee dddd mmm dddd  mmm gl mmm ddd ddd mmm hhh eee ff eee ccc ll nnn eee kkhh aaa bbb aaa bbb ff dd hhh jjj ee ccc hhh ff ddd f aaa  mmm jjj aaa bb ccc ggg lgg mmm aaa jjj jjj llbbb fff llll hhh kk ll mm gggg mmm ggee ff ddd aaa dd aaa kkhh bb nnn ddd kkkk mff bbb bbbb iiii bb aaa bbb ff nnn ffff iii eee ccc ddd jjhh cccc ff hhh ii mm eee cccc kkk gg mmm nnn cccc aaa ddd nnn eee cc hhh d mmm gggggaaa fee ffff aaa kk kkk nnn fff lll mmm gggg mmm hhh cc kkkk eee cccl ggggg mmmm ll nnn nnn aaa bbb kk eee ffff nn hhh bbb kkk dddd dddd kkk clll lll ee cccc ffff dkkk mmm ggg mmmf hhh iia ffff bbbb iiii f mmm ddd ddd nnn c mmmm ddd lll gggaaa iiii bbbb bbb dd lll mmm ddd kkk ddd gggg llll hhh bbb ddd nnn llkkk niii dd aaa f aaa ee kk ddd eee fff aaa aaa iii ggg hhh iii bbb jj kkkk ffff lll nn llllll ll hhh bbb fff gggg ml mmm lkkk kkk mmm dddd klll nmm ggg mmm ghhh iii ddd aaa iii fff cccc aaa bbb jjj ccc fff ggg nnn aaa aaa bbb jjj mmm eee ff ccc aaa iii bbb bbb jjj ghhh bbb jjaa iaa jjj kk cccc ghhh iii kkk lll mmmm ghhh kk ccc nhh jjjj bbbb immbb kkk ddd kkd eee ee fff aaa kkk jjj ggggg cl hhh iii bbb k dd eee ff eee cccc bbb ccc gg nnn aaa cccc ccc aaa bb jjj llkkk kkkk jjj fff lll mm mmmm kkk aaa jjj aaa ccc cff eef ff aaa ccc ggg gggg hhh jjj mmm fff aaa bbb fff nnn aaa kkd kkkk cccc jjj iii aaa bbb fff h gggggcc \n",
      "----\n",
      "iter 5000, loss: 20.256937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " kk lll mmm gggg mmm ddd ggg ggg hhh lll mmm lll lll ggg gggnn aaa bbb jjj ggg hhh iii iii jl aaa bbb iii bbb bbb bb kk eee kkk aaa bbbb bmm hhh fff lll hhh kkk kkk nnn lkkk lll mmm nnn cg mmmff nnn aaa bbb ccc iii kk hhh jjjj ffff jjjj dd mmm gggl hhh ccc ggg hhh ddd eee fff ggg lll mmmm kkk eee ccc ggg mmm nnn a ggg hhh jj nnn hhh iii iii bbb fff kkk nnn kkk kkk hhh kkk gggg nnn ddd llll ll mmm kkk dddd jjj lll hhh ccc ccc lll hhh kkk nn lll mmm lll lhhh bbb ccc ee ll mmm ddd dmm iii cccc ccc kkk nnn ddd eee bbb ccc ddd ddd eee fff ddd nnn dd hha ccc ddd gggg mmm jjj eee kkk ggg mmm eee ccc kk mmm ghhh jjj cccc fff kkd gggg nnn ccc nn nnn kkk gggg mmm nnn ghhh iii ff ghh bbb bbb iii iii bbb ccc hhh fff hhh jjj eee ff lllee iii bbb iii ii ccc ddd cc aaa ffee jjj nnn l nnn eee jjk jjj eee fff gggg hhh jjj hhh ccc aaa iii iii bbb jjj ccc nnn ll eee ccc cc hhh iii aaa iii kkk ggee ee kkk eee fff hhh jjj nnn ee gghh iii iii ff mmmm hhh cc hhh kkk ll mmmff ccc kkk eee jjjj jjj ddd nnnn aaa bbbb bbbb iii ccc lll hhh fff nnn ddd kkk mmm nnn ddd ggg mmm aaa bbb iii jjj ggg mmm nnn ffff ddd nnn jhh iii jjj gg lll mm hhh jjj ee jjj kkk ggg hhh kkk nnn lll nnn gggg hhh ff ddd mmm mmm eee ccc kkk ll hhh kkk gg nnn jjj dddd ccc nnn eee jjj nnn aaa ccc aaa jjjj iii jjj kkk ggg ggg hhh dd ddd aii jjj ccc aaa ii ee jjj kkk aaa jjj eee iii jjj iii gg mmm nnnn iii bbb iii bbb iii jjj cccc ccc aaa iii iii iii bbb iii ccc eee j lll nm ggg mmm eee bbb iii bbb bbb ccc aaa iii iii bb ffee ff llll m \n",
      "----\n",
      "iter 6000, loss: 16.009554\n",
      "----\n",
      " eee jjj kkk aaa bbb ccc fff cg mmm lll nnn eee iii jjj ddd eee jjj kkk jhh ccc kk lll mmaa jjj fff ddd aaa bbb ccc aaa bbb ccc dd ddd jjj kkk ll mmm eee fff iii fff kkk ddd nn ggg hhh kkk aaa bbb cgg nnn aaa cg l ggg hhh iii fff kkk eee ccc iii jjj aaa cc kkk ggg eee ccc ccc iii fff kkk hhh iii iii jjj iii ddd kkk kkk nnn lll ddd kkk nnn nnn eee jjj jjj fff gggg hhh dd gggg h llll hhh kkk iii jjj kl nnn eee iii dd hhh aaa bb ccc eee iii aaa bb jjj dd ggg hhh fff ddd ggg mmm hhh gg hhh kkk eee jjj fff aaa bbb ccc nnn kkk kkk fff ggg jjj dd kkk hhh fff fff iii jjj iii fff nnn aai iii jjj ddd nnn llll mmm ddd lll hhh jjj ccc ddd aaa bbb ccc llkkk fff ddd aaa bbb bbb bbb bbb bnn ggg mmm hhh fff ggg mmm lll llll hhh fff ddd ddd iii jjj jjj dd mmnn kl hhh bb ccc ff ddd lll nnn eee aaa cc ccc aaa cccc bbb bb ccc ggg mmm lll hhh kkk lll hhh jjj ee fff eee fff ddd aaa cccc jjj jjj iii fff kkk nnn kkk ee kkk ggg mmm lll hhh iii fff kkk ggg ll lll mmn ddd iii ccc jjj iii iii iii ff aaa ccc hhh ccc fff eee bb aaa cc eee ccc ddd ajj jjj kkk ghhh iii iii ccc jjj kkk ggg hhh fff ccc mmm nnn aaa bbb bbb bbb bbb bmm eee ff aaa bbb bbb bbb bbb bbb jjj aaa bbb bbb bbb ccc kkk aaa bbb cccc iii cc hhh kkk aaa cg hhh iii jjj ddd eee iii ggff kkk cccc bbb bmm lll hhh aaa m lll mmm ccc aaa bbb ccc ggjjj kkk aaa bbb ccc lll mmm gggg ggg hhh jjj iii kkk eee iii iii jjj ccc jjj aaa cc fff ggg mmm eee fff jjj kkk ghhh ccc kkk aaa bbb bbb bbb bbb ccc ddd ll ggg mmm nnn eee dd nnn lll hhh iii ccc nn mmm l \n",
      "----\n",
      "iter 7000, loss: 13.064784\n",
      "----\n",
      "   aaaf ddddd  cee nn d h bb ee aa  h l cccc bbfc   aaaaaa n bbbb aa kf ddddd  cc ee c ccce b cc aaa nn ccc jjjjjkkkk aaaaaaaaa hdddd bbbfccc bbb f aaaaaffff m nn bb d i d  n aaf af ee bbbd k cc ii e aaa jjjjjkjjjk nnn n fff nn h f gg mmmmm jjjjjjj l ccccc  e aaaf bb af hhdddddf n aaa af cccccc cc bbb baccccc j mm bn  n cccc af bbn eeeeee aaf bb jk a mm h cc l ccc ee a ed j iii dd d  ddd c nnnn ccccc ccc kkj mm mm i ccee bbbbf j jj iii h dddddg b   dd bbb d bbb  d bbb h aaalc ii aa bbbbb  c a nn aaa ccc eeje dd  m i m c nn cccc mm aaff ii nnn d jjjjjjjjjjjjjjjjjjkkk gggg ddddd jjjjjkjj a nn a ff iii eeeeee jjjkkk ii nn bbb a c jjjk eeee bbi  dd aa aaaaaaaaaa jjj iii df bbbb jf nn ee dddd aaafjjj ll nnn eeeeeeee ccccc dddd ddd ee n bb  af ee bbb c bb  n bb d dd hf nnn  bb f bb  dd cccccc e dg ddd jjjjjjjjj iiii h aaa mcccc aaaa ekkjk nnnn d jjjjj ii nn bbccc bbd hhhhhdddddd i cc m  eee cc h aaa hhddd d bbb   h kk bbbb jj i bb bbn hh kkkkk iii e ii kjj bb j bbbb cc cc cc bb eeee d jjjjjjj l nnn nnn d d cccccccc h ee a jjjjjjjjjjj i cc n a eeeeeeeejjj k gggg f iiii  e e bbb df ddddd fj bbbbb  c h ii bn nnn k ee bb c b jk dddd  bb  j nn cccc n bb afjkkk k lllllll ggggg bb  ccccc jk nn lff eeeee d k bbb  jk fff cccc bb e jjk ggg nnn hff n ccc nn aafff nnn bn bb jkjjkkk llcc ccccc bb  ccccccee d cc e eeee hhhhhhhhhh ll lll hhhhhhhh m i aa bb  hf i jjjjjjk nnnnn  ddd eee e gg nn i i jjffj ee e  k mmm c eeeeeee jjjjk l mmmm cccceee aaf nnn ee a aaaaj aaa aaaabbbb   c h jjjjkkkkkk iii  \n",
      "----\n",
      "iter 8000, loss: 18.512848\n",
      "----\n",
      " ii ff ggggghhh bb j kk nn g mnnn gbbb bbb jjj g ddd kl ll mhhh eee jjj bbb bbb jnnn kk gggggbbb bbbbb l j dd ff gddd jjjaaa fff g nnn d iii fff aa iii k aaa f iii f ddd ee aa l mmnhh dd dd eee k eee bbb ccc ddd aaa ffff aaa ii jjjjjjj dd lll hhh eee ff fff k aaa bbbbb ccc lkk aaaa k iii ff bbb bbbb hh gbbb bbbb l ii jjj aaa k aaa j kkk g nn gm m nn iii f aa bbb c ddd bbb bbbb c jjjjjk eeee ii ff eeee jaaa bbb c hhh dd aaa bbb cccc ll hhh e iii k gbbbb l bbb cl hhhh bbb ccccc aaa jjj gggggggnn ddd l aaaa fff dd l mmmddd l  fff g a ddd gdd a iii k aaa eef eee bbb cl eee bbb jc haaa bb ff g gg ma j nnn jj gg mm aa ghh g bbb ffff jjk kkkl kkk l a mmmhggggnn bbb bbbb m bbb c nn eee bbb cll hhh gggghh bbb bbb bbb cccc ffff dd aaa j iii ff gbbb bbbb l ffff g aaaa j aa kkk ddd e bbb bggbbb fffff g l mmmmi aa ee bbb cgggggnn bbbb ccc ddd j eee jj gdd l hhh ll mmmmniii j dd iii k dd bbb bbbb h ffff g mmmmmaaaa ii ff c a l mggnn iii f jjj ddd jjjjh dd gg aaaa fff kk gggggnn jjjjj aaa ii f dd eeee g ddd iii ff l hhh iii jjjjjjj ggnn iii k gggn kc kkkk gm eeee k aaa bbb cll f gbbb jjjj nnn k eee jjj gggggghh gggbbb bbb c m ggg ddd iii fffffff eee ii fff eeee jjjjjj g mmnn  nn mmhhh l hhh ee k l h l nnn iii k ggggggbbbb l jj bbb bbb cll mmmmnnnn ee jjj iii ff eeee jjjjj ee bbb clll mmghh ll ddd gh kkk l gggg hhh k ddd jj aa ee bbb cccc lll ddd dd aa dd jj gggggggnn bbb bbbb j bbb bbb ccc hhh ggggg hhh eee d gbbb bb fff k eee bbb cccc hhh bbb bbbb cmddd aa j a bbbb l eee ii fff iii ff bbb b \n",
      "----\n",
      "iter 9000, loss: 22.667407\n",
      "----\n",
      " a bbb cccc lllll m kc llllll ll mmmmnnn aaaa iii bbb c eee eee jjjjjjj ddd llll hhh iii jk aaaa jjj gggggggg hhh k ggggg mmmmmm mnnn aaa jjjj ll nnn ff ll kkkk aaa kk nnn gggg hhh aaa k ll hhh kk aaaa bbb c eee dd aaa jjj gggnnn fff kk ddd k hhh iii jjjjhh bbb cc m   eeee eee jj nnn jc ddd iii jjj lll hhh kk kkkk eeee bbb cccc nnn fff iii jjhh aaa fff iii f eee aaa bbb ccc hhh ggg ggg hhh iii jjj cc ddd ghhh aaa bbb ccc ggggggnnn f eee eee bbb ccc ddd bbb bbb jj dd aaa jj ddd fff glllll dddd k lll mm mmm hhh bbb jc jjjjjjk geef iii ff ggggg hhh dd l mmmm mm ddd ddd c  hhh iii jjc ddd ddd iii j hhh bbb bbbb c mmf kkl hh kk l hhh bbb j ddd ffff ggggggggggggggg lll kkk gnnn aaa aa fff aaa l ll kkk aaaa jj kk eee iii jjj aaa bbb cc dddd aaa ff  eee dd bbb jj g hhh l hh ffff aaa iii bb j ddd ggggghhh fff iii j eee bbb ccc ddd iii j iii jjjk llllll mmmmmmm ddd kk ddd gghh kkk aaaa jjjjjjjj g m hh gg ffff ll mmmmmmmmmmmmm mm ddd kkk iii jj eee ffff g hhh iii bbb c bbb c  e l nnn aaaa jj aaa ffff ffff aaa bbb ccc f kkkk l mnnn eee iii jjjjjjj gggg mm hhh kkk aaa bbb ccc hhh eeee bbb clc nnn iii jjk kk aaaa jjjjjjjjj kkl ll nnn fffff iii jjj iii bbb c ff ee iii jjjjjjjjjjjc kkkk aaa l hhh g h cc nnn eee g mmmmmm hhh eee c ddd kkk ee g nnn ll eeee f kkc kkkk kkk jjjk iii jj l m hhh ggnnn lc llllllll miii jjk g j f iii j aaa ff iii jj kk nnn jjjjj aaa ffff eee aaa bbb ccc hhh iii jk ggggnnn eee iii jjjj eee eee jj llc f llllll mm mmmmmmmmmmiii ii j gggggggg hh eeee bbb ccc kkk lll hhh e \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory RNN\n",
    "import numpy as np\n",
    "\n",
    "nruns = 10000\n",
    "noutput = 1000\n",
    "\n",
    "# data I/O\n",
    "data = open('input0.txt', 'r', encoding=\"utf8\").read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxu = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Wuu = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Wuo = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bu = np.zeros((hidden_size, 1)) # hidden bias\n",
    "bo = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, us, os, ps = {}, {}, {}, {}\n",
    "    mi, pi, ci = {}, {}, {}\n",
    "    us[-1] = np.copy(uprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        us[t] = np.tanh(np.dot(Wxu, xs[t]) + np.dot(Wuu, us[t-1]) + bu) # hidden state\n",
    "        # pi=softmax(u*mi)\n",
    "        mi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        pi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        #ys[t] = np.dot(Wuy, us[t]) + by # unnormalized log probabilities for next chars\n",
    "        # o=pi*ci\n",
    "        ci[t] = np.dot(Wuo, us[t]) + bo\n",
    "        os[t] = pi[t] * ci[t]\n",
    "        #ps[t] = softmax(os[t]) # probabilities for next chars (=softmax)\n",
    "        ps[t] = softmax(os[t])\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxu, dWuu, dWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "    dbu, dbo = np.zeros_like(bu), np.zeros_like(bo)\n",
    "    #dmi, dpi = np.zeros_like(mi), np.zeros_like(pi)\n",
    "    dunext = np.zeros_like(us[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        do = np.copy(ps[t])\n",
    "        do[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        \n",
    "        # pi=softmax(u*mi)\n",
    "        #dmi[t] = np.dot(Wxu, xs[t]) + bu\n",
    "        #dpi[t] = softmax(np.dot(us[t].T, mi[t]))\n",
    "        \n",
    "        #dWuo += np.dot(do, us[t].T)\n",
    "        dWuo += np.dot(do, np.dot(pi[t], us[t].T))\n",
    "        dbo += np.dot(do, pi[t])\n",
    "        \n",
    "        #du = np.dot(Wuo.T, do) + dunext # backprop into u\n",
    "        #print('shape ci: %s' % (str(np.shape(ci[t]))))\n",
    "        du = np.multiply(ci[t].T, (1-pi[t])*mi[t])\n",
    "        du += Wuo.T\n",
    "        duraw = (1 - us[t] * us[t]) # tanh'=1-tanh^2\n",
    "        #dbu += np.dot(do, pi[t]*np.dot(duraw, (np.multiply(ci[t].T, (1-pi[t])*mi[t]) + Wuo)))\n",
    "        dbu += np.dot(pi[t]*np.multiply(du, duraw), do)\n",
    "        \n",
    "        dWuu += duraw*us[t-1]*np.dot(du, do) \n",
    "        du = np.dot(pi[t]*du, do)\n",
    "        #print(np.shape(duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T))\n",
    "        #dWxu += duraw*pi[t]*((1-pi[t])*np.multiply(ci[t], mi[t].T) + Wuo).T*xs[t].T*do.T\n",
    "        #print(np.shape(pi[t]*(ci[t].T*(1-pi[t])*(np.dot(mi[t].T, duraw) + us[t]) + Wuo.T*duraw)*xs[t].T*do.T))\n",
    "        dWxu += pi[t]*(ci[t].T*(1-pi[t])*(np.dot(mi[t].T, duraw) + us[t]) + Wuo.T*duraw)*xs[t].T*do.T\n",
    "        \n",
    "        #dunext = np.dot(Wuu.T, duraw)\n",
    "    \n",
    "    for dparam in [dWxu, dWuu, dWuo, dbu, dbo]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "        clip_diag(dparam, 1) # clip weight diagonals a bit more\n",
    "    return loss, dWxu, dWuu, dWuo, dbu, dbo, us[len(inputs)-1]\n",
    "\n",
    "def sample(u, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        u = np.tanh(np.dot(Wxu, x) + np.dot(Wuu, u) + bu)\n",
    "        mmi = np.dot(Wxu, x) + bu\n",
    "        ppi = softmax(np.dot(u.T, mmi))\n",
    "        #o = np.dot(Wuo, u) + bo\n",
    "        c = np.dot(Wuo, u) + bo\n",
    "        o = ppi * c\n",
    "        p = softmax(o)\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "def clip_diag(w, cval):\n",
    "    n = min(np.shape(w))\n",
    "    w[range(n), range(n)] = np.clip(np.diagonal(w), -cval, cval)\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxu, mWuu, mWuo = np.zeros_like(Wxu), np.zeros_like(Wuu), np.zeros_like(Wuo)\n",
    "mbu, mbo = np.zeros_like(bu), np.zeros_like(bo) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "#while True:\n",
    "while n<nruns:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        uprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % noutput == 0 or n == nruns-1:\n",
    "        sample_ix = sample(uprev, inputs[0], 1500)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxu, dWuu, dWuo, dbu, dbo, uprev = lossFun(inputs, targets, uprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % noutput == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxu, Wuu, Wuo, bu, bo], \n",
    "                                [dWxu, dWuu, dWuo, dbu, dbo], \n",
    "                                [mWxu, mWuu, mWuo, mbu, mbo]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

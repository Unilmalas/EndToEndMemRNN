{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0056956  0.00283298 1.0056956 ]\n",
      " [0.00283298 1.0056956  1.0056956 ]\n",
      " [1.0056956  1.0056956  0.00283298]\n",
      " [1.0056956  0.00283298 1.0056956 ]]\n",
      "[[ 1.00738054 -0.00445907  1.00738054]\n",
      " [-0.00445907  1.00738054  1.00738054]\n",
      " [ 1.00738054  1.00738054 -0.00445907]\n",
      " [ 1.00738054 -0.00445907  1.00738054]]\n",
      "[[1.04046079 0.02812179 1.04046079]\n",
      " [0.02812179 1.04046079 1.04046079]\n",
      " [1.04046079 1.04046079 0.02812179]\n",
      " [1.04046079 0.02812179 1.04046079]]\n",
      "[[1.11678237 0.0192001  1.11678237]\n",
      " [0.0192001  1.11678237 1.11678237]\n",
      " [1.11678237 1.11678237 0.0192001 ]\n",
      " [1.11678237 0.0192001  1.11678237]]\n",
      "[[1.14458933 0.03843599 1.14458933]\n",
      " [0.03843599 1.14458933 1.14458933]\n",
      " [1.14458933 1.14458933 0.03843599]\n",
      " [1.14458933 0.03843599 1.14458933]]\n",
      "[[ 0.33186761 -0.75729888  0.33186761]\n",
      " [-0.75729888  0.33186761  0.33186761]\n",
      " [ 0.33186761  0.33186761 -0.75729888]\n",
      " [ 0.33186761 -0.75729888  0.33186761]]\n",
      "[[ 0.36044297 -0.71576451  0.36044297]\n",
      " [-0.71576451  0.36044297  0.36044297]\n",
      " [ 0.36044297  0.36044297 -0.71576451]\n",
      " [ 0.36044297 -0.71576451  0.36044297]]\n",
      "[[ 0.37503925 -0.70927041  0.37503925]\n",
      " [-0.70927041  0.37503925  0.37503925]\n",
      " [ 0.37503925  0.37503925 -0.70927041]\n",
      " [ 0.37503925 -0.70927041  0.37503925]]\n",
      "[[-0.24954456 -1.34757738 -0.24954456]\n",
      " [-1.34757738 -0.24954456 -0.24954456]\n",
      " [-0.24954456 -0.24954456 -1.34757738]\n",
      " [-0.24954456 -1.34757738 -0.24954456]]\n",
      "[[-0.21586805 -1.30367628 -0.21586805]\n",
      " [-1.30367628 -0.21586805 -0.21586805]\n",
      " [-0.21586805 -0.21586805 -1.30367628]\n",
      " [-0.21586805 -1.30367628 -0.21586805]]\n",
      "[[-0.20665865 -1.30739717 -0.20665865]\n",
      " [-1.30739717 -0.20665865 -0.20665865]\n",
      " [-0.20665865 -0.20665865 -1.30739717]\n",
      " [-0.20665865 -1.30739717 -0.20665865]]\n",
      "[[-0.74614235 -1.85468811 -0.74614235]\n",
      " [-1.85468811 -0.74614235 -0.74614235]\n",
      " [-0.74614235 -0.74614235 -1.85468811]\n",
      " [-0.74614235 -1.85468811 -0.74614235]]\n",
      "[[-0.68361865 -1.78130524 -0.68361865]\n",
      " [-1.78130524 -0.68361865 -0.68361865]\n",
      " [-0.68361865 -0.68361865 -1.78130524]\n",
      " [-0.68361865 -1.78130524 -0.68361865]]\n",
      "[[-1.02374439 -2.12817594 -1.02374439]\n",
      " [-2.12817594 -1.02374439 -1.02374439]\n",
      " [-1.02374439 -1.02374439 -2.12817594]\n",
      " [-1.02374439 -2.12817594 -1.02374439]]\n",
      "[[-0.93782442 -2.03707877 -0.93782442]\n",
      " [-2.03707877 -0.93782442 -0.93782442]\n",
      " [-0.93782442 -0.93782442 -2.03707877]\n",
      " [-0.93782442 -2.03707877 -0.93782442]]\n",
      "[[-1.25104849 -2.35913511 -1.25104849]\n",
      " [-2.35913511 -1.25104849 -1.25104849]\n",
      " [-1.25104849 -1.25104849 -2.35913511]\n",
      " [-1.25104849 -2.35913511 -1.25104849]]\n",
      "[[-1.13067054 -2.22552892 -1.13067054]\n",
      " [-2.22552892 -1.13067054 -1.13067054]\n",
      " [-1.13067054 -1.13067054 -2.22552892]\n",
      " [-1.13067054 -2.22552892 -1.13067054]]\n",
      "[[-1.45614557 -2.56201103 -1.45614557]\n",
      " [-2.56201103 -1.45614557 -1.45614557]\n",
      " [-1.45614557 -1.45614557 -2.56201103]\n",
      " [-1.45614557 -2.56201103 -1.45614557]]\n",
      "[[-1.33099237 -2.42667386 -1.33099237]\n",
      " [-2.42667386 -1.33099237 -1.33099237]\n",
      " [-1.33099237 -1.33099237 -2.42667386]\n",
      " [-1.33099237 -2.42667386 -1.33099237]]\n",
      "[[-1.65776108 -2.76650357 -1.65776108]\n",
      " [-2.76650357 -1.65776108 -1.65776108]\n",
      " [-1.65776108 -1.65776108 -2.76650357]\n",
      " [-1.65776108 -2.76650357 -1.65776108]]\n",
      "[[-1.49518161 -2.58571653 -1.49518161]\n",
      " [-2.58571653 -1.49518161 -1.49518161]\n",
      " [-1.49518161 -1.49518161 -2.58571653]\n",
      " [-1.49518161 -2.58571653 -1.49518161]]\n",
      "[[-1.86194748 -2.96779493 -1.86194748]\n",
      " [-2.96779493 -1.86194748 -1.86194748]\n",
      " [-1.86194748 -1.86194748 -2.96779493]\n",
      " [-1.86194748 -2.96779493 -1.86194748]]\n",
      "[[-1.68928126 -2.77874643 -1.68928126]\n",
      " [-2.77874643 -1.68928126 -1.68928126]\n",
      " [-1.68928126 -1.68928126 -2.77874643]\n",
      " [-1.68928126 -2.77874643 -1.68928126]]\n",
      "[[-2.06724384 -3.1759319  -2.06724384]\n",
      " [-3.1759319  -2.06724384 -2.06724384]\n",
      " [-2.06724384 -2.06724384 -3.1759319 ]\n",
      " [-2.06724384 -3.1759319  -2.06724384]]\n",
      "[[-1.83128159 -2.90597454 -1.83128159]\n",
      " [-2.90597454 -1.83128159 -1.83128159]\n",
      " [-1.83128159 -1.83128159 -2.90597454]\n",
      " [-1.83128159 -2.90597454 -1.83128159]]\n",
      "[[-1.74904117 -2.82100249 -1.74904117]\n",
      " [-2.82100249 -1.74904117 -1.74904117]\n",
      " [-1.74904117 -1.74904117 -2.82100249]\n",
      " [-1.74904117 -2.82100249 -1.74904117]]\n",
      "[[-1.56558831 -2.64432071 -1.56558831]\n",
      " [-2.64432071 -1.56558831 -1.56558831]\n",
      " [-1.56558831 -1.56558831 -2.64432071]\n",
      " [-1.56558831 -2.64432071 -1.56558831]]\n",
      "[[-3.01174587 -4.27000439 -3.01174587]\n",
      " [-4.27000439 -3.01174587 -3.01174587]\n",
      " [-3.01174587 -3.01174587 -4.27000439]\n",
      " [-3.01174587 -4.27000439 -3.01174587]]\n",
      "[[-2.39952379 -3.35488706 -2.39952379]\n",
      " [-3.35488706 -2.39952379 -2.39952379]\n",
      " [-2.39952379 -2.39952379 -3.35488706]\n",
      " [-2.39952379 -3.35488706 -2.39952379]]\n",
      "[[-2.45264812 -3.43165268 -2.45264812]\n",
      " [-3.43165268 -2.45264812 -2.45264812]\n",
      " [-2.45264812 -2.45264812 -3.43165268]\n",
      " [-2.45264812 -3.43165268 -2.45264812]]\n",
      "[[-2.49336512 -3.47381195 -2.49336512]\n",
      " [-3.47381195 -2.49336512 -2.49336512]\n",
      " [-2.49336512 -2.49336512 -3.47381195]\n",
      " [-2.49336512 -3.47381195 -2.49336512]]\n",
      "[[-2.52155985 -3.5103403  -2.52155985]\n",
      " [-3.5103403  -2.52155985 -2.52155985]\n",
      " [-2.52155985 -2.52155985 -3.5103403 ]\n",
      " [-2.52155985 -3.5103403  -2.52155985]]\n",
      "[[-2.55042963 -3.54987914 -2.55042963]\n",
      " [-3.54987914 -2.55042963 -2.55042963]\n",
      " [-2.55042963 -2.55042963 -3.54987914]\n",
      " [-2.55042963 -3.54987914 -2.55042963]]\n",
      "[[-2.58088474 -3.59206832 -2.58088474]\n",
      " [-3.59206832 -2.58088474 -2.58088474]\n",
      " [-2.58088474 -2.58088474 -3.59206832]\n",
      " [-2.58088474 -3.59206832 -2.58088474]]\n",
      "[[-2.60979699 -3.63230823 -2.60979699]\n",
      " [-3.63230823 -2.60979699 -2.60979699]\n",
      " [-2.60979699 -2.60979699 -3.63230823]\n",
      " [-2.60979699 -3.63230823 -2.60979699]]\n",
      "[[-2.64180177 -3.67632473 -2.64180177]\n",
      " [-3.67632473 -2.64180177 -2.64180177]\n",
      " [-2.64180177 -2.64180177 -3.67632473]\n",
      " [-2.64180177 -3.67632473 -2.64180177]]\n",
      "[[-2.67418057 -3.72089242 -2.67418057]\n",
      " [-3.72089242 -2.67418057 -2.67418057]\n",
      " [-2.67418057 -2.67418057 -3.72089242]\n",
      " [-2.67418057 -3.72089242 -2.67418057]]\n",
      "[[-2.70746059 -3.76696036 -2.70746059]\n",
      " [-3.76696036 -2.70746059 -2.70746059]\n",
      " [-2.70746059 -2.70746059 -3.76696036]\n",
      " [-2.70746059 -3.76696036 -2.70746059]]\n",
      "[[-2.7399413  -3.81194976 -2.7399413 ]\n",
      " [-3.81194976 -2.7399413  -2.7399413 ]\n",
      " [-2.7399413  -2.7399413  -3.81194976]\n",
      " [-2.7399413  -3.81194976 -2.7399413 ]]\n",
      "[[-2.77739435 -3.86305237 -2.77739435]\n",
      " [-3.86305237 -2.77739435 -2.77739435]\n",
      " [-2.77739435 -2.77739435 -3.86305237]\n",
      " [-2.77739435 -3.86305237 -2.77739435]]\n",
      "[[-2.81574284 -3.91540089 -2.81574284]\n",
      " [-3.91540089 -2.81574284 -2.81574284]\n",
      " [-2.81574284 -2.81574284 -3.91540089]\n",
      " [-2.81574284 -3.91540089 -2.81574284]]\n",
      "[[-2.85574086 -3.97031396 -2.85574086]\n",
      " [-3.97031396 -2.85574086 -2.85574086]\n",
      " [-2.85574086 -2.85574086 -3.97031396]\n",
      " [-2.85574086 -3.97031396 -2.85574086]]\n",
      "[[-2.89518132 -4.02448922 -2.89518132]\n",
      " [-4.02448922 -2.89518132 -2.89518132]\n",
      " [-2.89518132 -2.89518132 -4.02448922]\n",
      " [-2.89518132 -4.02448922 -2.89518132]]\n",
      "[[-2.94136861 -4.08697404 -2.94136861]\n",
      " [-4.08697404 -2.94136861 -2.94136861]\n",
      " [-2.94136861 -2.94136861 -4.08697404]\n",
      " [-2.94136861 -4.08697404 -2.94136861]]\n",
      "[[-2.98869887 -4.15103724 -2.98869887]\n",
      " [-4.15103724 -2.98869887 -2.98869887]\n",
      " [-2.98869887 -2.98869887 -4.15103724]\n",
      " [-2.98869887 -4.15103724 -2.98869887]]\n",
      "[[-3.03830246 -4.21859419 -3.03830246]\n",
      " [-4.21859419 -3.03830246 -3.03830246]\n",
      " [-3.03830246 -3.03830246 -4.21859419]\n",
      " [-3.03830246 -4.21859419 -3.03830246]]\n",
      "[[-3.0873976 -4.2854968 -3.0873976]\n",
      " [-4.2854968 -3.0873976 -3.0873976]\n",
      " [-3.0873976 -3.0873976 -4.2854968]\n",
      " [-3.0873976 -4.2854968 -3.0873976]]\n",
      "[[-3.14549227 -4.36347997 -3.14549227]\n",
      " [-4.36347997 -3.14549227 -3.14549227]\n",
      " [-3.14549227 -3.14549227 -4.36347997]\n",
      " [-3.14549227 -4.36347997 -3.14549227]]\n",
      "[[-3.20509052 -4.44351561 -3.20509052]\n",
      " [-4.44351561 -3.20509052 -3.20509052]\n",
      " [-3.20509052 -3.20509052 -4.44351561]\n",
      " [-3.20509052 -4.44351561 -3.20509052]]\n",
      "[[-3.26783615 -4.52833455 -3.26783615]\n",
      " [-4.52833455 -3.26783615 -3.26783615]\n",
      " [-3.26783615 -3.26783615 -4.52833455]\n",
      " [-3.26783615 -4.52833455 -3.26783615]]\n",
      "[[-3.33015329 -4.61262419 -3.33015329]\n",
      " [-4.61262419 -3.33015329 -3.33015329]\n",
      " [-3.33015329 -3.33015329 -4.61262419]\n",
      " [-3.33015329 -4.61262419 -3.33015329]]\n",
      "[[-3.40441368 -4.71161614 -3.40441368]\n",
      " [-4.71161614 -3.40441368 -3.40441368]\n",
      " [-3.40441368 -3.40441368 -4.71161614]\n",
      " [-3.40441368 -4.71161614 -3.40441368]]\n",
      "[[-3.48072999 -4.8133803  -3.48072999]\n",
      " [-4.8133803  -3.48072999 -3.48072999]\n",
      " [-3.48072999 -3.48072999 -4.8133803 ]\n",
      " [-3.48072999 -4.8133803  -3.48072999]]\n",
      "[[-3.56148051 -4.92179692 -3.56148051]\n",
      " [-4.92179692 -3.56148051 -3.56148051]\n",
      " [-3.56148051 -3.56148051 -4.92179692]\n",
      " [-3.56148051 -4.92179692 -3.56148051]]\n",
      "[[-3.64199099 -5.02995009 -3.64199099]\n",
      " [-5.02995009 -3.64199099 -3.64199099]\n",
      " [-3.64199099 -3.64199099 -5.02995009]\n",
      " [-3.64199099 -5.02995009 -3.64199099]]\n",
      "[[-3.73836105 -5.15762967 -3.73836105]\n",
      " [-5.15762967 -3.73836105 -3.73836105]\n",
      " [-3.73836105 -3.73836105 -5.15762967]\n",
      " [-3.73836105 -5.15762967 -3.73836105]]\n",
      "[[-3.83765556 -5.28920131 -3.83765556]\n",
      " [-5.28920131 -3.83765556 -3.83765556]\n",
      " [-3.83765556 -3.83765556 -5.28920131]\n",
      " [-3.83765556 -5.28920131 -3.83765556]]\n",
      "[[-3.9433283  -5.43019652 -3.9433283 ]\n",
      " [-5.43019652 -3.9433283  -3.9433283 ]\n",
      " [-3.9433283  -3.9433283  -5.43019652]\n",
      " [-3.9433283  -5.43019652 -3.9433283 ]]\n",
      "[[-4.04917378 -5.5714812  -4.04917378]\n",
      " [-5.5714812  -4.04917378 -4.04917378]\n",
      " [-4.04917378 -4.04917378 -5.5714812 ]\n",
      " [-4.04917378 -5.5714812  -4.04917378]]\n",
      "[[-4.17626873 -5.73894104 -4.17626873]\n",
      " [-5.73894104 -4.17626873 -4.17626873]\n",
      " [-4.17626873 -4.17626873 -5.73894104]\n",
      " [-4.17626873 -5.73894104 -4.17626873]]\n",
      "[[-4.3076987  -5.91208848 -4.3076987 ]\n",
      " [-5.91208848 -4.3076987  -4.3076987 ]\n",
      " [-4.3076987  -4.3076987  -5.91208848]\n",
      " [-4.3076987  -5.91208848 -4.3076987 ]]\n",
      "[[-4.4485128  -6.09885765 -4.4485128 ]\n",
      " [-6.09885765 -4.4485128  -4.4485128 ]\n",
      " [-4.4485128  -4.4485128  -6.09885765]\n",
      " [-4.4485128  -6.09885765 -4.4485128 ]]\n",
      "[[-4.59033639 -6.28700782 -4.59033639]\n",
      " [-6.28700782 -4.59033639 -4.59033639]\n",
      " [-4.59033639 -4.59033639 -6.28700782]\n",
      " [-4.59033639 -6.28700782 -4.59033639]]\n",
      "[[-4.7611509  -6.51089218 -4.7611509 ]\n",
      " [-6.51089218 -4.7611509  -4.7611509 ]\n",
      " [-4.7611509  -4.7611509  -6.51089218]\n",
      " [-4.7611509  -6.51089218 -4.7611509 ]]\n",
      "[[-4.93866548 -6.74343259 -4.93866548]\n",
      " [-6.74343259 -4.93866548 -4.93866548]\n",
      " [-4.93866548 -4.93866548 -6.74343259]\n",
      " [-4.93866548 -6.74343259 -4.93866548]]\n",
      "[[-5.13032588 -6.99610953 -5.13032588]\n",
      " [-6.99610953 -5.13032588 -5.13032588]\n",
      " [-5.13032588 -5.13032588 -6.99610953]\n",
      " [-5.13032588 -6.99610953 -5.13032588]]\n",
      "[[-5.32462062 -7.25225653 -5.32462062]\n",
      " [-7.25225653 -5.32462062 -5.32462062]\n",
      " [-5.32462062 -5.32462062 -7.25225653]\n",
      " [-5.32462062 -7.25225653 -5.32462062]]\n",
      "[[-5.55951795 -7.55847599 -5.55951795]\n",
      " [-7.55847599 -5.55951795 -5.55951795]\n",
      " [-5.55951795 -5.55951795 -7.55847599]\n",
      " [-5.55951795 -7.55847599 -5.55951795]]\n",
      "[[-5.80519292 -7.87839738 -5.80519292]\n",
      " [-7.87839738 -5.80519292 -5.80519292]\n",
      " [-5.80519292 -5.80519292 -7.87839738]\n",
      " [-5.80519292 -7.87839738 -5.80519292]]\n",
      "[[-6.07275346 -8.22883584 -6.07275346]\n",
      " [-8.22883584 -6.07275346 -6.07275346]\n",
      " [-6.07275346 -6.07275346 -8.22883584]\n",
      " [-6.07275346 -8.22883584 -6.07275346]]\n",
      "[[-6.34605207 -8.58670104 -6.34605207]\n",
      " [-8.58670104 -6.34605207 -6.34605207]\n",
      " [-6.34605207 -6.34605207 -8.58670104]\n",
      " [-6.34605207 -8.58670104 -6.34605207]]\n",
      "[[-6.67811711 -9.01703486 -6.67811711]\n",
      " [-9.01703486 -6.67811711 -6.67811711]\n",
      " [-6.67811711 -6.67811711 -9.01703486]\n",
      " [-6.67811711 -9.01703486 -6.67811711]]\n",
      "[[-7.02816171 -9.46985778 -7.02816171]\n",
      " [-9.46985778 -7.02816171 -7.02816171]\n",
      " [-7.02816171 -7.02816171 -9.46985778]\n",
      " [-7.02816171 -9.46985778 -7.02816171]]\n",
      "[[-7.41297148 -9.97015319 -7.41297148]\n",
      " [-9.97015319 -7.41297148 -7.41297148]\n",
      " [-7.41297148 -7.41297148 -9.97015319]\n",
      " [-7.41297148 -9.97015319 -7.41297148]]\n",
      "[[ -7.80945827 -10.4854053   -7.80945827]\n",
      " [-10.4854053   -7.80945827  -7.80945827]\n",
      " [ -7.80945827  -7.80945827 -10.4854053 ]\n",
      " [ -7.80945827 -10.4854053   -7.80945827]]\n",
      "[[ -8.29434777 -11.10954335  -8.29434777]\n",
      " [-11.10954335  -8.29434777  -8.29434777]\n",
      " [ -8.29434777  -8.29434777 -11.10954335]\n",
      " [ -8.29434777 -11.10954335  -8.29434777]]\n",
      "[[ -8.81009917 -11.7716606   -8.81009917]\n",
      " [-11.7716606   -8.81009917  -8.81009917]\n",
      " [ -8.81009917  -8.81009917 -11.7716606 ]\n",
      " [ -8.81009917 -11.7716606   -8.81009917]]\n",
      "[[ -9.38245477 -12.50950163  -9.38245477]\n",
      " [-12.50950163  -9.38245477  -9.38245477]\n",
      " [ -9.38245477  -9.38245477 -12.50950163]\n",
      " [ -9.38245477 -12.50950163  -9.38245477]]\n",
      "[[ -9.97799989 -13.27682055  -9.97799989]\n",
      " [-13.27682055  -9.97799989  -9.97799989]\n",
      " [ -9.97799989  -9.97799989 -13.27682055]\n",
      " [ -9.97799989 -13.27682055  -9.97799989]]\n",
      "[[-10.71238884 -14.21462939 -10.71238884]\n",
      " [-14.21462939 -10.71238884 -10.71238884]\n",
      " [-10.71238884 -10.71238884 -14.21462939]\n",
      " [-10.71238884 -14.21462939 -10.71238884]]\n"
     ]
    }
   ],
   "source": [
    "# neural turing machine with FF as controller\n",
    "# see also https://github.com/flomlo/ntm_keras\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def tanh_arr(x, d=False):\n",
    "    if d:\n",
    "        return 1 - np.tanh(x)\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "\n",
    "def siginv_arr(y, d=False): # sigmoid inversed\n",
    "    if d:\n",
    "        return 1 / y / (1 - y)\n",
    "    else:\n",
    "        return np.log( y / (1 - y) )\n",
    "\n",
    "def sig_arr(x, d=False): # sigmoid\n",
    "    if d:\n",
    "        return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x, d=False):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    res = ex / ex.sum()\n",
    "    if d:\n",
    "        return res * (1 - res)\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def simK(u, v): # similarity measure\n",
    "    return np.dot(u, v) / np.linalg.norm(u) / np.linalg.norm(v)\n",
    "\n",
    "def weights(wt, wt_1, Mt, kt, betat, gt, st, gammat):\n",
    "    nmemslots = np.shape(Mt)[0]\n",
    "    for i in range(nmemslots):\n",
    "        wt[i] = softmax(betat * simK(Mt[i], kt), False) # content addressing\n",
    "    wt = gt * wt + (1 - gt) * wt_1 # gated weighing\n",
    "    wt = np.convolve(wt, st, mode='same') # convolutional shift - todo: circular (does not make a big difference)\n",
    "    wt = (np.sign(wt) * np.abs(wt) ** gammat) / np.sum(np.sign(wt) * np.abs(wt) ** gammat) # sharpening (careful fractional powers)\n",
    "    return wt\n",
    "\n",
    "def concat(atgt, *args): # custom concat\n",
    "    res = atgt.flatten().tolist()\n",
    "    for e in args:\n",
    "        if type(e) is float:\n",
    "            res += [e]\n",
    "        elif type(e) is list:\n",
    "            res += e\n",
    "        else:\n",
    "            res += e.flatten().tolist()\n",
    "    return np.array(res)\n",
    "\n",
    "def act_fwd(hc, csplit): # activation forward pass\n",
    "    # subvectors: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    # lengths in csplit: ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    # activations: ReLU + ReLU + tanh + ? + ? + soft + sig(inv, clip) + sig + tanh\n",
    "    yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc = np.split(hc, csplit)\n",
    "    yc = np.maximum(yc, 0, yc) # ReLU\n",
    "    wtc = np.maximum(wtc, 0, wtc) # ReLU\n",
    "    ktc = tanh_arr(ktc, False)\n",
    "    betatc = np.maximum(betatc, 0, betatc) # ReLU ?\n",
    "    gtc = np.maximum(gtc, 0, gtc) # ReLU ?\n",
    "    stc = softmax(stc, False)\n",
    "    gammatc = np.clip(sig_arr(gammatc, False), -1, 1) # ?\n",
    "    etc = sig_arr(etc, False)\n",
    "    atc = tanh_arr(atc, False)\n",
    "    return concat(yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc)\n",
    "\n",
    "def act_bck(hc, csplit): #  activation backpropagation\n",
    "    # subvectors: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    # lengths in csplit: ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    # activations: ReLU + ReLU + tanh + ? + ? + soft + sig(inv, clip) + sig + tanh\n",
    "    yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc = np.split(hc, csplit)\n",
    "    yc = ((yc > 0) * 1.) # ReLU\n",
    "    wtc = ((wtc > 0) * 1.) # ReLU\n",
    "    ktc = tanh_arr(ktc, True)\n",
    "    betatc = ((betatc > 0) * 1.) # ReLU ?\n",
    "    gtc = ((gtc > 0) * 1.) # ReLU ?\n",
    "    stc = softmax(stc, True)\n",
    "    gammatc = np.clip(sig_arr(gammatc, True), -1, 1) # ?\n",
    "    etc = sig_arr(etc, True)\n",
    "    atc = tanh_arr(atc, True)\n",
    "    return concat(yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # parameters\n",
    "    ndata = 4 # size of external input data\n",
    "    etha = 0.1 # learning rate\n",
    "    ntmstps = 20\n",
    "    \n",
    "    # fill memory\n",
    "    m_depth = 3\n",
    "    Mt = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 0, 1]]) # memory (N slots), m_depth=3\n",
    "    nmemslots = np.shape(Mt)[0] # 4\n",
    "    wt = np.array([0.25, 0.25, 0.25, 0.25]) # weights 1-N: one for each memory entry, initial values\n",
    "    \n",
    "    # the controller vectors are (data_vector, kt, beta, gt, st, gamma)\n",
    "    kt = np.zeros((1, m_depth)) # key vector of length M (here: m_depth)\n",
    "    betat = 0.8 # key strength\n",
    "    gt = 0.5 # interpolation gate\n",
    "    st = np.array([0.1, 0.8, 0.1]) # shift weighting\n",
    "    gammat = 2.0 # sharpening\n",
    "    # length of controller vector: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    nctrlvec = ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    ctrl_inpv = np.zeros((1, nctrlvec)) # input controller\n",
    "    \n",
    "    # split vector data + wt-1 + kt + beta + gt + st + gamma\n",
    "    ctrl_out_split = np.array([ndata, ndata+nmemslots, ndata+nmemslots+m_depth, ndata+nmemslots+m_depth+1, ndata+nmemslots+m_depth+2, ndata+nmemslots+m_depth+5, ndata+nmemslots+m_depth+6, ndata+nmemslots+m_depth+6+nmemslots])\n",
    "    et = np.zeros((nmemslots)) # erase vector\n",
    "    at = np.zeros((nmemslots)) # add vector\n",
    "    \n",
    "    x = np.zeros((ndata)) # input external data\n",
    "    y = np.zeros((ndata)) # output external data\n",
    "    \n",
    "    # model parameters\n",
    "    W1 = np.random.randn(nctrlvec, nctrlvec)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(nctrlvec, nctrlvec)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, nctrlvec)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, nctrlvec)) # hidden-out bias\n",
    "    \n",
    "    # time steps\n",
    "    for thist in range(ntmstps):\n",
    "\n",
    "        # head moves todo: add multihead\n",
    "        iheadpos = 0\n",
    "        while iheadpos < nmemslots:\n",
    "            wt_1 = wt\n",
    "\n",
    "            # old memory Mt, read-weights wt from last step: feed read-vector to controller\n",
    "            # calculate read vector to feed controller\n",
    "            # read output vector from head location\n",
    "            rt = np.dot(wt, Mt) # dim M, weighted memories\n",
    "\n",
    "            # controller runs a single step (with input from outside)\n",
    "            x = np.array([[0., 1., 0., 0.]]) # receive input vector from outside todo: really outside!\n",
    "            ctrl_inpv = concat(x, wt, kt, betat, gt, st, gammat, et, at)\n",
    "            # set indices for erase/add\n",
    "            if iheadpos > 0:\n",
    "                et[iheadpos-1] = 0\n",
    "                at[iheadpos-1] = 0\n",
    "            et[iheadpos] = 1\n",
    "            at[iheadpos] = 1\n",
    "            # controller forward pass, single step\n",
    "            h1 = np.dot(ctrl_inpv, W1) + b1\n",
    "            #h1 = np.maximum(h1, 0, h1) # ReLU todo: different by parameter\n",
    "            h1 = act_fwd(h1[0], ctrl_out_split) # forward pass (different activation by subvector)\n",
    "            o2 = np.dot(h1, W2) + b2\n",
    "            # backward pass\n",
    "            y[1] = 1. # arbitrary external output data (truth), todo: check, what is the entire truth?\n",
    "            ctrl_outv = concat(y, wt, kt, betat, gt, st, gammat, et, at)\n",
    "            #dW1 = - etha * (o2 - ctrl_outv) * np.maximum(h1, 0, h1)\n",
    "            dW1 = - etha * (o2 - ctrl_outv) * h1\n",
    "            dW2 = dW1 * act_bck(h1, ctrl_out_split) * ctrl_inpv\n",
    "            W1 += dW1\n",
    "            W2 += dW2\n",
    "\n",
    "            # controller unactivated output divided into actual data output, reading, writing instructions\n",
    "            # split: output_dim, read_heads, write_heads\n",
    "            # split and apply activations:\n",
    "            # k and add_vector are activated via tanh, erase_vector via sigmoid (this is critical!),\n",
    "            # shift via softmax, gamma is sigmoided, inversed and clipped (probably not ideal)\n",
    "            # g is sigmoided, beta is linear (probably not ideal!)\n",
    "            y, wt, kt, betat, gt, st, gammat, et, at = np.split(o2[0], ctrl_out_split)\n",
    "\n",
    "            # write memory for each head: calculate weights, erase, add\n",
    "            wt = weights(wt, wt_1, Mt, kt, betat, gt, st, gammat)\n",
    "            Mt = Mt - Mt * np.dot(wt, et.T) # erase\n",
    "            Mt = Mt + np.dot(wt, at.T) # add\n",
    "\n",
    "            # calculate read weights, save in the state and use for next round\n",
    "            # todo\n",
    "            iheadpos += 1\n",
    "            \n",
    "            print(Mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "[94, 49, 73, 128, 174, 45, 45, 198, 156, 2]\n",
      "[[1, 1, 1, 0, 0, 1, 1, 0], [1, 1, 1, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 1, 1, 1], [1, 1, 1, 0, 1, 1, 1, 1], [0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 1, 0, 0, 1, 0, 0], [1, 1, 1, 0, 1, 1, 1, 0], [1, 0, 1, 1, 0, 0, 1, 0], [1, 0, 1, 1, 0, 1, 0, 1], [0, 0, 1, 1, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def rdbinseq(l): # random sequence of 8-bit binary vectors (int 0-255)\n",
    "    if l <= 0:\n",
    "        return []\n",
    "    res = []\n",
    "    for i in range(l):\n",
    "        res.append(random.randint(0, 255))\n",
    "    return res\n",
    "\n",
    "def rdbinseqb(n, l): # random sequence of n-bit binary vectors\n",
    "    if l <= 0:\n",
    "        return []\n",
    "    res = []\n",
    "    for i in range(l):\n",
    "        res.append([random.randint(0, 1) for j in range(n)])\n",
    "    return res\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(int('11111111', 2))\n",
    "    print(rdbinseq(10))\n",
    "    print(rdbinseqb(8, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]]\n",
      "[[-0.27381219  1.28205607 -0.27381219]\n",
      " [-0.27381219 -0.27381219  1.28205607]\n",
      " [ 1.28205607 -0.27381219 -0.27381219]\n",
      " [-0.27381219  1.28205607 -0.27381219]]\n"
     ]
    }
   ],
   "source": [
    "# neural turing machine with FF as controller\n",
    "# copy task\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "def rdbinseqb(n, l): # random sequence of n-bit binary vectors\n",
    "    if l <= 0:\n",
    "        return []\n",
    "    res = []\n",
    "    for i in range(l):\n",
    "        res.append([random.randint(0, 1) for j in range(n)])\n",
    "    return res\n",
    "\n",
    "def tanh_arr(x, d=False):\n",
    "    if d:\n",
    "        return 1 - np.tanh(x)\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "\n",
    "def siginv_arr(y, d=False): # sigmoid inversed\n",
    "    if d:\n",
    "        return 1 / y / (1 - y)\n",
    "    else:\n",
    "        return np.log( y / (1 - y) )\n",
    "\n",
    "def sig_arr(x, d=False): # sigmoid\n",
    "    if d:\n",
    "        return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x, d=False):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    res = ex / ex.sum()\n",
    "    if d:\n",
    "        return res * (1 - res)\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def simK(u, v): # similarity measure\n",
    "    return np.dot(u, v) / np.linalg.norm(u) / np.linalg.norm(v)\n",
    "\n",
    "def weights(wt, wt_1, Mt, kt, betat, gt, st, gammat):\n",
    "    nmemslots = np.shape(Mt)[0]\n",
    "    for i in range(nmemslots):\n",
    "        wt[i] = softmax(betat * simK(Mt[i], kt), False) # content addressing\n",
    "    wt = gt * wt + (1 - gt) * wt_1 # gated weighing\n",
    "    wt = np.convolve(wt, st, mode='same') # convolutional shift - todo: circular (does not make a big difference)\n",
    "    wt = (np.sign(wt) * np.abs(wt) ** gammat) / np.sum(np.sign(wt) * np.abs(wt) ** gammat) # sharpening (careful fractional powers)\n",
    "    return wt\n",
    "\n",
    "def concat(atgt, *args): # custom concat\n",
    "    res = atgt.flatten().tolist()\n",
    "    for e in args:\n",
    "        if type(e) is float:\n",
    "            res += [e]\n",
    "        elif type(e) is list:\n",
    "            res += e\n",
    "        else:\n",
    "            res += e.flatten().tolist()\n",
    "    return np.array(res)\n",
    "\n",
    "def act_fwd(hc, csplit): # activation forward pass\n",
    "    # subvectors: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    # lengths in csplit: ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    # activations: ReLU + ReLU + tanh + ? + ? + soft + sig(inv, clip) + sig + tanh\n",
    "    yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc = np.split(hc, csplit)\n",
    "    yc = np.maximum(yc, 0, yc) # ReLU\n",
    "    wtc = np.maximum(wtc, 0, wtc) # ReLU\n",
    "    ktc = tanh_arr(ktc, False)\n",
    "    betatc = np.maximum(betatc, 0, betatc) # ReLU ?\n",
    "    gtc = np.maximum(gtc, 0, gtc) # ReLU ?\n",
    "    stc = softmax(stc, False)\n",
    "    gammatc = np.clip(sig_arr(gammatc, False), -1, 1) # ?\n",
    "    etc = sig_arr(etc, False)\n",
    "    atc = tanh_arr(atc, False)\n",
    "    return concat(yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc)\n",
    "\n",
    "def act_bck(hc, csplit): #  activation backpropagation\n",
    "    # subvectors: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    # lengths in csplit: ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    # activations: ReLU + ReLU + tanh + ? + ? + soft + sig(inv, clip) + sig + tanh\n",
    "    yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc = np.split(hc, csplit)\n",
    "    yc = ((yc > 0) * 1.) # ReLU\n",
    "    wtc = ((wtc > 0) * 1.) # ReLU\n",
    "    ktc = tanh_arr(ktc, True)\n",
    "    betatc = ((betatc > 0) * 1.) # ReLU ?\n",
    "    gtc = ((gtc > 0) * 1.) # ReLU ?\n",
    "    stc = softmax(stc, True)\n",
    "    gammatc = np.clip(sig_arr(gammatc, True), -1, 1) # ?\n",
    "    etc = sig_arr(etc, True)\n",
    "    atc = tanh_arr(atc, True)\n",
    "    return concat(yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # parameters\n",
    "    ndata = 4 # size of external input data\n",
    "    etha = 0.1 # learning rate\n",
    "    ntmstps = 10\n",
    "    \n",
    "    # fill memory\n",
    "    m_depth = 3\n",
    "    #Mt = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 0, 1]]) # memory (N slots), m_depth=3\n",
    "    Mt = np.array(rdbinseqb(3, 4))\n",
    "    print(Mt)\n",
    "    nmemslots = np.shape(Mt)[0] # 4\n",
    "    wt = np.array([0.25, 0.25, 0.25, 0.25]) # weights 1-N: one for each memory entry, initial values\n",
    "    \n",
    "    # the controller vectors are (data_vector, kt, beta, gt, st, gamma)\n",
    "    kt = np.zeros((1, m_depth)) # key vector of length M (here: m_depth)\n",
    "    betat = 0.8 # key strength\n",
    "    gt = 0.5 # interpolation gate\n",
    "    st = np.array([0.1, 0.8, 0.1]) # shift weighting\n",
    "    gammat = 2.0 # sharpening\n",
    "    # length of controller vector: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    nctrlvec = ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    ctrl_inpv = np.zeros((1, nctrlvec)) # input controller\n",
    "    \n",
    "    # split vector data + wt-1 + kt + beta + gt + st + gamma\n",
    "    ctrl_out_split = np.array([ndata, ndata+nmemslots, ndata+nmemslots+m_depth, ndata+nmemslots+m_depth+1, ndata+nmemslots+m_depth+2, ndata+nmemslots+m_depth+5, ndata+nmemslots+m_depth+6, ndata+nmemslots+m_depth+6+nmemslots])\n",
    "    et = np.zeros((nmemslots)) # erase vector\n",
    "    at = np.zeros((nmemslots)) # add vector\n",
    "    \n",
    "    x = np.zeros((ndata)) # input external data\n",
    "    y = np.zeros((ndata)) # output external data\n",
    "    \n",
    "    # model parameters\n",
    "    W1 = np.random.randn(nctrlvec, nctrlvec)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(nctrlvec, nctrlvec)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, nctrlvec)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, nctrlvec)) # hidden-out bias\n",
    "    \n",
    "    # time steps\n",
    "    for thist in range(ntmstps):\n",
    "\n",
    "        # head moves todo: add multihead\n",
    "        iheadpos = 0\n",
    "        while iheadpos < nmemslots:\n",
    "            wt_1 = wt\n",
    "\n",
    "            # old memory Mt, read-weights wt from last step: feed read-vector to controller\n",
    "            # calculate read vector to feed controller\n",
    "            # read output vector from head location\n",
    "            rt = np.dot(wt, Mt) # dim M, weighted memories\n",
    "\n",
    "            # controller runs a single step (with input from outside)\n",
    "            x = np.array([[0., 1., 0., 0.]]) # receive input vector from outside todo: really outside!\n",
    "            ctrl_inpv = concat(x, wt, kt, betat, gt, st, gammat, et, at)\n",
    "            # set indices for erase/add\n",
    "            if iheadpos > 0:\n",
    "                et[iheadpos-1] = 0\n",
    "                at[iheadpos-1] = 0\n",
    "            et[iheadpos] = 1\n",
    "            at[iheadpos] = 1\n",
    "            # controller forward pass, single step\n",
    "            h1 = np.dot(ctrl_inpv, W1) + b1\n",
    "            #h1 = np.maximum(h1, 0, h1) # ReLU todo: different by parameter\n",
    "            h1 = act_fwd(h1[0], ctrl_out_split) # forward pass (different activation by subvector)\n",
    "            o2 = np.dot(h1, W2) + b2\n",
    "            # backward pass\n",
    "            y[1] = 1. # arbitrary external output data (truth), todo: check, what is the entire truth?\n",
    "            ctrl_outv = concat(y, wt, kt, betat, gt, st, gammat, et, at)\n",
    "            #dW1 = - etha * (o2 - ctrl_outv) * np.maximum(h1, 0, h1)\n",
    "            dW1 = - etha * (o2 - ctrl_outv) * h1\n",
    "            dW2 = dW1 * act_bck(h1, ctrl_out_split) * ctrl_inpv\n",
    "            W1 += dW1\n",
    "            W2 += dW2\n",
    "\n",
    "            # controller unactivated output divided into actual data output, reading, writing instructions\n",
    "            # split: output_dim, read_heads, write_heads\n",
    "            # split and apply activations:\n",
    "            # k and add_vector are activated via tanh, erase_vector via sigmoid (this is critical!),\n",
    "            # shift via softmax, gamma is sigmoided, inversed and clipped (probably not ideal)\n",
    "            # g is sigmoided, beta is linear (probably not ideal!)\n",
    "            y, wt, kt, betat, gt, st, gammat, et, at = np.split(o2[0], ctrl_out_split)\n",
    "\n",
    "            # write memory for each head: calculate weights, erase, add\n",
    "            wt = weights(wt, wt_1, Mt, kt, betat, gt, st, gammat)\n",
    "            Mt = Mt - Mt * np.dot(wt, et.T) # erase\n",
    "            Mt = Mt + np.dot(wt, at.T) # add\n",
    "\n",
    "            # calculate read weights, save in the state and use for next round\n",
    "            # todo\n",
    "            iheadpos += 1\n",
    "            \n",
    "    print(Mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qkvn  [[-1  1]\n",
      " [ 1  1]\n",
      " [ 0  0]] [[-1  0]\n",
      " [ 0  1]\n",
      " [ 1  1]] [[ 0 -1]\n",
      " [ 1  0]\n",
      " [-1  1]]\n",
      "scores  [[ 1  1]\n",
      " [-1  1]]\n",
      "3\n",
      "scores corrected for dim  [[ 0.57735027  0.57735027]\n",
      " [-0.57735027  0.57735027]]\n",
      "softmax  [[0.3016453  0.3016453 ]\n",
      " [0.09506409 0.3016453 ]]\n",
      "weighted values  [[-0.3016453   0.3016453   0.        ]\n",
      " [-0.3016453   0.09506409  0.20658121]]\n",
      "weighted sum  [-0.60329061  0.39670939  0.20658121]\n"
     ]
    }
   ],
   "source": [
    "# key-value-query attention test - full matrix version\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    input_sent = 'thinking machines'\n",
    "    # embeddings\n",
    "    xn = np.array([[1, 0, 0, 0], [0, 1, 0, 0]])\n",
    "\n",
    "    # weights\n",
    "    Wk = np.array([[-1, 0, 1], [0, 1, 1], [1, -1, 0], [1, 0, -1]])\n",
    "    Wv = np.array([[0, 1, -1], [-1, 0, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    Wq = np.array([[-1, 1, 0], [1, 1, 0], [-1, 0, 1], [-1, 1, 0]])\n",
    "\n",
    "    # query, key, value\n",
    "    qn = np.dot(Wq.T, xn.T)\n",
    "    kn = np.dot(Wk.T, xn.T)\n",
    "    vn = np.dot(Wv.T, xn.T)\n",
    "\n",
    "    print('qkvn ', qn, kn, vn)\n",
    "\n",
    "    # scores\n",
    "    sn = np.dot(qn.T, kn)\n",
    "    print('scores ', sn)\n",
    "\n",
    "    # key dimension\n",
    "    dn = np.shape(kn)[0]\n",
    "    print(dn)\n",
    "    sn = np.true_divide(sn, np.sqrt(dn))\n",
    "    print('scores corrected for dim ', sn)\n",
    "    \n",
    "    # softmax\n",
    "    sn = softmax(sn)\n",
    "    print('softmax ', sn)\n",
    "    \n",
    "    # weighted value vectors\n",
    "    zn = np.dot(sn, vn.T)\n",
    "    print('weighted values ', zn)\n",
    "    \n",
    "    # sum up weighted value vectors\n",
    "    zsum = np.sum(zn, axis=0)\n",
    "    print('weighted sum ', zsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07392436  0.11811963  0.04419527]\n",
      " [-0.03939406  0.11811963  0.20174407]\n",
      " [-0.06150925  0.17587763 -0.02597784]\n",
      " [-0.07392436  0.11811963  0.04419527]]\n"
     ]
    }
   ],
   "source": [
    "# similarity test\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(x, d=False):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    res = ex / ex.sum()\n",
    "    if d:\n",
    "        return res * (1 - res)\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def simK(u, v): # similarity measure\n",
    "    return np.dot(u, v) / np.linalg.norm(u) / np.linalg.norm(v)\n",
    "\n",
    "def attention(q, k, v): # scaled dot-product attention\n",
    "    dk = np.shape(k)[1] # dimension of keys\n",
    "    return np.dot(softmax(np.true_divide(np.dot(q, k.T), np.sqrt(dk))), v)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    v0 = np.array([[1,0]])\n",
    "    x = range(0,30,1)\n",
    "    y = [simK(v0, np.array([[v/20,1]]).T) for v in x]\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()\n",
    "    Q = np.array([[-1, 1, 0], [1, 1, 0], [-1, 0, 1], [-1, 1, 0]]) # query\n",
    "    K = np.array([[-1, 0, 1], [0, 1, 1], [1, -1, 0], [1, 0, -1]]) # key\n",
    "    V = np.array([[0, 1, -1], [-1, 0, 1], [1, 0, 1], [0, 1, 1]]) # value\n",
    "    print(attention(Q, K, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

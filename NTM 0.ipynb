{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0056956  0.00283298 1.0056956 ]\n",
      " [0.00283298 1.0056956  1.0056956 ]\n",
      " [1.0056956  1.0056956  0.00283298]\n",
      " [1.0056956  0.00283298 1.0056956 ]]\n",
      "[[ 1.00738054 -0.00445907  1.00738054]\n",
      " [-0.00445907  1.00738054  1.00738054]\n",
      " [ 1.00738054  1.00738054 -0.00445907]\n",
      " [ 1.00738054 -0.00445907  1.00738054]]\n",
      "[[1.04046079 0.02812179 1.04046079]\n",
      " [0.02812179 1.04046079 1.04046079]\n",
      " [1.04046079 1.04046079 0.02812179]\n",
      " [1.04046079 0.02812179 1.04046079]]\n",
      "[[1.11678237 0.0192001  1.11678237]\n",
      " [0.0192001  1.11678237 1.11678237]\n",
      " [1.11678237 1.11678237 0.0192001 ]\n",
      " [1.11678237 0.0192001  1.11678237]]\n",
      "[[1.14458933 0.03843599 1.14458933]\n",
      " [0.03843599 1.14458933 1.14458933]\n",
      " [1.14458933 1.14458933 0.03843599]\n",
      " [1.14458933 0.03843599 1.14458933]]\n",
      "[[ 0.33186761 -0.75729888  0.33186761]\n",
      " [-0.75729888  0.33186761  0.33186761]\n",
      " [ 0.33186761  0.33186761 -0.75729888]\n",
      " [ 0.33186761 -0.75729888  0.33186761]]\n",
      "[[ 0.36044297 -0.71576451  0.36044297]\n",
      " [-0.71576451  0.36044297  0.36044297]\n",
      " [ 0.36044297  0.36044297 -0.71576451]\n",
      " [ 0.36044297 -0.71576451  0.36044297]]\n",
      "[[ 0.37503925 -0.70927041  0.37503925]\n",
      " [-0.70927041  0.37503925  0.37503925]\n",
      " [ 0.37503925  0.37503925 -0.70927041]\n",
      " [ 0.37503925 -0.70927041  0.37503925]]\n",
      "[[-0.24954456 -1.34757738 -0.24954456]\n",
      " [-1.34757738 -0.24954456 -0.24954456]\n",
      " [-0.24954456 -0.24954456 -1.34757738]\n",
      " [-0.24954456 -1.34757738 -0.24954456]]\n",
      "[[-0.21586805 -1.30367628 -0.21586805]\n",
      " [-1.30367628 -0.21586805 -0.21586805]\n",
      " [-0.21586805 -0.21586805 -1.30367628]\n",
      " [-0.21586805 -1.30367628 -0.21586805]]\n",
      "[[-0.20665865 -1.30739717 -0.20665865]\n",
      " [-1.30739717 -0.20665865 -0.20665865]\n",
      " [-0.20665865 -0.20665865 -1.30739717]\n",
      " [-0.20665865 -1.30739717 -0.20665865]]\n",
      "[[-0.74614235 -1.85468811 -0.74614235]\n",
      " [-1.85468811 -0.74614235 -0.74614235]\n",
      " [-0.74614235 -0.74614235 -1.85468811]\n",
      " [-0.74614235 -1.85468811 -0.74614235]]\n",
      "[[-0.68361865 -1.78130524 -0.68361865]\n",
      " [-1.78130524 -0.68361865 -0.68361865]\n",
      " [-0.68361865 -0.68361865 -1.78130524]\n",
      " [-0.68361865 -1.78130524 -0.68361865]]\n",
      "[[-1.02374439 -2.12817594 -1.02374439]\n",
      " [-2.12817594 -1.02374439 -1.02374439]\n",
      " [-1.02374439 -1.02374439 -2.12817594]\n",
      " [-1.02374439 -2.12817594 -1.02374439]]\n",
      "[[-0.93782442 -2.03707877 -0.93782442]\n",
      " [-2.03707877 -0.93782442 -0.93782442]\n",
      " [-0.93782442 -0.93782442 -2.03707877]\n",
      " [-0.93782442 -2.03707877 -0.93782442]]\n",
      "[[-1.25104849 -2.35913511 -1.25104849]\n",
      " [-2.35913511 -1.25104849 -1.25104849]\n",
      " [-1.25104849 -1.25104849 -2.35913511]\n",
      " [-1.25104849 -2.35913511 -1.25104849]]\n",
      "[[-1.13067054 -2.22552892 -1.13067054]\n",
      " [-2.22552892 -1.13067054 -1.13067054]\n",
      " [-1.13067054 -1.13067054 -2.22552892]\n",
      " [-1.13067054 -2.22552892 -1.13067054]]\n",
      "[[-1.45614557 -2.56201103 -1.45614557]\n",
      " [-2.56201103 -1.45614557 -1.45614557]\n",
      " [-1.45614557 -1.45614557 -2.56201103]\n",
      " [-1.45614557 -2.56201103 -1.45614557]]\n",
      "[[-1.33099237 -2.42667386 -1.33099237]\n",
      " [-2.42667386 -1.33099237 -1.33099237]\n",
      " [-1.33099237 -1.33099237 -2.42667386]\n",
      " [-1.33099237 -2.42667386 -1.33099237]]\n",
      "[[-1.65776108 -2.76650357 -1.65776108]\n",
      " [-2.76650357 -1.65776108 -1.65776108]\n",
      " [-1.65776108 -1.65776108 -2.76650357]\n",
      " [-1.65776108 -2.76650357 -1.65776108]]\n",
      "[[-1.49518161 -2.58571653 -1.49518161]\n",
      " [-2.58571653 -1.49518161 -1.49518161]\n",
      " [-1.49518161 -1.49518161 -2.58571653]\n",
      " [-1.49518161 -2.58571653 -1.49518161]]\n",
      "[[-1.86194748 -2.96779493 -1.86194748]\n",
      " [-2.96779493 -1.86194748 -1.86194748]\n",
      " [-1.86194748 -1.86194748 -2.96779493]\n",
      " [-1.86194748 -2.96779493 -1.86194748]]\n",
      "[[-1.68928126 -2.77874643 -1.68928126]\n",
      " [-2.77874643 -1.68928126 -1.68928126]\n",
      " [-1.68928126 -1.68928126 -2.77874643]\n",
      " [-1.68928126 -2.77874643 -1.68928126]]\n",
      "[[-2.06724384 -3.1759319  -2.06724384]\n",
      " [-3.1759319  -2.06724384 -2.06724384]\n",
      " [-2.06724384 -2.06724384 -3.1759319 ]\n",
      " [-2.06724384 -3.1759319  -2.06724384]]\n",
      "[[-1.83128159 -2.90597454 -1.83128159]\n",
      " [-2.90597454 -1.83128159 -1.83128159]\n",
      " [-1.83128159 -1.83128159 -2.90597454]\n",
      " [-1.83128159 -2.90597454 -1.83128159]]\n",
      "[[-1.74904117 -2.82100249 -1.74904117]\n",
      " [-2.82100249 -1.74904117 -1.74904117]\n",
      " [-1.74904117 -1.74904117 -2.82100249]\n",
      " [-1.74904117 -2.82100249 -1.74904117]]\n",
      "[[-1.56558831 -2.64432071 -1.56558831]\n",
      " [-2.64432071 -1.56558831 -1.56558831]\n",
      " [-1.56558831 -1.56558831 -2.64432071]\n",
      " [-1.56558831 -2.64432071 -1.56558831]]\n",
      "[[-3.01174587 -4.27000439 -3.01174587]\n",
      " [-4.27000439 -3.01174587 -3.01174587]\n",
      " [-3.01174587 -3.01174587 -4.27000439]\n",
      " [-3.01174587 -4.27000439 -3.01174587]]\n",
      "[[-2.39952379 -3.35488706 -2.39952379]\n",
      " [-3.35488706 -2.39952379 -2.39952379]\n",
      " [-2.39952379 -2.39952379 -3.35488706]\n",
      " [-2.39952379 -3.35488706 -2.39952379]]\n",
      "[[-2.45264812 -3.43165268 -2.45264812]\n",
      " [-3.43165268 -2.45264812 -2.45264812]\n",
      " [-2.45264812 -2.45264812 -3.43165268]\n",
      " [-2.45264812 -3.43165268 -2.45264812]]\n",
      "[[-2.49336512 -3.47381195 -2.49336512]\n",
      " [-3.47381195 -2.49336512 -2.49336512]\n",
      " [-2.49336512 -2.49336512 -3.47381195]\n",
      " [-2.49336512 -3.47381195 -2.49336512]]\n",
      "[[-2.52155985 -3.5103403  -2.52155985]\n",
      " [-3.5103403  -2.52155985 -2.52155985]\n",
      " [-2.52155985 -2.52155985 -3.5103403 ]\n",
      " [-2.52155985 -3.5103403  -2.52155985]]\n",
      "[[-2.55042963 -3.54987914 -2.55042963]\n",
      " [-3.54987914 -2.55042963 -2.55042963]\n",
      " [-2.55042963 -2.55042963 -3.54987914]\n",
      " [-2.55042963 -3.54987914 -2.55042963]]\n",
      "[[-2.58088474 -3.59206832 -2.58088474]\n",
      " [-3.59206832 -2.58088474 -2.58088474]\n",
      " [-2.58088474 -2.58088474 -3.59206832]\n",
      " [-2.58088474 -3.59206832 -2.58088474]]\n",
      "[[-2.60979699 -3.63230823 -2.60979699]\n",
      " [-3.63230823 -2.60979699 -2.60979699]\n",
      " [-2.60979699 -2.60979699 -3.63230823]\n",
      " [-2.60979699 -3.63230823 -2.60979699]]\n",
      "[[-2.64180177 -3.67632473 -2.64180177]\n",
      " [-3.67632473 -2.64180177 -2.64180177]\n",
      " [-2.64180177 -2.64180177 -3.67632473]\n",
      " [-2.64180177 -3.67632473 -2.64180177]]\n",
      "[[-2.67418057 -3.72089242 -2.67418057]\n",
      " [-3.72089242 -2.67418057 -2.67418057]\n",
      " [-2.67418057 -2.67418057 -3.72089242]\n",
      " [-2.67418057 -3.72089242 -2.67418057]]\n",
      "[[-2.70746059 -3.76696036 -2.70746059]\n",
      " [-3.76696036 -2.70746059 -2.70746059]\n",
      " [-2.70746059 -2.70746059 -3.76696036]\n",
      " [-2.70746059 -3.76696036 -2.70746059]]\n",
      "[[-2.7399413  -3.81194976 -2.7399413 ]\n",
      " [-3.81194976 -2.7399413  -2.7399413 ]\n",
      " [-2.7399413  -2.7399413  -3.81194976]\n",
      " [-2.7399413  -3.81194976 -2.7399413 ]]\n",
      "[[-2.77739435 -3.86305237 -2.77739435]\n",
      " [-3.86305237 -2.77739435 -2.77739435]\n",
      " [-2.77739435 -2.77739435 -3.86305237]\n",
      " [-2.77739435 -3.86305237 -2.77739435]]\n",
      "[[-2.81574284 -3.91540089 -2.81574284]\n",
      " [-3.91540089 -2.81574284 -2.81574284]\n",
      " [-2.81574284 -2.81574284 -3.91540089]\n",
      " [-2.81574284 -3.91540089 -2.81574284]]\n",
      "[[-2.85574086 -3.97031396 -2.85574086]\n",
      " [-3.97031396 -2.85574086 -2.85574086]\n",
      " [-2.85574086 -2.85574086 -3.97031396]\n",
      " [-2.85574086 -3.97031396 -2.85574086]]\n",
      "[[-2.89518132 -4.02448922 -2.89518132]\n",
      " [-4.02448922 -2.89518132 -2.89518132]\n",
      " [-2.89518132 -2.89518132 -4.02448922]\n",
      " [-2.89518132 -4.02448922 -2.89518132]]\n",
      "[[-2.94136861 -4.08697404 -2.94136861]\n",
      " [-4.08697404 -2.94136861 -2.94136861]\n",
      " [-2.94136861 -2.94136861 -4.08697404]\n",
      " [-2.94136861 -4.08697404 -2.94136861]]\n",
      "[[-2.98869887 -4.15103724 -2.98869887]\n",
      " [-4.15103724 -2.98869887 -2.98869887]\n",
      " [-2.98869887 -2.98869887 -4.15103724]\n",
      " [-2.98869887 -4.15103724 -2.98869887]]\n",
      "[[-3.03830246 -4.21859419 -3.03830246]\n",
      " [-4.21859419 -3.03830246 -3.03830246]\n",
      " [-3.03830246 -3.03830246 -4.21859419]\n",
      " [-3.03830246 -4.21859419 -3.03830246]]\n",
      "[[-3.0873976 -4.2854968 -3.0873976]\n",
      " [-4.2854968 -3.0873976 -3.0873976]\n",
      " [-3.0873976 -3.0873976 -4.2854968]\n",
      " [-3.0873976 -4.2854968 -3.0873976]]\n",
      "[[-3.14549227 -4.36347997 -3.14549227]\n",
      " [-4.36347997 -3.14549227 -3.14549227]\n",
      " [-3.14549227 -3.14549227 -4.36347997]\n",
      " [-3.14549227 -4.36347997 -3.14549227]]\n",
      "[[-3.20509052 -4.44351561 -3.20509052]\n",
      " [-4.44351561 -3.20509052 -3.20509052]\n",
      " [-3.20509052 -3.20509052 -4.44351561]\n",
      " [-3.20509052 -4.44351561 -3.20509052]]\n",
      "[[-3.26783615 -4.52833455 -3.26783615]\n",
      " [-4.52833455 -3.26783615 -3.26783615]\n",
      " [-3.26783615 -3.26783615 -4.52833455]\n",
      " [-3.26783615 -4.52833455 -3.26783615]]\n",
      "[[-3.33015329 -4.61262419 -3.33015329]\n",
      " [-4.61262419 -3.33015329 -3.33015329]\n",
      " [-3.33015329 -3.33015329 -4.61262419]\n",
      " [-3.33015329 -4.61262419 -3.33015329]]\n",
      "[[-3.40441368 -4.71161614 -3.40441368]\n",
      " [-4.71161614 -3.40441368 -3.40441368]\n",
      " [-3.40441368 -3.40441368 -4.71161614]\n",
      " [-3.40441368 -4.71161614 -3.40441368]]\n",
      "[[-3.48072999 -4.8133803  -3.48072999]\n",
      " [-4.8133803  -3.48072999 -3.48072999]\n",
      " [-3.48072999 -3.48072999 -4.8133803 ]\n",
      " [-3.48072999 -4.8133803  -3.48072999]]\n",
      "[[-3.56148051 -4.92179692 -3.56148051]\n",
      " [-4.92179692 -3.56148051 -3.56148051]\n",
      " [-3.56148051 -3.56148051 -4.92179692]\n",
      " [-3.56148051 -4.92179692 -3.56148051]]\n",
      "[[-3.64199099 -5.02995009 -3.64199099]\n",
      " [-5.02995009 -3.64199099 -3.64199099]\n",
      " [-3.64199099 -3.64199099 -5.02995009]\n",
      " [-3.64199099 -5.02995009 -3.64199099]]\n",
      "[[-3.73836105 -5.15762967 -3.73836105]\n",
      " [-5.15762967 -3.73836105 -3.73836105]\n",
      " [-3.73836105 -3.73836105 -5.15762967]\n",
      " [-3.73836105 -5.15762967 -3.73836105]]\n",
      "[[-3.83765556 -5.28920131 -3.83765556]\n",
      " [-5.28920131 -3.83765556 -3.83765556]\n",
      " [-3.83765556 -3.83765556 -5.28920131]\n",
      " [-3.83765556 -5.28920131 -3.83765556]]\n",
      "[[-3.9433283  -5.43019652 -3.9433283 ]\n",
      " [-5.43019652 -3.9433283  -3.9433283 ]\n",
      " [-3.9433283  -3.9433283  -5.43019652]\n",
      " [-3.9433283  -5.43019652 -3.9433283 ]]\n",
      "[[-4.04917378 -5.5714812  -4.04917378]\n",
      " [-5.5714812  -4.04917378 -4.04917378]\n",
      " [-4.04917378 -4.04917378 -5.5714812 ]\n",
      " [-4.04917378 -5.5714812  -4.04917378]]\n",
      "[[-4.17626873 -5.73894104 -4.17626873]\n",
      " [-5.73894104 -4.17626873 -4.17626873]\n",
      " [-4.17626873 -4.17626873 -5.73894104]\n",
      " [-4.17626873 -5.73894104 -4.17626873]]\n",
      "[[-4.3076987  -5.91208848 -4.3076987 ]\n",
      " [-5.91208848 -4.3076987  -4.3076987 ]\n",
      " [-4.3076987  -4.3076987  -5.91208848]\n",
      " [-4.3076987  -5.91208848 -4.3076987 ]]\n",
      "[[-4.4485128  -6.09885765 -4.4485128 ]\n",
      " [-6.09885765 -4.4485128  -4.4485128 ]\n",
      " [-4.4485128  -4.4485128  -6.09885765]\n",
      " [-4.4485128  -6.09885765 -4.4485128 ]]\n",
      "[[-4.59033639 -6.28700782 -4.59033639]\n",
      " [-6.28700782 -4.59033639 -4.59033639]\n",
      " [-4.59033639 -4.59033639 -6.28700782]\n",
      " [-4.59033639 -6.28700782 -4.59033639]]\n",
      "[[-4.7611509  -6.51089218 -4.7611509 ]\n",
      " [-6.51089218 -4.7611509  -4.7611509 ]\n",
      " [-4.7611509  -4.7611509  -6.51089218]\n",
      " [-4.7611509  -6.51089218 -4.7611509 ]]\n",
      "[[-4.93866548 -6.74343259 -4.93866548]\n",
      " [-6.74343259 -4.93866548 -4.93866548]\n",
      " [-4.93866548 -4.93866548 -6.74343259]\n",
      " [-4.93866548 -6.74343259 -4.93866548]]\n",
      "[[-5.13032588 -6.99610953 -5.13032588]\n",
      " [-6.99610953 -5.13032588 -5.13032588]\n",
      " [-5.13032588 -5.13032588 -6.99610953]\n",
      " [-5.13032588 -6.99610953 -5.13032588]]\n",
      "[[-5.32462062 -7.25225653 -5.32462062]\n",
      " [-7.25225653 -5.32462062 -5.32462062]\n",
      " [-5.32462062 -5.32462062 -7.25225653]\n",
      " [-5.32462062 -7.25225653 -5.32462062]]\n",
      "[[-5.55951795 -7.55847599 -5.55951795]\n",
      " [-7.55847599 -5.55951795 -5.55951795]\n",
      " [-5.55951795 -5.55951795 -7.55847599]\n",
      " [-5.55951795 -7.55847599 -5.55951795]]\n",
      "[[-5.80519292 -7.87839738 -5.80519292]\n",
      " [-7.87839738 -5.80519292 -5.80519292]\n",
      " [-5.80519292 -5.80519292 -7.87839738]\n",
      " [-5.80519292 -7.87839738 -5.80519292]]\n",
      "[[-6.07275346 -8.22883584 -6.07275346]\n",
      " [-8.22883584 -6.07275346 -6.07275346]\n",
      " [-6.07275346 -6.07275346 -8.22883584]\n",
      " [-6.07275346 -8.22883584 -6.07275346]]\n",
      "[[-6.34605207 -8.58670104 -6.34605207]\n",
      " [-8.58670104 -6.34605207 -6.34605207]\n",
      " [-6.34605207 -6.34605207 -8.58670104]\n",
      " [-6.34605207 -8.58670104 -6.34605207]]\n",
      "[[-6.67811711 -9.01703486 -6.67811711]\n",
      " [-9.01703486 -6.67811711 -6.67811711]\n",
      " [-6.67811711 -6.67811711 -9.01703486]\n",
      " [-6.67811711 -9.01703486 -6.67811711]]\n",
      "[[-7.02816171 -9.46985778 -7.02816171]\n",
      " [-9.46985778 -7.02816171 -7.02816171]\n",
      " [-7.02816171 -7.02816171 -9.46985778]\n",
      " [-7.02816171 -9.46985778 -7.02816171]]\n",
      "[[-7.41297148 -9.97015319 -7.41297148]\n",
      " [-9.97015319 -7.41297148 -7.41297148]\n",
      " [-7.41297148 -7.41297148 -9.97015319]\n",
      " [-7.41297148 -9.97015319 -7.41297148]]\n",
      "[[ -7.80945827 -10.4854053   -7.80945827]\n",
      " [-10.4854053   -7.80945827  -7.80945827]\n",
      " [ -7.80945827  -7.80945827 -10.4854053 ]\n",
      " [ -7.80945827 -10.4854053   -7.80945827]]\n",
      "[[ -8.29434777 -11.10954335  -8.29434777]\n",
      " [-11.10954335  -8.29434777  -8.29434777]\n",
      " [ -8.29434777  -8.29434777 -11.10954335]\n",
      " [ -8.29434777 -11.10954335  -8.29434777]]\n",
      "[[ -8.81009917 -11.7716606   -8.81009917]\n",
      " [-11.7716606   -8.81009917  -8.81009917]\n",
      " [ -8.81009917  -8.81009917 -11.7716606 ]\n",
      " [ -8.81009917 -11.7716606   -8.81009917]]\n",
      "[[ -9.38245477 -12.50950163  -9.38245477]\n",
      " [-12.50950163  -9.38245477  -9.38245477]\n",
      " [ -9.38245477  -9.38245477 -12.50950163]\n",
      " [ -9.38245477 -12.50950163  -9.38245477]]\n",
      "[[ -9.97799989 -13.27682055  -9.97799989]\n",
      " [-13.27682055  -9.97799989  -9.97799989]\n",
      " [ -9.97799989  -9.97799989 -13.27682055]\n",
      " [ -9.97799989 -13.27682055  -9.97799989]]\n",
      "[[-10.71238884 -14.21462939 -10.71238884]\n",
      " [-14.21462939 -10.71238884 -10.71238884]\n",
      " [-10.71238884 -10.71238884 -14.21462939]\n",
      " [-10.71238884 -14.21462939 -10.71238884]]\n"
     ]
    }
   ],
   "source": [
    "# neural turing machine with FF as controller\n",
    "# see also https://github.com/flomlo/ntm_keras\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def tanh_arr(x, d=False):\n",
    "    if d:\n",
    "        return 1 - np.tanh(x)\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "\n",
    "def siginv_arr(y, d=False): # sigmoid inversed\n",
    "    if d:\n",
    "        return 1 / y / (1 - y)\n",
    "    else:\n",
    "        return np.log( y / (1 - y) )\n",
    "\n",
    "def sig_arr(x, d=False): # sigmoid\n",
    "    if d:\n",
    "        return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x, d=False):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    res = ex / ex.sum()\n",
    "    if d:\n",
    "        return res * (1 - res)\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def simK(u, v): # similarity measure\n",
    "    return np.dot(u, v) / np.linalg.norm(u) / np.linalg.norm(v)\n",
    "\n",
    "def weights(wt, wt_1, Mt, kt, betat, gt, st, gammat):\n",
    "    nmemslots = np.shape(Mt)[0]\n",
    "    for i in range(nmemslots):\n",
    "        wt[i] = softmax(betat * simK(Mt[i], kt), False) # content addressing\n",
    "    wt = gt * wt + (1 - gt) * wt_1 # gated weighing\n",
    "    wt = np.convolve(wt, st, mode='same') # convolutional shift - todo: circular (does not make a big difference)\n",
    "    wt = (np.sign(wt) * np.abs(wt) ** gammat) / np.sum(np.sign(wt) * np.abs(wt) ** gammat) # sharpening (careful fractional powers)\n",
    "    return wt\n",
    "\n",
    "def concat(atgt, *args): # custom concat\n",
    "    res = atgt.flatten().tolist()\n",
    "    for e in args:\n",
    "        if type(e) is float:\n",
    "            res += [e]\n",
    "        elif type(e) is list:\n",
    "            res += e\n",
    "        else:\n",
    "            res += e.flatten().tolist()\n",
    "    return np.array(res)\n",
    "\n",
    "def act_fwd(hc, csplit): # activation forward pass\n",
    "    # subvectors: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    # lengths in csplit: ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    # activations: ReLU + ReLU + tanh + ? + ? + soft + sig(inv, clip) + sig + tanh\n",
    "    yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc = np.split(hc, csplit)\n",
    "    yc = np.maximum(yc, 0, yc) # ReLU\n",
    "    wtc = np.maximum(wtc, 0, wtc) # ReLU\n",
    "    ktc = tanh_arr(ktc, False)\n",
    "    betatc = np.maximum(betatc, 0, betatc) # ReLU ?\n",
    "    gtc = np.maximum(gtc, 0, gtc) # ReLU ?\n",
    "    stc = softmax(stc, False)\n",
    "    gammatc = np.clip(sig_arr(gammatc, False), -1, 1) # ?\n",
    "    etc = sig_arr(etc, False)\n",
    "    atc = tanh_arr(atc, False)\n",
    "    return concat(yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc)\n",
    "\n",
    "def act_bck(hc, csplit): #  activation backpropagation\n",
    "    # subvectors: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    # lengths in csplit: ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    # activations: ReLU + ReLU + tanh + ? + ? + soft + sig(inv, clip) + sig + tanh\n",
    "    yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc = np.split(hc, csplit)\n",
    "    yc = ((yc > 0) * 1.) # ReLU\n",
    "    wtc = ((wtc > 0) * 1.) # ReLU\n",
    "    ktc = tanh_arr(ktc, True)\n",
    "    betatc = ((betatc > 0) * 1.) # ReLU ?\n",
    "    gtc = ((gtc > 0) * 1.) # ReLU ?\n",
    "    stc = softmax(stc, True)\n",
    "    gammatc = np.clip(sig_arr(gammatc, True), -1, 1) # ?\n",
    "    etc = sig_arr(etc, True)\n",
    "    atc = tanh_arr(atc, True)\n",
    "    return concat(yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # parameters\n",
    "    ndata = 4 # size of external input data\n",
    "    etha = 0.1 # learning rate\n",
    "    ntmstps = 20\n",
    "    \n",
    "    # fill memory\n",
    "    m_depth = 3\n",
    "    Mt = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 0, 1]]) # memory (N slots), m_depth=3\n",
    "    nmemslots = np.shape(Mt)[0] # 4\n",
    "    wt = np.array([0.25, 0.25, 0.25, 0.25]) # weights 1-N: one for each memory entry, initial values\n",
    "    \n",
    "    # the controller vectors are (data_vector, kt, beta, gt, st, gamma)\n",
    "    kt = np.zeros((1, m_depth)) # key vector of length M (here: m_depth)\n",
    "    betat = 0.8 # key strength\n",
    "    gt = 0.5 # interpolation gate\n",
    "    st = np.array([0.1, 0.8, 0.1]) # shift weighting\n",
    "    gammat = 2.0 # sharpening\n",
    "    # length of controller vector: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    nctrlvec = ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    ctrl_inpv = np.zeros((1, nctrlvec)) # input controller\n",
    "    \n",
    "    # split vector data + wt-1 + kt + beta + gt + st + gamma\n",
    "    ctrl_out_split = np.array([ndata, ndata+nmemslots, ndata+nmemslots+m_depth, ndata+nmemslots+m_depth+1, ndata+nmemslots+m_depth+2, ndata+nmemslots+m_depth+5, ndata+nmemslots+m_depth+6, ndata+nmemslots+m_depth+6+nmemslots])\n",
    "    et = np.zeros((nmemslots)) # erase vector\n",
    "    at = np.zeros((nmemslots)) # add vector\n",
    "    \n",
    "    x = np.zeros((ndata)) # input external data\n",
    "    y = np.zeros((ndata)) # output external data\n",
    "    \n",
    "    # model parameters\n",
    "    W1 = np.random.randn(nctrlvec, nctrlvec)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(nctrlvec, nctrlvec)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, nctrlvec)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, nctrlvec)) # hidden-out bias\n",
    "    \n",
    "    # time steps\n",
    "    for thist in range(ntmstps):\n",
    "\n",
    "        # head moves todo: add multihead\n",
    "        iheadpos = 0\n",
    "        while iheadpos < nmemslots:\n",
    "            wt_1 = wt\n",
    "\n",
    "            # old memory Mt, read-weights wt from last step: feed read-vector to controller\n",
    "            # calculate read vector to feed controller\n",
    "            # read output vector from head location\n",
    "            rt = np.dot(wt, Mt) # dim M, weighted memories\n",
    "\n",
    "            # controller runs a single step (with input from outside)\n",
    "            x = np.array([[0., 1., 0., 0.]]) # receive input vector from outside todo: really outside!\n",
    "            ctrl_inpv = concat(x, wt, kt, betat, gt, st, gammat, et, at)\n",
    "            # set indices for erase/add\n",
    "            if iheadpos > 0:\n",
    "                et[iheadpos-1] = 0\n",
    "                at[iheadpos-1] = 0\n",
    "            et[iheadpos] = 1\n",
    "            at[iheadpos] = 1\n",
    "            # controller forward pass, single step\n",
    "            h1 = np.dot(ctrl_inpv, W1) + b1\n",
    "            #h1 = np.maximum(h1, 0, h1) # ReLU todo: different by parameter\n",
    "            h1 = act_fwd(h1[0], ctrl_out_split) # forward pass (different activation by subvector)\n",
    "            o2 = np.dot(h1, W2) + b2\n",
    "            # backward pass\n",
    "            y[1] = 1. # arbitrary external output data (truth), todo: check, what is the entire truth?\n",
    "            ctrl_outv = concat(y, wt, kt, betat, gt, st, gammat, et, at)\n",
    "            #dW1 = - etha * (o2 - ctrl_outv) * np.maximum(h1, 0, h1)\n",
    "            dW1 = - etha * (o2 - ctrl_outv) * h1\n",
    "            dW2 = dW1 * act_bck(h1, ctrl_out_split) * ctrl_inpv\n",
    "            W1 += dW1\n",
    "            W2 += dW2\n",
    "\n",
    "            # controller unactivated output divided into actual data output, reading, writing instructions\n",
    "            # split: output_dim, read_heads, write_heads\n",
    "            # split and apply activations:\n",
    "            # k and add_vector are activated via tanh, erase_vector via sigmoid (this is critical!),\n",
    "            # shift via softmax, gamma is sigmoided, inversed and clipped (probably not ideal)\n",
    "            # g is sigmoided, beta is linear (probably not ideal!)\n",
    "            y, wt, kt, betat, gt, st, gammat, et, at = np.split(o2[0], ctrl_out_split)\n",
    "\n",
    "            # write memory for each head: calculate weights, erase, add\n",
    "            wt = weights(wt, wt_1, Mt, kt, betat, gt, st, gammat)\n",
    "            Mt = Mt - Mt * np.dot(wt, et.T) # erase\n",
    "            Mt = Mt + np.dot(wt, at.T) # add\n",
    "\n",
    "            # calculate read weights, save in the state and use for next round\n",
    "            # todo\n",
    "            iheadpos += 1\n",
    "            \n",
    "            print(Mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "[94, 49, 73, 128, 174, 45, 45, 198, 156, 2]\n",
      "[[1, 1, 1, 0, 0, 1, 1, 0], [1, 1, 1, 0, 1, 1, 0, 0], [0, 1, 0, 0, 0, 1, 1, 1], [1, 1, 1, 0, 1, 1, 1, 1], [0, 1, 0, 1, 1, 1, 1, 1], [0, 0, 1, 0, 0, 1, 0, 0], [1, 1, 1, 0, 1, 1, 1, 0], [1, 0, 1, 1, 0, 0, 1, 0], [1, 0, 1, 1, 0, 1, 0, 1], [0, 0, 1, 1, 1, 1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def rdbinseq(l): # random sequence of 8-bit binary vectors (int 0-255)\n",
    "    if l <= 0:\n",
    "        return []\n",
    "    res = []\n",
    "    for i in range(l):\n",
    "        res.append(random.randint(0, 255))\n",
    "    return res\n",
    "\n",
    "def rdbinseqb(n, l): # random sequence of n-bit binary vectors\n",
    "    if l <= 0:\n",
    "        return []\n",
    "    res = []\n",
    "    for i in range(l):\n",
    "        res.append([random.randint(0, 1) for j in range(n)])\n",
    "    return res\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(int('11111111', 2))\n",
    "    print(rdbinseq(10))\n",
    "    print(rdbinseqb(8, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]]\n",
      "[[-0.27381219  1.28205607 -0.27381219]\n",
      " [-0.27381219 -0.27381219  1.28205607]\n",
      " [ 1.28205607 -0.27381219 -0.27381219]\n",
      " [-0.27381219  1.28205607 -0.27381219]]\n"
     ]
    }
   ],
   "source": [
    "# neural turing machine with FF as controller\n",
    "# copy task\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "def rdbinseqb(n, l): # random sequence of n-bit binary vectors\n",
    "    if l <= 0:\n",
    "        return []\n",
    "    res = []\n",
    "    for i in range(l):\n",
    "        res.append([random.randint(0, 1) for j in range(n)])\n",
    "    return res\n",
    "\n",
    "def tanh_arr(x, d=False):\n",
    "    if d:\n",
    "        return 1 - np.tanh(x)\n",
    "    else:\n",
    "        return np.tanh(x)\n",
    "\n",
    "def siginv_arr(y, d=False): # sigmoid inversed\n",
    "    if d:\n",
    "        return 1 / y / (1 - y)\n",
    "    else:\n",
    "        return np.log( y / (1 - y) )\n",
    "\n",
    "def sig_arr(x, d=False): # sigmoid\n",
    "    if d:\n",
    "        return np.exp(-x) / (1 + np.exp(-x)) ** 2\n",
    "    else:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x, d=False):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    res = ex / ex.sum()\n",
    "    if d:\n",
    "        return res * (1 - res)\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def simK(u, v): # similarity measure\n",
    "    return np.dot(u, v) / np.linalg.norm(u) / np.linalg.norm(v)\n",
    "\n",
    "def weights(wt, wt_1, Mt, kt, betat, gt, st, gammat):\n",
    "    nmemslots = np.shape(Mt)[0]\n",
    "    for i in range(nmemslots):\n",
    "        wt[i] = softmax(betat * simK(Mt[i], kt), False) # content addressing\n",
    "    wt = gt * wt + (1 - gt) * wt_1 # gated weighing\n",
    "    wt = np.convolve(wt, st, mode='same') # convolutional shift - todo: circular (does not make a big difference)\n",
    "    wt = (np.sign(wt) * np.abs(wt) ** gammat) / np.sum(np.sign(wt) * np.abs(wt) ** gammat) # sharpening (careful fractional powers)\n",
    "    return wt\n",
    "\n",
    "def concat(atgt, *args): # custom concat\n",
    "    res = atgt.flatten().tolist()\n",
    "    for e in args:\n",
    "        if type(e) is float:\n",
    "            res += [e]\n",
    "        elif type(e) is list:\n",
    "            res += e\n",
    "        else:\n",
    "            res += e.flatten().tolist()\n",
    "    return np.array(res)\n",
    "\n",
    "def act_fwd(hc, csplit): # activation forward pass\n",
    "    # subvectors: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    # lengths in csplit: ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    # activations: ReLU + ReLU + tanh + ? + ? + soft + sig(inv, clip) + sig + tanh\n",
    "    yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc = np.split(hc, csplit)\n",
    "    yc = np.maximum(yc, 0, yc) # ReLU\n",
    "    wtc = np.maximum(wtc, 0, wtc) # ReLU\n",
    "    ktc = tanh_arr(ktc, False)\n",
    "    betatc = np.maximum(betatc, 0, betatc) # ReLU ?\n",
    "    gtc = np.maximum(gtc, 0, gtc) # ReLU ?\n",
    "    stc = softmax(stc, False)\n",
    "    gammatc = np.clip(sig_arr(gammatc, False), -1, 1) # ?\n",
    "    etc = sig_arr(etc, False)\n",
    "    atc = tanh_arr(atc, False)\n",
    "    return concat(yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc)\n",
    "\n",
    "def act_bck(hc, csplit): #  activation backpropagation\n",
    "    # subvectors: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    # lengths in csplit: ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    # activations: ReLU + ReLU + tanh + ? + ? + soft + sig(inv, clip) + sig + tanh\n",
    "    yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc = np.split(hc, csplit)\n",
    "    yc = ((yc > 0) * 1.) # ReLU\n",
    "    wtc = ((wtc > 0) * 1.) # ReLU\n",
    "    ktc = tanh_arr(ktc, True)\n",
    "    betatc = ((betatc > 0) * 1.) # ReLU ?\n",
    "    gtc = ((gtc > 0) * 1.) # ReLU ?\n",
    "    stc = softmax(stc, True)\n",
    "    gammatc = np.clip(sig_arr(gammatc, True), -1, 1) # ?\n",
    "    etc = sig_arr(etc, True)\n",
    "    atc = tanh_arr(atc, True)\n",
    "    return concat(yc, wtc, ktc, betatc, gtc, stc, gammatc, etc, atc)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # parameters\n",
    "    ndata = 4 # size of external input data\n",
    "    etha = 0.1 # learning rate\n",
    "    ntmstps = 10\n",
    "    \n",
    "    # fill memory\n",
    "    m_depth = 3\n",
    "    #Mt = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0], [1, 0, 1]]) # memory (N slots), m_depth=3\n",
    "    Mt = np.array(rdbinseqb(3, 4))\n",
    "    print(Mt)\n",
    "    nmemslots = np.shape(Mt)[0] # 4\n",
    "    wt = np.array([0.25, 0.25, 0.25, 0.25]) # weights 1-N: one for each memory entry, initial values\n",
    "    \n",
    "    # the controller vectors are (data_vector, kt, beta, gt, st, gamma)\n",
    "    kt = np.zeros((1, m_depth)) # key vector of length M (here: m_depth)\n",
    "    betat = 0.8 # key strength\n",
    "    gt = 0.5 # interpolation gate\n",
    "    st = np.array([0.1, 0.8, 0.1]) # shift weighting\n",
    "    gammat = 2.0 # sharpening\n",
    "    # length of controller vector: data + wt-1 + kt + beta + gt + st + gamma + et + at\n",
    "    nctrlvec = ndata + nmemslots + m_depth + 1 + 1 + 3 + 1 + 2 * nmemslots\n",
    "    ctrl_inpv = np.zeros((1, nctrlvec)) # input controller\n",
    "    \n",
    "    # split vector data + wt-1 + kt + beta + gt + st + gamma\n",
    "    ctrl_out_split = np.array([ndata, ndata+nmemslots, ndata+nmemslots+m_depth, ndata+nmemslots+m_depth+1, ndata+nmemslots+m_depth+2, ndata+nmemslots+m_depth+5, ndata+nmemslots+m_depth+6, ndata+nmemslots+m_depth+6+nmemslots])\n",
    "    et = np.zeros((nmemslots)) # erase vector\n",
    "    at = np.zeros((nmemslots)) # add vector\n",
    "    \n",
    "    x = np.zeros((ndata)) # input external data\n",
    "    y = np.zeros((ndata)) # output external data\n",
    "    \n",
    "    # model parameters\n",
    "    W1 = np.random.randn(nctrlvec, nctrlvec)*0.01 # input to hidden\n",
    "    W2 = np.random.randn(nctrlvec, nctrlvec)*0.01 # hidden to output\n",
    "    b1 = np.zeros((1, nctrlvec)) # inp-hidden bias\n",
    "    b2 = np.zeros((1, nctrlvec)) # hidden-out bias\n",
    "    \n",
    "    # time steps\n",
    "    for thist in range(ntmstps):\n",
    "\n",
    "        # head moves todo: add multihead\n",
    "        iheadpos = 0\n",
    "        while iheadpos < nmemslots:\n",
    "            wt_1 = wt\n",
    "\n",
    "            # old memory Mt, read-weights wt from last step: feed read-vector to controller\n",
    "            # calculate read vector to feed controller\n",
    "            # read output vector from head location\n",
    "            rt = np.dot(wt, Mt) # dim M, weighted memories\n",
    "\n",
    "            # controller runs a single step (with input from outside)\n",
    "            x = np.array([[0., 1., 0., 0.]]) # receive input vector from outside todo: really outside!\n",
    "            ctrl_inpv = concat(x, wt, kt, betat, gt, st, gammat, et, at)\n",
    "            # set indices for erase/add\n",
    "            if iheadpos > 0:\n",
    "                et[iheadpos-1] = 0\n",
    "                at[iheadpos-1] = 0\n",
    "            et[iheadpos] = 1\n",
    "            at[iheadpos] = 1\n",
    "            # controller forward pass, single step\n",
    "            h1 = np.dot(ctrl_inpv, W1) + b1\n",
    "            #h1 = np.maximum(h1, 0, h1) # ReLU todo: different by parameter\n",
    "            h1 = act_fwd(h1[0], ctrl_out_split) # forward pass (different activation by subvector)\n",
    "            o2 = np.dot(h1, W2) + b2\n",
    "            # backward pass\n",
    "            y[1] = 1. # arbitrary external output data (truth), todo: check, what is the entire truth?\n",
    "            ctrl_outv = concat(y, wt, kt, betat, gt, st, gammat, et, at)\n",
    "            #dW1 = - etha * (o2 - ctrl_outv) * np.maximum(h1, 0, h1)\n",
    "            dW1 = - etha * (o2 - ctrl_outv) * h1\n",
    "            dW2 = dW1 * act_bck(h1, ctrl_out_split) * ctrl_inpv\n",
    "            W1 += dW1\n",
    "            W2 += dW2\n",
    "\n",
    "            # controller unactivated output divided into actual data output, reading, writing instructions\n",
    "            # split: output_dim, read_heads, write_heads\n",
    "            # split and apply activations:\n",
    "            # k and add_vector are activated via tanh, erase_vector via sigmoid (this is critical!),\n",
    "            # shift via softmax, gamma is sigmoided, inversed and clipped (probably not ideal)\n",
    "            # g is sigmoided, beta is linear (probably not ideal!)\n",
    "            y, wt, kt, betat, gt, st, gammat, et, at = np.split(o2[0], ctrl_out_split)\n",
    "\n",
    "            # write memory for each head: calculate weights, erase, add\n",
    "            wt = weights(wt, wt_1, Mt, kt, betat, gt, st, gammat)\n",
    "            Mt = Mt - Mt * np.dot(wt, et.T) # erase\n",
    "            Mt = Mt + np.dot(wt, at.T) # add\n",
    "\n",
    "            # calculate read weights, save in the state and use for next round\n",
    "            # todo\n",
    "            iheadpos += 1\n",
    "            \n",
    "    print(Mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'into': 0, 'kitchen': 1, 'sam': 2, 'the': 3, 'walks': 4, '': 5, 'an': 6, 'apple': 7, 'picks': 8, 'up': 9, 'bedroom': 10, 'drops': 11, 'is': 12, 'where': 13}\n",
      "[1.31351085e-03 1.60122327e-07 7.42432013e-04 2.38615087e-03\n",
      " 7.28085913e-06 1.34016459e-06 3.14062288e-05 7.39954981e-04\n",
      " 7.87630566e-07 2.22681567e-03 9.90902630e-01 1.64277869e-03\n",
      " 2.23272785e-07 4.52829329e-06]\n",
      "bedroom\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 10 # embedding dimension\n",
    "    lr = 0.1 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    doc_raw = 'sam walks into the kitchen. sam picks up an apple. sam walks into the bedroom. sam drops the apple.'\n",
    "    query_raw = 'where is sam'\n",
    "    document = doc_raw.split('.')\n",
    "    document.append(query_raw)\n",
    "    senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    print(dictionary.token2id)\n",
    "    document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dictionary)\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "    n_memories = len(document)\n",
    "    #print(voc)\n",
    "    #print(document)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.1 # input to memory embedding\n",
    "    tA = np.zeros(d_embed) # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.1 # query embedding\n",
    "    tB = np.zeros(d_embed) # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.1 # output to memory embedding\n",
    "    tC = np.zeros(d_embed) # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.1 # final weight matrix\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mtA = np.zeros_like(tA)\n",
    "    mB = np.zeros_like(B)\n",
    "    mtB = np.zeros_like(tB)\n",
    "    mC = np.zeros_like(C)\n",
    "    mtC = np.zeros_like(tC)\n",
    "    mW = np.zeros_like(W)\n",
    "    \n",
    "    for iterctr in range(10000):\n",
    "\n",
    "        # forward pass\n",
    "\n",
    "        # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "        x = np.zeros((n_memories, voc))\n",
    "        m = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            thissent = document[i].split()\n",
    "            for j in range(len(thissent)):\n",
    "                x[i][j] = dictionary.token2id[thissent[j]]\n",
    "        for i in range(n_memories):\n",
    "            m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "        #print(x)\n",
    "        #print(m)\n",
    "\n",
    "        # query embedding u = B * q + tB\n",
    "        q = np.zeros(voc)\n",
    "        u = np.zeros(d_embed)\n",
    "        thissent = query_raw.split()\n",
    "        for j in range(len(thissent)):\n",
    "            q[j] = dictionary.token2id[thissent[j]]\n",
    "        u = np.dot(B, q) + tB\n",
    "\n",
    "        # match of query with memory p = softmax(u * mi) for all i\n",
    "        p = np.zeros((n_memories, d_embed))\n",
    "        p = softmax(np.dot(u, m.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi + tC\n",
    "        c = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            c[i] = np.dot(C, x[i].T) + tC\n",
    "\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(p.T, c)\n",
    "\n",
    "        # predicted label a = softmax( W * (o + u))\n",
    "        a_predict = softmax(np.dot(W, (o + u)))\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dC = np.zeros_like(C)\n",
    "        dW = np.zeros_like(W)\n",
    "        dtA = np.zeros_like(tA)\n",
    "        dtB = np.zeros_like(tB)\n",
    "        dtC = np.zeros_like(tC)\n",
    "\n",
    "        truth = np.zeros_like(tA)\n",
    "        truth[10] = 1 # bedroom\n",
    "        dy = a_predict - truth\n",
    "        # dA = dy a_predict * (1-a_predict) W sumi p[i] (1-p[i]) ( u.T * 1A * x[i]) c[i]\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABCunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        Wunit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "        tunit = np.ones_like(tA)\n",
    "\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u, np.dot(ABCunit, x[i].T)), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1B q).T m[i]) c[i] + 1B q)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(np.dot(ABCunit, q), m[i]), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dC = dy a_predict * (1-a_predict) W sumi p[i] 1C x[i]\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*np.dot(ABCunit, x[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dC = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dC)\n",
    "\n",
    "        # dW = dy a_predict * (1-a_predict) (o + u)\n",
    "        dW = (np.dot(dy, a_predict*(1-a_predict)) * Wunit * (o + u))\n",
    "        #print(dW)   \n",
    "\n",
    "        # dtA = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) (u.T 1tA) c[i])\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u.T, tunit), c[i])\n",
    "        dtA = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtA)\n",
    "\n",
    "        # dtB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1tB).T m[i] c[i]) + 1tB)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*(np.dot(np.dot(tunit.T, m[i]), c[i]) + tunit)\n",
    "        dtB = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtB)\n",
    "\n",
    "        # dtC = dy a_predict * (1-a_predict) W ( sumi p[i] 1tC )\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*tunit\n",
    "        dtC = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtC)\n",
    "\n",
    "        # maybe clip ?\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,C,W,tA,tB,tC], [dA,dB,dC,dW,dtA,dtB,dtC], [mA,mB,mC,mW,mtA,mtB,mtC]):\n",
    "            memwghts += dweights * dweights\n",
    "            weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bathroom': 0, 'mary': 1, 'moved': 2, 'the': 3, 'to': 4, '': 5, 'hallway': 6, 'john': 7, 'went': 8, 'is': 9, 'mary?': 10, 'where': 11}\n",
      "[9.99999968e-01 2.55514840e-08 4.19416174e-10 6.86523189e-10\n",
      " 9.61669880e-10 6.16187546e-10 6.17625220e-10 7.72161930e-10\n",
      " 6.33943041e-10 5.70095601e-10 6.42006174e-10 5.86618848e-10]\n",
      "bathroom\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 0 # embedding dimension\n",
    "    lr = 1.8 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    doc_raw = 'Mary moved to the bathroom. John went to the hallway.'\n",
    "    query_raw = 'Where is Mary?'\n",
    "    document = doc_raw.lower().split('.')\n",
    "    document.append(query_raw.lower())\n",
    "    senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    print(dictionary.token2id)\n",
    "    document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dictionary)\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "    n_memories = len(document)\n",
    "    #print(voc)\n",
    "    #print(document)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding\n",
    "    tA = np.random.randn(d_embed)*0.01 # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding\n",
    "    tB = np.random.randn(d_embed)*0.01 # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.01 # output to memory embedding\n",
    "    tC = np.random.randn(d_embed)*0.01 # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.01 # final weight matrix\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mtA = np.zeros_like(tA)\n",
    "    mB = np.zeros_like(B)\n",
    "    mtB = np.zeros_like(tB)\n",
    "    mC = np.zeros_like(C)\n",
    "    mtC = np.zeros_like(tC)\n",
    "    mW = np.zeros_like(W)\n",
    "\n",
    "    x = np.zeros((n_memories, voc))\n",
    "    for i in range(n_memories):\n",
    "        thissent = document[i].lower().split()\n",
    "        for j in range(len(thissent)):\n",
    "            x[i][j] = dictionary.token2id[thissent[j]]\n",
    "         \n",
    "    for iterctr in range(10000):\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "        m = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "\n",
    "        # query embedding u = B * q + tB\n",
    "        q = np.zeros(voc)\n",
    "        u = np.zeros(d_embed)\n",
    "        thissent = query_raw.lower().split()\n",
    "        for j in range(len(thissent)):\n",
    "            q[j] = dictionary.token2id[thissent[j]]\n",
    "        u = np.dot(B, q) + tB\n",
    "\n",
    "        # match of query with memory p = softmax(u * mi) for all i\n",
    "        p = np.zeros((n_memories, d_embed))\n",
    "        p = softmax(np.dot(u, m.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi + tC\n",
    "        c = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            c[i] = np.dot(C, x[i].T) + tC\n",
    "\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(p.T, c)\n",
    "\n",
    "        # predicted label a = softmax( W * (o + u))\n",
    "        a_predict = softmax(np.dot(W, (o + u)))\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dC = np.zeros_like(C)\n",
    "        dW = np.zeros_like(W)\n",
    "        dtA = np.zeros_like(tA)\n",
    "        dtB = np.zeros_like(tB)\n",
    "        dtC = np.zeros_like(tC)\n",
    "\n",
    "        truth = np.zeros_like(tA)\n",
    "        truth[0] = 1 # answer\n",
    "        dy = a_predict - truth\n",
    "        # dA = dy a_predict * (1-a_predict) W sumi p[i] (1-p[i]) ( u.T * 1A * x[i]) c[i]\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABCunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        Wunit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "        tunit = np.ones_like(tA)\n",
    "\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u, np.dot(ABCunit, x[i].T)), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1B q).T m[i]) c[i] + 1B q)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(np.dot(ABCunit, q), m[i]), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dC = dy a_predict * (1-a_predict) W sumi p[i] 1C x[i]\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*np.dot(ABCunit, x[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dC = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dC)\n",
    "\n",
    "        # dW = dy a_predict * (1-a_predict) (o + u)\n",
    "        dW = (np.dot(dy, a_predict*(1-a_predict)) * Wunit * (o + u))\n",
    "        #print(dW)   \n",
    "\n",
    "        # dtA = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) (u.T 1tA) c[i])\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u.T, tunit), c[i])\n",
    "        dtA = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtA)\n",
    "\n",
    "        # dtB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1tB).T m[i] c[i]) + 1tB)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*(np.dot(np.dot(tunit.T, m[i]), c[i]) + tunit)\n",
    "        dtB = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtB)\n",
    "\n",
    "        # dtC = dy a_predict * (1-a_predict) W ( sumi p[i] 1tC )\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*tunit\n",
    "        dtC = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtC)\n",
    "\n",
    "        # maybe clip ?\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,C,W,tA,tB,tC], [dA,dB,dC,dW,dtA,dtB,dtC], [mA,mB,mC,mW,mtA,mtB,mtC]):\n",
    "            memwghts += dweights * dweights\n",
    "            #weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "            weights += -lr * dweights\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'bathroom': 1, 'hallway': 2, 'john': 3, 'journeyed': 4, 'mary': 5, 'the': 6, 'to': 7, 'travelled': 8, '?': 9, 'is': 10, 'where': 11, 'back': 12, 'bedroom': 13, 'daniel': 14, 'moved': 15, 'went': 16, 'kitchen': 17, 'sandra': 18, 'garden': 19, 'office': 20}\n",
      "[(['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.'], ['where', 'is', 'john', '?'], 'hallway'), (['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.', 'daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.'], ['where', 'is', 'mary', '?'], 'bathroom'), (['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.', 'daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'john', 'went', 'to', 'the', 'hallway', '.', 'sandra', 'journeyed', 'to', 'the', 'kitchen', '.'], ['where', 'is', 'sandra', '?'], 'kitchen'), (['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.', 'daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'john', 'went', 'to', 'the', 'hallway', '.', 'sandra', 'journeyed', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'john', 'went', 'to', 'the', 'garden', '.'], ['where', 'is', 'sandra', '?'], 'hallway'), (['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.', 'daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'john', 'went', 'to', 'the', 'hallway', '.', 'sandra', 'journeyed', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'john', 'went', 'to', 'the', 'garden', '.', 'sandra', 'went', 'back', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'kitchen', '.'], ['where', 'is', 'sandra', '?'], 'kitchen'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.'], ['where', 'is', 'sandra', '?'], 'hallway'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'went', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'garden', '.'], ['where', 'is', 'sandra', '?'], 'garden'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'went', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'garden', '.', 'sandra', 'travelled', 'to', 'the', 'office', '.', 'daniel', 'journeyed', 'to', 'the', 'hallway', '.'], ['where', 'is', 'daniel', '?'], 'hallway'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'went', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'garden', '.', 'sandra', 'travelled', 'to', 'the', 'office', '.', 'daniel', 'journeyed', 'to', 'the', 'hallway', '.', 'daniel', 'journeyed', 'to', 'the', 'office', '.', 'john', 'moved', 'to', 'the', 'hallway', '.'], ['where', 'is', 'sandra', '?'], 'office'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'went', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'garden', '.', 'sandra', 'travelled', 'to', 'the', 'office', '.', 'daniel', 'journeyed', 'to', 'the', 'hallway', '.', 'daniel', 'journeyed', 'to', 'the', 'office', '.', 'john', 'moved', 'to', 'the', 'hallway', '.', 'john', 'travelled', 'to', 'the', 'bathroom', '.', 'john', 'journeyed', 'to', 'the', 'office', '.'], ['where', 'is', 'daniel', '?'], 'office')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import functools\n",
    "from gensim import corpora\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''\n",
    "    argument: a sentence string\n",
    "    returns a list of tokens(words)\n",
    "    '''\n",
    "    return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    " \n",
    "def parse_stories(lines):\n",
    "    '''\n",
    "    - Parse stories provided in the bAbI tasks format\n",
    "    - A story starts from line 1 to line 15. Every 3rd line,\n",
    "      there is a question & answer.\n",
    "    - Function extracts sub-stories within a story and\n",
    "      creates tuples\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        line = line.strip().lower()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A & support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    " \n",
    "def get_stories(f):\n",
    "    '''\n",
    "    argument: filename\n",
    "    returns list of all stories in the argument data-set file\n",
    "    '''\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines())\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: functools.reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data]\n",
    "    return data\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    stories = get_stories(open('qa1_test_short.txt', 'r'))\n",
    "    dct = corpora.Dictionary()\n",
    "    for stry in stories:\n",
    "        dct.add_documents([[s for s in stry[0]], stry[1], [stry[2]]])\n",
    "    print(dct.token2id)\n",
    "    print(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'bathroom': 1, 'hallway': 2, 'john': 3, 'journeyed': 4, 'mary': 5, 'the': 6, 'to': 7, 'travelled': 8, '?': 9, 'is': 10, 'where': 11, 'back': 12, 'bedroom': 13, 'daniel': 14, 'moved': 15, 'went': 16, 'kitchen': 17, 'sandra': 18, 'garden': 19, 'office': 20}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (60,) (21,21) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-82a56c94531d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    207\u001b[0m                 \u001b[0mdEAtmp\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mABCunit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mdEAtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdEAtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mdA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ma_predict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0miq\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdEAtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m             \u001b[1;31m#print(dA)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (60,) (21,21) "
     ]
    }
   ],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "import re\n",
    "import functools\n",
    "\n",
    "def parse_stories(lines):\n",
    "    '''\n",
    "    - Parse stories provided in the bAbI tasks format\n",
    "    - A story starts from line 1 to line 15. Every 3rd line,\n",
    "      there is a question & answer.\n",
    "    - Function extracts sub-stories within a story and\n",
    "      creates tuples\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        line = line.strip().lower()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A & support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    " \n",
    "def get_stories(f):\n",
    "    '''\n",
    "    argument: filename\n",
    "    returns list of all stories in the argument data-set file\n",
    "    '''\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines())\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: functools.reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data]\n",
    "    return data\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "def splitlst(lst, sep): # split list at separator sep into different lists\n",
    "    res = []\n",
    "    thisl = []\n",
    "    for e in lst:\n",
    "        if e == sep:\n",
    "            res.append(thisl)\n",
    "            thisl = []\n",
    "        else:\n",
    "            thisl.append(e)\n",
    "    if len(thisl) > 0:\n",
    "        res.append(thisl)\n",
    "    return res\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 0 # embedding dimension\n",
    "    lr = 1.8 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    #stoplist = []\n",
    "    #doc_raw = 'Mary moved to the bathroom. John went to the hallway.'\n",
    "    #query_raw = 'Where is Mary?'\n",
    "    #document = doc_raw.lower().split('.')\n",
    "    #document.append(query_raw.lower())\n",
    "    #senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    #dictionary = corpora.Dictionary(senttoken)\n",
    "    #print(dictionary.token2id)\n",
    "    \n",
    "    stories = get_stories(open('qa1_test_short.txt', 'r'))\n",
    "    dct = corpora.Dictionary()\n",
    "    n_memories = 0\n",
    "    for stry in stories:\n",
    "        dct.add_documents([[s for s in stry[0]], stry[1], [stry[2]]])\n",
    "        n_memories += stry[0].count('.')\n",
    "    print(dct.token2id)\n",
    "    #print(stories)\n",
    "    \n",
    "    #document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dct)\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding\n",
    "    tA = np.random.randn(d_embed)*0.01 # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding\n",
    "    tB = np.random.randn(d_embed)*0.01 # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.01 # output to memory embedding\n",
    "    tC = np.random.randn(d_embed)*0.01 # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.01 # final weight matrix\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mtA = np.zeros_like(tA)\n",
    "    mB = np.zeros_like(B)\n",
    "    mtB = np.zeros_like(tB)\n",
    "    mC = np.zeros_like(C)\n",
    "    mtC = np.zeros_like(tC)\n",
    "    mW = np.zeros_like(W)\n",
    "\n",
    "    x = np.zeros((n_memories, voc))\n",
    "    q = np.zeros((n_memories, voc)) # we are going to store the queries for each memory index i\n",
    "    trth = np.zeros((n_memories, voc)) # and also the answers (might later add an extra index to save memory - todo)\n",
    "    #for i in range(n_memories):\n",
    "        #thissent = document[i].lower().split()\n",
    "        #for j in range(len(thissent)):\n",
    "            #x[i][j] = dct.token2id[thissent[j]]\n",
    "    i = 0\n",
    "    for stry in stories: # one by one, vectorize each story\n",
    "        thisxline = splitlst(stry[0], '.')\n",
    "        thisqline = stry[1]\n",
    "        for l in range(len(thisxline)): # for each fact\n",
    "            for j in range(len(thisxline[l])): # for each word in fact\n",
    "                x[i][j] = dct.token2id[thisxline[l][j]]\n",
    "            for j in range(len(thisqline)): # for each word in query\n",
    "                q[i][j] = dct.token2id[thisqline[j]]\n",
    "            trth[i][dct.token2id[stry[2]]] = 1\n",
    "            i += 1\n",
    "            \n",
    "    a_predict = np.zeros((n_memories, voc))\n",
    "         \n",
    "    for iterctr in range(100):\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "        m = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "        \n",
    "        # need to iter over all queries\n",
    "        for iq in range(n_memories):\n",
    "\n",
    "            # query embedding u = B * q + tB\n",
    "            u = np.zeros(d_embed)\n",
    "            u = np.dot(B, q[iq]) + tB\n",
    "\n",
    "            # match of query with memory p = softmax(u * mi) for all i\n",
    "            p = np.zeros((n_memories, d_embed))\n",
    "            p = softmax(np.dot(u, m.T))\n",
    "\n",
    "            # output corresponding to input xi: ci = C * xi + tC\n",
    "            c = np.zeros((n_memories, d_embed))\n",
    "            for i in range(n_memories):\n",
    "                c[i] = np.dot(C, x[i].T) + tC\n",
    "\n",
    "            # response vector from memory o = sum pi * ci\n",
    "            o = np.zeros(d_embed)\n",
    "            o = np.dot(p.T, c)\n",
    "\n",
    "            # predicted label a = softmax( W * (o + u))\n",
    "            a_predict[iq] = softmax(np.dot(W, (o + u)))\n",
    "            #print(a_predict)\n",
    "\n",
    "            # backpropagation\n",
    "\n",
    "            dA = np.zeros_like(A)\n",
    "            dB = np.zeros_like(B)\n",
    "            dC = np.zeros_like(C)\n",
    "            dW = np.zeros_like(W)\n",
    "            dtA = np.zeros_like(tA)\n",
    "            dtB = np.zeros_like(tB)\n",
    "            dtC = np.zeros_like(tC)\n",
    "\n",
    "            dy = a_predict - trth[iq]\n",
    "            # dA = dy a_predict * (1-a_predict) W sumi p[i] (1-p[i]) ( u.T * 1A * x[i]) c[i]\n",
    "            #print('V: %d' % (voc))\n",
    "            #print('d: %d', (d_embed))\n",
    "            ABCunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "            Wunit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "            tunit = np.ones_like(tA)\n",
    "\n",
    "            dEAtmp = 0.\n",
    "            for i in range(n_memories):\n",
    "                dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u, np.dot(ABCunit, x[i].T)), c[i])\n",
    "            dEAtmp = W * dEAtmp\n",
    "            dA = (np.dot(dy, a_predict[iq] * (1-a_predict[iq])) * dEAtmp).T\n",
    "            #print(dA)\n",
    "\n",
    "            # dB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1B q).T m[i]) c[i] + 1B q)\n",
    "            dEAtmp = 0.\n",
    "            for i in range(n_memories):\n",
    "                dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(np.dot(ABCunit, q[iq]), m[i]), c[i])\n",
    "            dEAtmp = W * dEAtmp\n",
    "            dB = (np.dot(dy, a_predict[iq]*(1-a_predict[iq])) * dEAtmp).T\n",
    "            #print(dB)\n",
    "\n",
    "            # dC = dy a_predict * (1-a_predict) W sumi p[i] 1C x[i]\n",
    "            dEAtmp = 0.\n",
    "            for i in range(n_memories):\n",
    "                dEAtmp += p[i]*np.dot(ABCunit, x[i])\n",
    "            dEAtmp = W * dEAtmp\n",
    "            dC = (np.dot(dy, a_predict[iq]*(1-a_predict[iq])) * dEAtmp).T\n",
    "            #print(dC)\n",
    "\n",
    "            # dW = dy a_predict * (1-a_predict) (o + u)\n",
    "            dW = (np.dot(dy, a_predict[iq]*(1-a_predict[iq])) * Wunit * (o + u))\n",
    "            #print(dW)   \n",
    "\n",
    "            # dtA = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) (u.T 1tA) c[i])\n",
    "            dEAtmp = 0.\n",
    "            for i in range(n_memories):\n",
    "                dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u.T, tunit), c[i])\n",
    "            dtA = (np.dot(dy * a_predict[iq] * (1-a_predict[iq]), W) * dEAtmp).T\n",
    "            #print(dtA)\n",
    "\n",
    "            # dtB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1tB).T m[i] c[i]) + 1tB)\n",
    "            dEAtmp = 0.\n",
    "            for i in range(n_memories):\n",
    "                dEAtmp += p[i]*(1.-p[i])*(np.dot(np.dot(tunit.T, m[i]), c[i]) + tunit)\n",
    "            dtB = (np.dot(dy * a_predict[iq] * (1-a_predict[iq]), W) * dEAtmp).T\n",
    "            #print(dtB)\n",
    "\n",
    "            # dtC = dy a_predict * (1-a_predict) W ( sumi p[i] 1tC )\n",
    "            dEAtmp = 0.\n",
    "            for i in range(n_memories):\n",
    "                dEAtmp += p[i]*tunit\n",
    "            dtC = (np.dot(dy * a_predict[iq] * (1-a_predict[iq]), W) * dEAtmp).T\n",
    "            #print(dtC)\n",
    "\n",
    "            # maybe clip ?\n",
    "\n",
    "            # update weights with Adagrad\n",
    "            for weights, dweights, memwghts in zip([A,B,C,W,tA,tB,tC], [dA,dB,dC,dW,dtA,dtB,dtC], [mA,mB,mC,mW,mtA,mtB,mtC]):\n",
    "                memwghts += dweights * dweights\n",
    "                weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "\n",
    "            #print(A)\n",
    "    for iq in range(n_memories):\n",
    "        print(a_predict[iq])\n",
    "        #print(np.argmax(a_predict))\n",
    "        print(dct[np.argmax(a_predict[iq])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['john', 'travelled', 'to', 'the', 'hallway'], ['mary', 'journeyed', 'to', 'the', 'bathroom']]\n"
     ]
    }
   ],
   "source": [
    "def splitlst(lst, sep): # split list at separator sep into different lists\n",
    "    res = []\n",
    "    thisl = []\n",
    "    for e in lst:\n",
    "        if e == sep:\n",
    "            res.append(thisl)\n",
    "            thisl = []\n",
    "        else:\n",
    "            thisl.append(e)\n",
    "    if len(thisl) > 0:\n",
    "        res.append(thisl)\n",
    "    return res\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    lst = ['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.']\n",
    "    print(splitlst(lst, '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'into': 0, 'kitchen': 1, 'sam': 2, 'the': 3, 'walks': 4, '': 5, 'an': 6, 'apple': 7, 'picks': 8, 'up': 9, 'bedroom': 10, 'drops': 11, 'is': 12, 'where': 13}\n",
      "[1.31351085e-03 1.60122327e-07 7.42432013e-04 2.38615087e-03\n",
      " 7.28085913e-06 1.34016459e-06 3.14062288e-05 7.39954981e-04\n",
      " 7.87630566e-07 2.22681567e-03 9.90902630e-01 1.64277869e-03\n",
      " 2.23272785e-07 4.52829329e-06]\n",
      "bedroom\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 10 # embedding dimension\n",
    "    lr = 0.1 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    doc_raw = 'sam walks into the kitchen. sam picks up an apple. sam walks into the bedroom. sam drops the apple.'\n",
    "    query_raw = 'where is sam'\n",
    "    document = doc_raw.split('.')\n",
    "    document.append(query_raw)\n",
    "    senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    print(dictionary.token2id)\n",
    "    document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dictionary)\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "    n_memories = len(document)\n",
    "    #print(voc)\n",
    "    #print(document)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.1 # input to memory embedding\n",
    "    tA = np.zeros(d_embed) # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.1 # query embedding\n",
    "    tB = np.zeros(d_embed) # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.1 # output to memory embedding\n",
    "    tC = np.zeros(d_embed) # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.1 # final weight matrix\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mtA = np.zeros_like(tA)\n",
    "    mB = np.zeros_like(B)\n",
    "    mtB = np.zeros_like(tB)\n",
    "    mC = np.zeros_like(C)\n",
    "    mtC = np.zeros_like(tC)\n",
    "    mW = np.zeros_like(W)\n",
    "    \n",
    "    for iterctr in range(10000):\n",
    "\n",
    "        # forward pass\n",
    "\n",
    "        # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "        x = np.zeros((n_memories, voc))\n",
    "        m = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            thissent = document[i].split()\n",
    "            for j in range(len(thissent)):\n",
    "                x[i][j] = dictionary.token2id[thissent[j]]\n",
    "        for i in range(n_memories):\n",
    "            m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "        #print(x)\n",
    "        #print(m)\n",
    "\n",
    "        # query embedding u = B * q + tB\n",
    "        q = np.zeros(voc)\n",
    "        u = np.zeros(d_embed)\n",
    "        thissent = query_raw.split()\n",
    "        for j in range(len(thissent)):\n",
    "            q[j] = dictionary.token2id[thissent[j]]\n",
    "        u = np.dot(B, q) + tB\n",
    "\n",
    "        # match of query with memory p = softmax(u * mi) for all i\n",
    "        p = np.zeros((n_memories, d_embed))\n",
    "        p = softmax(np.dot(u, m.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi + tC\n",
    "        c = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            c[i] = np.dot(C, x[i].T) + tC\n",
    "\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(p.T, c)\n",
    "\n",
    "        # predicted label a = softmax( W * (o + u))\n",
    "        a_predict = softmax(np.dot(W, (o + u)))\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dC = np.zeros_like(C)\n",
    "        dW = np.zeros_like(W)\n",
    "        dtA = np.zeros_like(tA)\n",
    "        dtB = np.zeros_like(tB)\n",
    "        dtC = np.zeros_like(tC)\n",
    "\n",
    "        truth = np.zeros_like(tA)\n",
    "        truth[10] = 1 # bedroom\n",
    "        dy = a_predict - truth\n",
    "        # dA = dy a_predict * (1-a_predict) W sumi p[i] (1-p[i]) ( u.T * 1A * x[i]) c[i]\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABCunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        Wunit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "        tunit = np.ones_like(tA)\n",
    "\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u, np.dot(ABCunit, x[i].T)), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1B q).T m[i]) c[i] + 1B q)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(np.dot(ABCunit, q), m[i]), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dC = dy a_predict * (1-a_predict) W sumi p[i] 1C x[i]\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*np.dot(ABCunit, x[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dC = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dC)\n",
    "\n",
    "        # dW = dy a_predict * (1-a_predict) (o + u)\n",
    "        dW = (np.dot(dy, a_predict*(1-a_predict)) * Wunit * (o + u))\n",
    "        #print(dW)   \n",
    "\n",
    "        # dtA = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) (u.T 1tA) c[i])\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u.T, tunit), c[i])\n",
    "        dtA = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtA)\n",
    "\n",
    "        # dtB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1tB).T m[i] c[i]) + 1tB)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*(np.dot(np.dot(tunit.T, m[i]), c[i]) + tunit)\n",
    "        dtB = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtB)\n",
    "\n",
    "        # dtC = dy a_predict * (1-a_predict) W ( sumi p[i] 1tC )\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*tunit\n",
    "        dtC = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtC)\n",
    "\n",
    "        # maybe clip ?\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,C,W,tA,tB,tC], [dA,dB,dC,dW,dtA,dtB,dtC], [mA,mB,mC,mW,mtA,mtB,mtC]):\n",
    "            memwghts += dweights * dweights\n",
    "            weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

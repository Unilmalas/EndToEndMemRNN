{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'into': 0, 'kitchen': 1, 'sam': 2, 'the': 3, 'walks': 4, '': 5, 'an': 6, 'apple': 7, 'picks': 8, 'up': 9, 'bedroom': 10, 'drops': 11, 'is': 12, 'where': 13}\n",
      "[1.31351085e-03 1.60122327e-07 7.42432013e-04 2.38615087e-03\n",
      " 7.28085913e-06 1.34016459e-06 3.14062288e-05 7.39954981e-04\n",
      " 7.87630566e-07 2.22681567e-03 9.90902630e-01 1.64277869e-03\n",
      " 2.23272785e-07 4.52829329e-06]\n",
      "bedroom\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 10 # embedding dimension\n",
    "    lr = 0.1 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    doc_raw = 'sam walks into the kitchen. sam picks up an apple. sam walks into the bedroom. sam drops the apple.'\n",
    "    query_raw = 'where is sam'\n",
    "    document = doc_raw.split('.')\n",
    "    document.append(query_raw)\n",
    "    senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    print(dictionary.token2id)\n",
    "    document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dictionary)\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "    n_memories = len(document)\n",
    "    #print(voc)\n",
    "    #print(document)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.1 # input to memory embedding\n",
    "    tA = np.zeros(d_embed) # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.1 # query embedding\n",
    "    tB = np.zeros(d_embed) # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.1 # output to memory embedding\n",
    "    tC = np.zeros(d_embed) # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.1 # final weight matrix\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mtA = np.zeros_like(tA)\n",
    "    mB = np.zeros_like(B)\n",
    "    mtB = np.zeros_like(tB)\n",
    "    mC = np.zeros_like(C)\n",
    "    mtC = np.zeros_like(tC)\n",
    "    mW = np.zeros_like(W)\n",
    "    \n",
    "    for iterctr in range(10000):\n",
    "\n",
    "        # forward pass\n",
    "\n",
    "        # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "        x = np.zeros((n_memories, voc))\n",
    "        m = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            thissent = document[i].split()\n",
    "            for j in range(len(thissent)):\n",
    "                x[i][j] = dictionary.token2id[thissent[j]]\n",
    "        for i in range(n_memories):\n",
    "            m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "        #print(x)\n",
    "        #print(m)\n",
    "\n",
    "        # query embedding u = B * q + tB\n",
    "        q = np.zeros(voc)\n",
    "        u = np.zeros(d_embed)\n",
    "        thissent = query_raw.split()\n",
    "        for j in range(len(thissent)):\n",
    "            q[j] = dictionary.token2id[thissent[j]]\n",
    "        u = np.dot(B, q) + tB\n",
    "\n",
    "        # match of query with memory p = softmax(u * mi) for all i\n",
    "        p = np.zeros((n_memories, d_embed))\n",
    "        p = softmax(np.dot(u, m.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi + tC\n",
    "        c = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            c[i] = np.dot(C, x[i].T) + tC\n",
    "\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(p.T, c)\n",
    "\n",
    "        # predicted label a = softmax( W * (o + u))\n",
    "        a_predict = softmax(np.dot(W, (o + u)))\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dC = np.zeros_like(C)\n",
    "        dW = np.zeros_like(W)\n",
    "        dtA = np.zeros_like(tA)\n",
    "        dtB = np.zeros_like(tB)\n",
    "        dtC = np.zeros_like(tC)\n",
    "\n",
    "        truth = np.zeros_like(tA)\n",
    "        truth[10] = 1 # bedroom\n",
    "        dy = a_predict - truth\n",
    "        # dA = dy a_predict * (1-a_predict) W sumi p[i] (1-p[i]) ( u.T * 1A * x[i]) c[i]\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABCunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        Wunit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "        tunit = np.ones_like(tA)\n",
    "\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u, np.dot(ABCunit, x[i].T)), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1B q).T m[i]) c[i] + 1B q)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(np.dot(ABCunit, q), m[i]), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dC = dy a_predict * (1-a_predict) W sumi p[i] 1C x[i]\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*np.dot(ABCunit, x[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dC = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dC)\n",
    "\n",
    "        # dW = dy a_predict * (1-a_predict) (o + u)\n",
    "        dW = (np.dot(dy, a_predict*(1-a_predict)) * Wunit * (o + u))\n",
    "        #print(dW)   \n",
    "\n",
    "        # dtA = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) (u.T 1tA) c[i])\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u.T, tunit), c[i])\n",
    "        dtA = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtA)\n",
    "\n",
    "        # dtB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1tB).T m[i] c[i]) + 1tB)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*(np.dot(np.dot(tunit.T, m[i]), c[i]) + tunit)\n",
    "        dtB = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtB)\n",
    "\n",
    "        # dtC = dy a_predict * (1-a_predict) W ( sumi p[i] 1tC )\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*tunit\n",
    "        dtC = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtC)\n",
    "\n",
    "        # maybe clip ?\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,C,W,tA,tB,tC], [dA,dB,dC,dW,dtA,dtB,dtC], [mA,mB,mC,mW,mtA,mtB,mtC]):\n",
    "            memwghts += dweights * dweights\n",
    "            weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bathroom': 0, 'mary': 1, 'moved': 2, 'the': 3, 'to': 4, '': 5, 'hallway': 6, 'john': 7, 'went': 8, 'is': 9, 'mary?': 10, 'where': 11}\n",
      "[1.80358042e-26 2.62683263e-52 3.43609638e-23 1.95285802e-46\n",
      " 1.74655170e-80 5.77479641e-51 8.38014096e-27 1.59883218e-76\n",
      " 5.97746901e-61 5.03652155e-57 8.98635207e-54 1.00000000e+00]\n",
      "where\n"
     ]
    }
   ],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 0 # embedding dimension\n",
    "    lr = 1.8 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    stoplist = []\n",
    "    doc_raw = 'Mary moved to the bathroom. John went to the hallway.'\n",
    "    query_raw = 'Where is Mary?'\n",
    "    document = doc_raw.lower().split('.')\n",
    "    document.append(query_raw.lower())\n",
    "    senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    dictionary = corpora.Dictionary(senttoken)\n",
    "    print(dictionary.token2id)\n",
    "    document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dictionary)\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "    n_memories = len(document)\n",
    "    #print(voc)\n",
    "    #print(document)\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding\n",
    "    tA = np.random.randn(d_embed)*0.01 # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding\n",
    "    tB = np.random.randn(d_embed)*0.01 # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.01 # output to memory embedding\n",
    "    tC = np.random.randn(d_embed)*0.01 # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.01 # final weight matrix\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mtA = np.zeros_like(tA)\n",
    "    mB = np.zeros_like(B)\n",
    "    mtB = np.zeros_like(tB)\n",
    "    mC = np.zeros_like(C)\n",
    "    mtC = np.zeros_like(tC)\n",
    "    mW = np.zeros_like(W)\n",
    "\n",
    "    x = np.zeros((n_memories, voc))\n",
    "    for i in range(n_memories):\n",
    "        thissent = document[i].lower().split()\n",
    "        for j in range(len(thissent)):\n",
    "            x[i][j] = dictionary.token2id[thissent[j]]\n",
    "         \n",
    "    for iterctr in range(1000):\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "        m = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "\n",
    "        # query embedding u = B * q + tB\n",
    "        q = np.zeros(voc)\n",
    "        u = np.zeros(d_embed)\n",
    "        thissent = query_raw.lower().split()\n",
    "        for j in range(len(thissent)):\n",
    "            q[j] = dictionary.token2id[thissent[j]]\n",
    "        u = np.dot(B, q) + tB\n",
    "\n",
    "        # match of query with memory p = softmax(u * mi) for all i\n",
    "        p = np.zeros((n_memories, d_embed))\n",
    "        p = softmax(np.dot(u, m.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi + tC\n",
    "        c = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            c[i] = np.dot(C, x[i].T) + tC\n",
    "\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(p.T, c)\n",
    "\n",
    "        # predicted label a = softmax( W * (o + u))\n",
    "        a_predict = softmax(np.dot(W, (o + u)))\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dC = np.zeros_like(C)\n",
    "        dW = np.zeros_like(W)\n",
    "        dtA = np.zeros_like(tA)\n",
    "        dtB = np.zeros_like(tB)\n",
    "        dtC = np.zeros_like(tC)\n",
    "\n",
    "        truth = np.zeros_like(tA)\n",
    "        truth[1] = 1 # answer\n",
    "        dy = a_predict - truth\n",
    "        # dA = dy a_predict * (1-a_predict) W sumi p[i] (1-p[i]) ( u.T * 1A * x[i]) c[i]\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABCunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        Wunit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "        tunit = np.ones_like(tA)\n",
    "\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u, np.dot(ABCunit, x[i].T)), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1B q).T m[i]) c[i] + 1B q)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(np.dot(ABCunit, q), m[i]), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dC = dy a_predict * (1-a_predict) W sumi p[i] 1C x[i]\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*np.dot(ABCunit, x[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dC = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dC)\n",
    "\n",
    "        # dW = dy a_predict * (1-a_predict) (o + u)\n",
    "        dW = (np.dot(dy, a_predict*(1-a_predict)) * Wunit * (o + u))\n",
    "        #print(dW)   \n",
    "\n",
    "        # dtA = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) (u.T 1tA) c[i])\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u.T, tunit), c[i])\n",
    "        dtA = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtA)\n",
    "\n",
    "        # dtB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1tB).T m[i] c[i]) + 1tB)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*(np.dot(np.dot(tunit.T, m[i]), c[i]) + tunit)\n",
    "        dtB = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtB)\n",
    "\n",
    "        # dtC = dy a_predict * (1-a_predict) W ( sumi p[i] 1tC )\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*tunit\n",
    "        dtC = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtC)\n",
    "\n",
    "        # maybe clip ?\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,C,W,tA,tB,tC], [dA,dB,dC,dW,dtA,dtB,dtC], [mA,mB,mC,mW,mtA,mtB,mtC]):\n",
    "            memwghts += dweights * dweights\n",
    "            weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dictionary[np.argmax(a_predict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'bathroom': 1, 'hallway': 2, 'john': 3, 'journeyed': 4, 'mary': 5, 'the': 6, 'to': 7, 'travelled': 8, '?': 9, 'is': 10, 'where': 11, 'back': 12, 'bedroom': 13, 'daniel': 14, 'moved': 15, 'went': 16, 'kitchen': 17, 'sandra': 18, 'garden': 19, 'office': 20}\n",
      "[(['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.'], ['where', 'is', 'john', '?'], 'hallway'), (['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.', 'daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.'], ['where', 'is', 'mary', '?'], 'bathroom'), (['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.', 'daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'john', 'went', 'to', 'the', 'hallway', '.', 'sandra', 'journeyed', 'to', 'the', 'kitchen', '.'], ['where', 'is', 'sandra', '?'], 'kitchen'), (['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.', 'daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'john', 'went', 'to', 'the', 'hallway', '.', 'sandra', 'journeyed', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'john', 'went', 'to', 'the', 'garden', '.'], ['where', 'is', 'sandra', '?'], 'hallway'), (['john', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'journeyed', 'to', 'the', 'bathroom', '.', 'daniel', 'went', 'back', 'to', 'the', 'bathroom', '.', 'john', 'moved', 'to', 'the', 'bedroom', '.', 'john', 'went', 'to', 'the', 'hallway', '.', 'sandra', 'journeyed', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'john', 'went', 'to', 'the', 'garden', '.', 'sandra', 'went', 'back', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'kitchen', '.'], ['where', 'is', 'sandra', '?'], 'kitchen'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.'], ['where', 'is', 'sandra', '?'], 'hallway'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'went', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'garden', '.'], ['where', 'is', 'sandra', '?'], 'garden'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'went', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'garden', '.', 'sandra', 'travelled', 'to', 'the', 'office', '.', 'daniel', 'journeyed', 'to', 'the', 'hallway', '.'], ['where', 'is', 'daniel', '?'], 'hallway'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'went', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'garden', '.', 'sandra', 'travelled', 'to', 'the', 'office', '.', 'daniel', 'journeyed', 'to', 'the', 'hallway', '.', 'daniel', 'journeyed', 'to', 'the', 'office', '.', 'john', 'moved', 'to', 'the', 'hallway', '.'], ['where', 'is', 'sandra', '?'], 'office'), (['sandra', 'travelled', 'to', 'the', 'kitchen', '.', 'sandra', 'travelled', 'to', 'the', 'hallway', '.', 'mary', 'went', 'to', 'the', 'bathroom', '.', 'sandra', 'moved', 'to', 'the', 'garden', '.', 'sandra', 'travelled', 'to', 'the', 'office', '.', 'daniel', 'journeyed', 'to', 'the', 'hallway', '.', 'daniel', 'journeyed', 'to', 'the', 'office', '.', 'john', 'moved', 'to', 'the', 'hallway', '.', 'john', 'travelled', 'to', 'the', 'bathroom', '.', 'john', 'journeyed', 'to', 'the', 'office', '.'], ['where', 'is', 'daniel', '?'], 'office')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import functools\n",
    "from gensim import corpora\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''\n",
    "    argument: a sentence string\n",
    "    returns a list of tokens(words)\n",
    "    '''\n",
    "    return [ x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    " \n",
    "def parse_stories(lines):\n",
    "    '''\n",
    "    - Parse stories provided in the bAbI tasks format\n",
    "    - A story starts from line 1 to line 15. Every 3rd line,\n",
    "      there is a question & answer.\n",
    "    - Function extracts sub-stories within a story and\n",
    "      creates tuples\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        line = line.strip().lower()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A & support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    " \n",
    "def get_stories(f):\n",
    "    '''\n",
    "    argument: filename\n",
    "    returns list of all stories in the argument data-set file\n",
    "    '''\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines())\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: functools.reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data]\n",
    "    return data\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    stories = get_stories(open('qa1_test_short.txt', 'r'))\n",
    "    dct = corpora.Dictionary()\n",
    "    for stry in stories:\n",
    "        dct.add_documents([[s for s in stry[0]], stry[1], [stry[2]]])\n",
    "    print(dct.token2id)\n",
    "    print(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-to-end memory network\n",
    "\n",
    "import numpy as np\n",
    "from gensim import corpora\n",
    "import math\n",
    "import re\n",
    "import functools\n",
    "\n",
    "def parse_stories(lines):\n",
    "    '''\n",
    "    - Parse stories provided in the bAbI tasks format\n",
    "    - A story starts from line 1 to line 15. Every 3rd line,\n",
    "      there is a question & answer.\n",
    "    - Function extracts sub-stories within a story and\n",
    "      creates tuples\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        line = line.strip().lower()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A & support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    " \n",
    "def get_stories(f):\n",
    "    '''\n",
    "    argument: filename\n",
    "    returns list of all stories in the argument data-set file\n",
    "    '''\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines())\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: functools.reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data]\n",
    "    return data\n",
    "\n",
    "def softmax(x):\n",
    "    ex = np.exp(x - np.max(x))\n",
    "    return ex / ex.sum()\n",
    "\n",
    "def lj(j, J, k, d): \n",
    "    return (1-j/J)-(k/d)*(1-2*j/J)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # load input data\n",
    "    #data = open('input1.txt', 'r').read() # input text file: sentences separated by .\n",
    "\n",
    "    # hyperparameters\n",
    "    d_embed = 0 # embedding dimension\n",
    "    lr = 1.8 # learning rate\n",
    "    \n",
    "    # input to memory embedding mi = A * xi + tA\n",
    "        #stoplist = set('for a of the and to in'.split())\n",
    "    #stoplist = []\n",
    "    #doc_raw = 'Mary moved to the bathroom. John went to the hallway.'\n",
    "    #query_raw = 'Where is Mary?'\n",
    "    #document = doc_raw.lower().split('.')\n",
    "    #document.append(query_raw.lower())\n",
    "    #senttoken = [ [word for word in sentence.lower().split(' ') if word not in stoplist] for sentence in document ]\n",
    "    #dictionary = corpora.Dictionary(senttoken)\n",
    "    #print(dictionary.token2id)\n",
    "    \n",
    "    stories = get_stories(open('qa1_test_short.txt', 'r'))\n",
    "    dct = corpora.Dictionary()\n",
    "    n_memories = 0\n",
    "    for stry in stories:\n",
    "        dct.add_documents([[s for s in stry[0]], stry[1], [stry[2]]])\n",
    "        n_memories += stry[0].count('.')\n",
    "    print(dct.token2id)\n",
    "    #print(stories)\n",
    "    \n",
    "    #document.pop(len(document)-1) # query at the end of document\n",
    "    d_embed = len(dct)\n",
    "    #voc = 0 # size of vocabulary\n",
    "    #for d in document:\n",
    "        #print(d)\n",
    "        #if len(d) == 0:\n",
    "            #document.remove(d)\n",
    "        #voc = max(voc, len(d.split()))\n",
    "    voc = d_embed\n",
    "\n",
    "    # initiate weigth matrices\n",
    "    A = np.random.randn(d_embed, voc)*0.01 # input to memory embedding\n",
    "    tA = np.random.randn(d_embed)*0.01 # temporal encoding A\n",
    "    B = np.random.randn(d_embed, voc)*0.01 # query embedding\n",
    "    tB = np.random.randn(d_embed)*0.01 # temporal encoding B\n",
    "    C = np.random.randn(d_embed, voc)*0.01 # output to memory embedding\n",
    "    tC = np.random.randn(d_embed)*0.01 # temporal encoding C\n",
    "    W = np.random.randn(voc, d_embed)*0.01 # final weight matrix\n",
    "    \n",
    "    # memory for Adagrad\n",
    "    mA = np.zeros_like(A)\n",
    "    mtA = np.zeros_like(tA)\n",
    "    mB = np.zeros_like(B)\n",
    "    mtB = np.zeros_like(tB)\n",
    "    mC = np.zeros_like(C)\n",
    "    mtC = np.zeros_like(tC)\n",
    "    mW = np.zeros_like(W)\n",
    "\n",
    "    x = np.zeros((n_memories, voc))\n",
    "    #for i in range(n_memories):\n",
    "        #thissent = document[i].lower().split()\n",
    "        #for j in range(len(thissent)):\n",
    "            #x[i][j] = dct.token2id[thissent[j]]\n",
    "    i = 0\n",
    "    for stry in stories:\n",
    "        thisline = stry[0].split('.')\n",
    "        for l in range(len(thisline)):\n",
    "            thissent = thisline.split()\n",
    "            for j in range(len(thissent)):\n",
    "                x[i][j] = dct.token2id[thissent[j]]\n",
    "                i += 1\n",
    "         \n",
    "    for iterctr in range(1000):\n",
    "\n",
    "        # forward pass\n",
    "        \n",
    "        # embedding simple: m_i = A_ij * x_ij + T_A_j\n",
    "        m = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            m[i] = np.dot(A, x[i].T) + tA # simple embedding\n",
    "            #m[i][j] = lj(j,len(document[i]),j,d_embed) * A[i][j] * x[i][j] + tA[j] # with positional encoding\n",
    "\n",
    "        # query embedding u = B * q + tB\n",
    "        q = np.zeros(voc)\n",
    "        u = np.zeros(d_embed)\n",
    "        thissent = query_raw.lower().split()\n",
    "        for j in range(len(thissent)):\n",
    "            q[j] = dct.token2id[thissent[j]]\n",
    "        u = np.dot(B, q) + tB\n",
    "\n",
    "        # match of query with memory p = softmax(u * mi) for all i\n",
    "        p = np.zeros((n_memories, d_embed))\n",
    "        p = softmax(np.dot(u, m.T))\n",
    "\n",
    "        # output corresponding to input xi: ci = C * xi + tC\n",
    "        c = np.zeros((n_memories, d_embed))\n",
    "        for i in range(n_memories):\n",
    "            c[i] = np.dot(C, x[i].T) + tC\n",
    "\n",
    "        # response vector from memory o = sum pi * ci\n",
    "        o = np.zeros(d_embed)\n",
    "        o = np.dot(p.T, c)\n",
    "\n",
    "        # predicted label a = softmax( W * (o + u))\n",
    "        a_predict = softmax(np.dot(W, (o + u)))\n",
    "        #print(a_predict)\n",
    "\n",
    "        # backpropagation\n",
    "\n",
    "        dA = np.zeros_like(A)\n",
    "        dB = np.zeros_like(B)\n",
    "        dC = np.zeros_like(C)\n",
    "        dW = np.zeros_like(W)\n",
    "        dtA = np.zeros_like(tA)\n",
    "        dtB = np.zeros_like(tB)\n",
    "        dtC = np.zeros_like(tC)\n",
    "\n",
    "        truth = np.zeros_like(tA)\n",
    "        truth[1] = 1 # answer\n",
    "        dy = a_predict - truth\n",
    "        # dA = dy a_predict * (1-a_predict) W sumi p[i] (1-p[i]) ( u.T * 1A * x[i]) c[i]\n",
    "        #print('V: %d' % (voc))\n",
    "        #print('d: %d', (d_embed))\n",
    "        ABCunit = np.pad(np.identity(voc), ((0,d_embed-voc),(0,0)), 'constant', constant_values=(0))\n",
    "        Wunit = np.pad(np.identity(voc), ((0,0), (0,d_embed-voc)), 'constant', constant_values=(0))\n",
    "        tunit = np.ones_like(tA)\n",
    "\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u, np.dot(ABCunit, x[i].T)), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dA = (np.dot(dy, a_predict * (1-a_predict)) * dEAtmp).T\n",
    "        #print(dA)\n",
    "\n",
    "        # dB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1B q).T m[i]) c[i] + 1B q)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(np.dot(ABCunit, q), m[i]), c[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dB = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dB)\n",
    "\n",
    "        # dC = dy a_predict * (1-a_predict) W sumi p[i] 1C x[i]\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*np.dot(ABCunit, x[i])\n",
    "        dEAtmp = W * dEAtmp\n",
    "        dC = (np.dot(dy, a_predict*(1-a_predict)) * dEAtmp).T\n",
    "        #print(dC)\n",
    "\n",
    "        # dW = dy a_predict * (1-a_predict) (o + u)\n",
    "        dW = (np.dot(dy, a_predict*(1-a_predict)) * Wunit * (o + u))\n",
    "        #print(dW)   \n",
    "\n",
    "        # dtA = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) (u.T 1tA) c[i])\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*np.dot(np.dot(u.T, tunit), c[i])\n",
    "        dtA = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtA)\n",
    "\n",
    "        # dtB = dy a_predict * (1-a_predict) W ( sumi p[i] (1-p[i]) ((1tB).T m[i] c[i]) + 1tB)\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*(1.-p[i])*(np.dot(np.dot(tunit.T, m[i]), c[i]) + tunit)\n",
    "        dtB = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtB)\n",
    "\n",
    "        # dtC = dy a_predict * (1-a_predict) W ( sumi p[i] 1tC )\n",
    "        dEAtmp = 0.\n",
    "        for i in range(n_memories):\n",
    "            dEAtmp += p[i]*tunit\n",
    "        dtC = (np.dot(dy * a_predict * (1-a_predict), W) * dEAtmp).T\n",
    "        #print(dtC)\n",
    "\n",
    "        # maybe clip ?\n",
    "\n",
    "        # update weights with Adagrad\n",
    "        for weights, dweights, memwghts in zip([A,B,C,W,tA,tB,tC], [dA,dB,dC,dW,dtA,dtB,dtC], [mA,mB,mC,mW,mtA,mtB,mtC]):\n",
    "            memwghts += dweights * dweights\n",
    "            weights += -lr * dweights / np.sqrt(memwghts + 1.e-8)\n",
    "\n",
    "        #print(A)\n",
    "    print(a_predict)\n",
    "    #print(np.argmax(a_predict))\n",
    "    print(dct[np.argmax(a_predict)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
